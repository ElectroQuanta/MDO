#+TITLE: Project
#+AUTHOR: José Pires
#+DATE: [2021-10-19 ter 11:25]
#+EMAIL: a50178@alunos.uminho.pt

#+LATEX_COMPILER: xelatex

# dont export unless explicitly stated
# https://stackoverflow.com/a/52062773
#+PROPERTY: header-args :eval never-export

* Preamble
- This folder contains all the information about the project.
- It is written incrementally, i.e., as the several project phases take place,
  the document versions pertaining to each phase are stored in the folder
  *submission*.
** Screenshots                                                     :noexport:
   :PROPERTIES:
   :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
   :END:
[[file:sec/img/hand-gesture-recog-procedure.png]]

[[file:sec/img/dbms-abstraction-levels.png]]

[[file:sec/img/dbms-struct.png]]

[[file:sec/img/sql-dll.png]]

[[file:sec/img/sql-dml-1.png]]

[[file:sec/img/sql-dml-2.png]]

[[file:sec/img/doxygen-out1.png]]

[[file:sec/img/doxygen-out2.png]]

[[file:sec/img/doxygen-out3.png]]

[[file:sec/img/doxygen-out4.png]]

[[file:sec/img/doxygen-out5.png]]

[[file:sec/img/doxygen-out6.png]]

* Versions [4/4]
1. [X] Problem statement: deadline - <2021-10-28 qui>
2. [X] Phase 1
3. [X] Analysis phase: deadline
4. [X] Design phase: <2021-12-15 qua 23:00>

* Notes                                                           :Important:
  :PROPERTIES:
  :ID:       ea2d2209-c1f3-4de8-9acb-90bca065b262
  :END:
** Requirements
   - buildroot
   - c/c++ (both)
   - Device drivers
   - Linux/Raspberry Pi
   - CPS: Cyber-physical systems
   - Makefiles
** Problem statement
   DEADLINE: <2021-10-28 qui>
   Think about the project and deliver a document about it.
** Research
*** Topics [0/3]
 - [ ] Cyber-physical systems characteristics: _identify the required features
   for the system_
 - [ ] Analyze previous projects to understand what is feasible in terms of
   Real-time systems using device drivers with Raspberry Pi: _gives an overall
   idea of what can be achieved_
 - [ ] Situate the project requirements, the project constraints
*** Selection
- [[file:readme.org][Marketing digital Outdoor]]
** Report
*** Outline [2/8]
1. [X] Introduction
   1. Motivation and context
   2. Problem statement
   3. Market research
   4. Goals
   5. Project planning
   6. Report outline
2. [X] Analysis
   1. Background and state of the art
   2. Requirements and constraints
   3. System overview
   4. System architecture
      1. HW architecture
      2. SW architecture
   5. Subsystems decomposition
      1. Events
      2. Use cases
      3. State machine diagram
      4. Sequence diagram
   6. Budget
3. [ ] Theoretical foundations
4. [ ] Design
   1. HW specification
      1. Block diagram with COTS components, if possible
      2. List of constraints of functions to be implemented in HW or SW
   2. HW interfaces definition
      1. I/O ports
      2. HW registers
      3. Memory addresses for shared or I/O by memory mapping
      4. HW interrupts
   3. SW specification
      1. Identify main subsystems
      2. System tasks
   4. SW interfaces definition
      - Define the APIs in detail:
	- header files with:
	  - functions prototypes
	  - data structure declarations
	  - class declarations
   5. Start-up/shutdown process specification
   6. Error Handling specification
5. [ ] Implementation
6. [ ] Testing
7. [ ] Verification/Validation
8. [ ] Conclusions
** Marketing Digital Outdoor                                      :Important:
*** Topics
1. Motivation and context
   - Scenting marketing is a great approach to draw people into stores.
   - Scent sense is the fastest way to the brain, thus, providing an exceptional
     opportunity for marketing.
   - Combining that with additional stimuli, like eye and earing, can
     significantly boost the marketing outcome.
     #+begin_quote
   - Knowing your target audience it is critical for the success of stores.
   - Marketing can be used to collect, analyse, investigate and decide the best
     politics for addressing a specific target audience, understanding the
     its behavioral patterns. 
   - To understand behavioral patterns _machine-learning models_ can be used.
     #+end_quote
2. Concept
   - Offer a marketing digital outdoor for brands to advertise and captivate customers
   - Brands can buy advertisement space and time by sending the following data:
	| Digital Outdoor location | Fragrance name | Start Time | Stop Time | Audio Message | Video |
     - The brands can send the data to our company database through our
       website/application.
     - The data will then be sent to the marketing digital outdoor using a wireless
       communication technology.
   - The advertisement data will be exposed into a display, an audio message
     transmitted, as well as the indicate fragrance between the designated time
     interval.
   - COVID pandemics changed the perspectives about user interaction with the
     surroundings, with non touch interfaces being preferred. Thus, a non touch
     user interface is a must have.
   - When a user approaches the marketing digital outdoor, a proximity sensor will detect
     it and activate the _user interaction mode_.
     - When activating this mode the camera is started mirroring the user into
       the display and providing additional options.
     - In this mode, the user can:
       1) apply image filters related to the brand
       2) take pics
       3) create GIFs
       4) share them
     - This mode requires:
       - Facial detection for image filter application
       - Hand gesture recognition for navigating the menus and activating
         options
       - A virtual keyboard (non-touch) will be provided for user input, by
         mapping the screen to the keys and waiting for a designated time before
         accepting it.
       - A set of hand gestures to be used in user interaction
     - Several sharing platforms shall be provided namely social media and
       email.
       - Tracking the nr of shares provides feedback for the brands in respect
         of the brand awareness.
       - Additionally, brands can also monitor this by checking their social
         media accounts.
 
*** Technologies [0/11]
1. [ ] Computer vision for facial and gesture recognition (OpenCV)
2. [ ] Database for marketing digital outdoor management (SQL)
3. [ ] Website/Application for brands communication to our database
4. [ ] Wireless communication technology for remote communication with digital outdoors
5. [ ] Image filter application
6. [ ] Infrared detection
7. [ ] Camera recording
8. [ ] Audio output
9. [ ] Nebulizer technology for scenting
10. [ ] Screen mapping to keys for virtual keyboard
11. [ ] Social media and e-mail sharing APIs

*** ✔ DONE Problem statement
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-10-23 sáb 11:55]
    :END:

*** ✔ DONE Market research [3/3]
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-11-19 sex 23:30]
    :END:
1) [X] Scenting marketing: trends, market value
2) [X] Digital Outdoors: quantity, market value
3) [X] Combined marketing: are they digital outdoor + scenting?

*** Project planning
**** Gantt diagram [0/8]
1) [ ] Planning
   - [ ] Kick-off meeting
   - [ ] Problem Statement
   - [ ] Market Research
   - [ ] Project Planning
2) [ ] Analysis
   - [ ] System overview
   - [ ] Requirements and constraints
   - [ ] System architecture
   - [ ] System Analysis
   - [ ] Estimated budgets
   - [ ] Subsystem decomposition
     - [ ] Events
     - [ ] Use-cases
     - [ ] Dynamic operation: state-machine diagram
     - [ ] Flow of events: sequence diagram
3) [ ] Design
   - [ ] Analysis review
   - [ ] HW specification
   - [ ] Component shipping
   - [ ] Software specification
     - [ ] Remote client
     - [ ] Remote server + database
     - [ ] Local system
4) [ ] Implementation
   - [ ] HW testing
   - [ ] SW implementation
     - [ ] Remote client
     - [ ] Remote server + database
     - [ ] Local system
   - [ ] HW implementation
     - [ ] Breadboard
     - [ ] PCB design
   - [ ] System configuration
5) [ ] Testing
   1) [ ] SW unit testing
   2) [ ] SW integrated testing
   3) [ ] HW unit testing
   4) [ ] HW integrated testing
   5) [ ] Functional testing
6) [ ] Verification/Validation
   1) [ ] Verification
   2) [ ] Validation
7) [ ] Report Writing
   1) [ ] Problem statement
   2) [ ] Analysis
   3) [ ] Design
   4) [ ] Implementation
   5) [ ] Final
8) [ ] Documentation
   1) [ ] Problem statement
   2) [ ] Analysis
   3) [ ] Design
   4) [ ] Implementation
   5) [ ] Testing
**** Required HW [3/7]
Research link: https://www.one-tab.com/page/TZxmVAXJTO6nVyNO593ARA

- [X] Raspberry Pi 4: 52 EUR
- [X] HDMI screen: 50 EUR - https://shorturl.at/oyAOR 
- [ ] Relay/Transistor + Ultrassonic actuator for nebulizing fragrance
- [ ] Audio output
- [ ] Power supply
- [ ] Mechanical structure
- [X] Camera: 14 EUR - https://shorturl.at/gnpCU
*** ✔ DONE Analysis
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-11 sáb 19:50]
    :END:
**** System architecture
*Example*
#+BEGIN_QUOTE
BRAND -> DB
RC -> RS: q brand Nestle
RS -> DB: query brand Nestle
DS -> RS: Nestle milka.mp4 milka.wav Chocolate
RS -> RC: Nestle milka.mp4 milka.wav Chocolate

COMPANY -> MDO-L
Staff member login
    RC -> RS: q mdo systems
    RS -> DB: query mdo-systems
    DB -> RS: std::<vector> mdo_systems
    for(i = 0; i < mdo_systems.size(); i++ )
        RS -> RC: mdo_systems[i]
RC -> RC: Select MDO-L machine
Send command
    RC -> RS: mdo <nr> <command> (mdo 1 get mode)
    RS -> RS: parse command
    RS -> RS: get mdo_nr IP (query mdo-systems 1 IP)
    RS -> MDO-L: connect IP:port
    MDO-L -> RS: connected
    RS -> MDO-L: get mode
    MDO-L -> RS: normal mode (example)
    RS -> RC: normal
#+END_QUOTE
*** ✔ DONE Theoretical foundations [17/17]  
    :PROPERTIES:
    :ATTACH_DIR: /home/zmpl/OneDrive - Universidade do Minho/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
    :END:
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-11 sáb 19:50]
    :END:
1) [X] *Project methodology: Waterfall model*
2) [X] *Multitasking and Pthreads*
3) [X] *Client-Server architecture & TCP/IP & OSI model*
4) [X] /Daemons/
5) [X] /Device drivers/
6) [X] *Nebulizer technology for scenting*
7) [X] *Computer Vision*
   1) [X] *OpenCV*
   2) [X] *Gesture recognition algorithms*
   3) [X] *Face detection algorithms*
      1) see [[file:biblio/OpenCV3_Computer-Vision-in-C++-with-the-OpenCV-Library.pdf][openCV3 book]] (pg. 883)
8) [X] *RDBMS (Relational Database management system) (SQL)*
9) [X] /User detection technologies: IR, ultrasonic/
10) [X] /Camera recording and codecs/
11) [X] /Image filtering APIs/
12) [X] /GIFs generation/
13) [X] *Social media and e-mail sharing APIs*
14) [X] /UI framework: Qt/
15) [X] /File transfer protocols/
16) [X] *Makefiles*
17) [X] *Source code documentation*: Doxygen

Legend:
- *Ze*
- /Hugo/
*** ▭▭ IN-PROGRESS Design
    :LOGBOOK:
    - State "▭▭ IN-PROGRESS" from              [2021-12-13 seg 21:16]
    :END:

Next Thursday, December 16th, we will have the presentations for the Design Phase.
Submit the *presentation* and *report* until Wednesday 15th, 23:00h.

Design Phase must include (some specifications may vary according to your project):
1) *Hardware Specification* [6/6]:
   - [X] Architecture
     - In the analysis phase an overview of the HW architecture was
       conceptualized. In this section, a more specific HW architecture is
       illustrated, using a block-diagram.
     - BLOCK DIAGRAM and comment it
   - [X] Hardware Component Specification [8/8]
     1) [X] Raspberry Pi
	1) SD card
     2) [X] Power supply module
     3) [X] Ultrasonic sensor
     4) [X] Fragrance diffuser actuator: https://shorturl.at/hlxFQ
     5) [X] LCD display: https://shorturl.at/dnoIZ
     6) [X] Raspberry Pi Camera
     7) [X] Colunas
     8) [X] On/off button
   - [X] Peripherals Pinout/Mapping/Connection Layout
     - Draw a schematic for connecting all HW components
     - Add a table containing:
       |            | PIN MAPPING      |          |
       | Controller | Interface device | Function |
   - [X] Test Cases
     - *Ultrasonic sensor*: one will connect the ultrasonic sensor to Raspberry
       Pi. Then an object will be approximated to the sensor from several
       distances with the corresponding distance being measure with a measuring
       tape. If the distance measured by the sensor and the measuring device are
       within the error margin provided by the manufactured, the device is
       compliant.
       | HW item | Type of test | Description | Expected result |
   - [X] PCB layout (when applicable)
     - Custom PCB for ultrasonic sensors + fragrance diffuser actuator
   - [X] Tools
     - PADS, KiCad
2) *Software Specification* [11/11]:
   - [X] Entity Relations Diagram
   - [X] Classes Diagram
     - [X] Remote Client
     - [X] Remote Server
     - [X] Local System
   - [X] Data Formats
   - [X] Flowcharts
   - [X] Tasks Division (accurate plan)
   - [X] Thread Priorities
   - [X] GUI Layouts
   - [X] Test Cases
   - [X] Software COTs
   - [X] Third-party Libraries/apps
   - [X] Tools

It is compulsory to show/identify which/where all class topics are going to be implemented and used by you! Also third-party Libraries/apps must be identified so you can use other's software and materials with our consent. 
*** Implementation [0/3]
1) [ ] Remote Client
2) [ ] Remote Server
3) [ ] Local System

**** Remote Client [0/3]
1) [ ] *UI*
   1) [ ] Design the UI, populating with items, by phases:
      1) Initial view: Login + Register
      2) Main view: only if Login was successful
2) [ ] *Sockets & Client/Server Arch*
   1) [ ] Connect
   2) [ ] Send
   3) [ ] Recv
3) [ ] /Implement classes as needed/
   1) [ ] User
   2) [ ] Admin
   3) [ ] Station
   4) [ ] Timetable
   5) [ ] Ad

_Legend_:
- *Top priority*
- /Medium priority/
- ~Low priority~
**** Remote Server [0/6]
1) [ ] *MySQL (DB Server)*
   1) [ ] Create tables
   2) [ ] Run some tests
2) [ ] DB
   1) [ ] *handleQuery*
   2) [ ] /UpdateLocalSys/
   3) [ ] ~handleCmd~
3) [ ] *Sockets & Client/Server Arch*
   1) [ ] Accept connection
   2) [ ] Recv
   3) [ ] Send
4) [ ] *Parser*
5) [ ] /File Transfer/
6) [ ] ~CLI~

_Legend_:
- *Top priority*
- /Medium priority/
- ~Low priority~
**** Local System [0/2]
1) [ ] ~HW~ [0/3]
   1) [ ] Tests [0/6]:
      1) [ ] Power Supply
      2) [ ] LCD Display
      3) [ ] Speakers
      4) [ ] Fragrance diffuser
      5) [ ] Camera
      6) [ ] Ultrasonic sensor
   2) [ ] Mechanical design
   3) [ ] PCB manufacturing
2) [ ] SW: implementation [0/2]
   1) [ ] Linux & Raspbian tests [0/8]
      1) [ ] *UI*
	 1) [ ] Design the UI, populating with items, by phases:
	    1) Initial view: Login + Register
	    2) Main view: only if Login was successful
      2) [ ] *Sockets & Client/Server Arch* [0/3]
	 1) [ ] Connect
	 2) [ ] Send
	 3) [ ] Recv
      3) [ ] *Computer Vision* [0/4]
	 1) [ ] *Frame grabbing*
	 2) [ ] *Face detection*
	 3) [ ] *Gesture recognition*
	    1) [ ] Determine set of gestures required
	    2) [ ] Create database of gestures
	 4) [ ] /Image filter overlay/
      4) [ ] /Normal mode/ [0/3]
	 1) [ ] *Video reproduction*
	 2) [ ] *Audio reproduction*
	 3) [ ] /Fragrance diffusion/ [0/2]
	    1) [ ] Device driver
	    2) [ ] Daemon
      5) [ ] /User detection/
	 1) [ ] /Device driver/
	 2) [ ] /Daemon/
      6) [ ] /Twitter sharing/
	 1) [ ] Create post
	 2) [ ] Share
	 3) [ ] Handle response
      7) [ ] ~GIF generator~
      8) [ ] ~File transfer~
   2) [ ] Raspberry Pi deployment [0/3]
      1) [ ] Buildroot customization
      2) [ ] Startup/Shutdown sequence
      3) [ ] System verification

_Legend_:
- *Top priority*
- /Medium priority/
- ~Low priority~
**** Collaborative work
***** Folder struct
 - code [0/3]
   - [ ] RC
     - [ ] src
     - [ ] inc
     - [ ] obj
     - [ ] bin
     - [ ] doc
     - [ ] ver
   - [ ] RS
     - [ ] src
     - [ ] inc
     - [ ] obj
     - [ ] bin
     - [ ] doc
     - [ ] ver
   - [ ] LS
     - [ ] src
     - [ ] inc
     - [ ] obj
     - [ ] bin
     - [ ] doc
     - [ ] ver
***** Git workflow
 1) Create separate branches
    1) Example
       1) Main: stable
       2) Developer: ongoing work
       3) Feature: implement specific feature
    2) ToDo
       1) Main: stable
       2) Hugo: RC + RS
	  1) Login: implement specific feature
       3) Ze: Local
***** Documentation
 1) Doxygen
    1) Add snippets
    2) Scope of doc
       1) *block*: used to describe classes, functions, structures, and enumerations.
	  #+BEGIN_SRC c
       /**
	* allocate dynamic memory and initialze App
	* @return initialized App_T
	*/
       App_T App_init();
	  #+END_SRC
	  1) *Public*: public interfaces are documented in the interface file (=.h=)
	     #+BEGIN_SRC c
	  /**
	   * allocate dynamic memory and initialze App
	   * @return initialized App_T
	   */
	  App_T App_init();
	     #+END_SRC
	  2) *Private*: private interfaces are documented in the implementation file (=.c=)
	     #+BEGIN_SRC c
	  /**
	   * @brief Saves the database
	   * @param db: a constructed database
	   * @param list: a constructed list to save
	   * @param serialize: pointer to generic function capable of serializing 
	   * the specific data of the database
	   * @param print: pointer to generic function to debug info
	   * @return true, if successfull; false otherwise
	   *
	   * Used to save users, activities and packs to the database.
	   * *serialize* functions must be implemented by clients.
	   * *print* functions must be implemented by clients.
	   * @see User.h
	   * @see Activity.h
	   * @see Pack.h
	   */
	  static bool App_save_database(Database_T db, List_T list, 
					void * (*serialize)(Fifo_T fifo),
					void(*print)(void *data))
	  {
	      void *data;
	      if(List_isDirty(list)) {
		  /* Reopen database and rewind list */
		  Database_close(db);
		  Database_open(db, "wb");
		  List_rewind(list);
		  /* While an user exists */
		  while ( (data = List_pop(list)) != NULL ) {
		      /* Serialize object to file and remove from the list */
		      App_serialize(db, serialize, data);
		      List_remove(&list, data);
		      List_rewind(list);
		      if(print) {
			  print(data);
			  print_msg_wait("Wait\n", -1); 
		      }
		  }
		  return true;
        
	      }
	      return false;

	  }
	     #+END_SRC
       2) *file*: used to describe modules or classes.
	 #+BEGIN_SRC c
       /**
	* @file App.h
	* @author Jose Pires
	* @date 12 Jan 2019
	*
	* @brief App module containing the application logic
	*
	* It contains only two public functions:
	* 1. Init - to initialize the app's memory
	* 2. Exec - contains all application logic
	*/
	 #+END_SRC
       3) *inline*: used to describe =#defines=, parameters, class members, and
       structure and enumeration fields.
       #+BEGIN_SRC c
       #define DATABASE_USERS "user.db" /**< Database file for users */
       #+END_SRC
* Diagrams
Diagrams can be drawn using [2/2]:
- [X] draw.io
  - User mockups
  - State-machine
- [X] [[id:03c3f7e2-18cd-4956-ad92-13e4a6cc1e60][PlantUML]] (stored in Proj/diags/plantuml)
  - Sequence diagram
  - Class diagram
  - Component diagram
  - Deployment diag
  - Entity-Relationship diagram

** PlantUML
   :PROPERTIES:
   :ID:       03c3f7e2-18cd-4956-ad92-13e4a6cc1e60
   :END:
 [[https://plantuml.com/][PlantUML]] is a tool for quickly drawing diagrams from text based descriptions.
 It is specially adequate for sequence diagrams, as draw.io is not very fluid.
*** Setup [0/6]
 1) [ ] Download PlantUML from the [[https://sourceforge.net/projects/plantuml/files/plantuml.jar/download][website]]: =plantuml.jar=
 2) [ ] Place the =plantuml.jar= file into a known directory and add it to the
    path
 3) [ ] Write a diagram text file in an extension =.pu= (example input/test.pu) -
    check the user manual for this
 4) [ ] Navigate to the =input= folder using cd
 5) [ ] Generate the diagram from the terminal using:
     #+BEGIN_SRC bash
     java -jar plantuml.jar test.pu -o ../out java -jar plantuml.jar test.puput/
     #+END_SRC
 6) [ ] Check the generate png file: =output/test.png=
*** Workflow [0/4]
 1) [ ] Write a diagram text file in an extension =.pu= (example input/test.pu) -
    check the user manual for this
 2) [ ] Navigate to the =input= folder using cd
 3) [ ] Generate the diagram from the terminal using:
     #+BEGIN_SRC bash
     java -jar plantuml.jar test.pu -o ../output/
     #+END_SRC
 4) [ ] Check the generate png file: =output/test.png=
*** ✔ DONE Sequence diagrams
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-12 dom 01:13]
    :END:
**** Declaring participants
 If the keyword participant is used to declare a participant, more control on that participant is possible.

 The order of declaration will be the (default) order of display.

 Using these other keywords to declare participants will change the shape of the participant representation:
 1) actor
 2) boundary
 3) control
 4) entity
 5) database
 6) collections
 7) queue

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/decl-partic.png :exports both
   ' title PlantUML (comment)
 @startuml

 participant Participant as Foo
 actor       Actor       as Foo1
 boundary    Boundary    as Foo2
 control     Control     as Foo3
 entity      Entity      as Foo4
 database    Database    as Foo5
 collections Collections as Foo6
 queue       Queue       as Foo7
 Foo -> Foo1 : To actor 
 Foo -> Foo2 : To boundary
 Foo -> Foo3 : To control
 Foo -> Foo4 : To entity
 Foo -> Foo5 : To database
 Foo -> Foo6 : To collections
 Foo -> Foo7: To queue

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/decl-partic.png]]
**** Change arrow style
 You can change arrow style by several ways:
 1) add a final x to denote a lost message
 2) use \ or / instead of < or > to have only the bottom or top part of the arrow
 3) repeat the arrow head (for example, >> or //) head to have a thin drawing
 4) use -- instead of - to have a dotted arrow
 5) add a final "o" at arrow head
 6) use bidirectional arrow <->

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/arrow-style.png :exports both
 @startuml
 ' comments as needed
 ' lost message
 Bob ->x Alice 
 ' sync message
 Bob -> Alice 
 ' async message
 Bob ->> Alice
 Bob -\ Alice
 Bob \\- Alice
 Bob //-- Alice

 Bob ->o Alice
 Bob o\\-- Alice

 ' bidirectional message
 Bob <-> Alice
 Bob <->o Alice
 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/arrow-style.png]]

**** Grouping messages
 ([[https://plantuml.com/sequence-diagram#425ba4350c02142c][src]])

 It is possible to group messages together using the following keywords:
 1) alt/else
 2) opt
 3) loop
 4) par
 5) break
 6) critical
 7) group, followed by a text to be displayed

 It is possible to add a text that will be displayed into the header (for group,
 see next paragraph 'Secondary group label').

 The end keyword is used to close the group.

 Note that it is possible to nest groups. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/group-msg.png :exports both
   ' title PlantUML (comment)
 @startuml
 Alice -> Bob: Authentication Request

 alt successful case

     Bob -> Alice: Authentication Accepted

 else some kind of failure

     Bob -> Alice: Authentication Failure
     group My own label
     Alice -> Log : Log attack start
         loop 1000 times
             Alice -> Bob: DNS Attack
         end
     Alice -> Log : Log attack end
     end

 else Another type of failure

    Bob -> Alice: Please repeat

 end
 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/examples/seq-diag/group-msg.png]]
**** Notes on messages
 It is possible to put notes on message using the note left or note right keywords just after the message.

 You can have a multi-line note using the end note keywords. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/notes-msgs.png :exports both
 @startuml
 Alice->Bob : hello
 note left: this is a first note

 Bob->Alice : ok
 note right: this is another note

 Bob->Bob : I am thinking
 note left
 a note
 can also be defined
 on several lines
 end note
 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/notes-msgs.png]]

**** Divider or separator
  If you want, you can split a diagram using == separator to divide your diagram
  into logical steps. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/divider.png :exports both
 @startuml

 == Initialization ==

 Alice -> Bob: Authentication Request
 Bob --> Alice: Authentication Response

 == Repetition ==

 Alice -> Bob: Another authentication Request
 Alice <-- Bob: another authentication Response

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/divider.png]]

**** Lifeline activation and destruction
 The =activate= and =deactivate= are used to denote participant activation.

 Once a participant is activated, its lifeline appears.

 The activate and deactivate apply on the previous message.

 The =destroy= denote the end of the lifeline of a participant. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/lifeline.png :exports both
 @startuml
 participant User

 User -> A: DoWork
 activate A

 A -> B: << createRequest >>
 activate B

 B -> C: DoWork
 activate C
 C --> B: WorkDone
 destroy C

 B --> A: RequestCreated
 deactivate B

 A -> User: Done
 deactivate A

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/lifeline.png]]

**** Participant creation
  You can use the =create= keyword just before the first reception of a message
  to emphasize the fact that this message is actually creating this new object. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/partic-creation.png :exports both
 @startuml
 Bob -> Alice : hello

 create Other
 Alice -> Other : new

 create control String
 Alice -> String
 note right : You can also put notes!

 Alice --> Bob : ok

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/partic-creation.png]]

**** Incoming and outgoing messages
 You can use incoming or outgoing arrows if you want to focus on a part of the diagram.

 Use square brackets to denote the left "[" or the right "]" side of the
 diagram. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/in-out-msgs.png :exports both
 @startuml
 [-> A: DoWork

 activate A

 A -> A: Internal call
 activate A

 A ->] : << createRequest >>

 A<--] : RequestCreated
 deactivate A
 [<- A: Done
 deactivate A
 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/in-out-msgs.png]]

**** Anchors and duration
  With =teoz= it is possible to add anchors to the diagram and use the anchors to
  specify duration time. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/anchors-duration.png :exports both
 @startuml
 !pragma teoz true

 {start} Alice -> Bob : start doing things during duration
 Bob -> Max : something
 Max -> Bob : something else
 {end} Bob -> Alice : finish

 {start} <-> {end} : some time

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/anchors-duration.png]]


 You can use the -Pcommand-line option to specify the pragma:
 #+BEGIN_SRC bash
 java -jar plantuml.jar -Pteoz=true
 #+END_SRC

**** Participants encompass
 It is possible to draw a box around some participants, using box and end box commands.

 You can add an optional title or a optional background color, after the box
 keyword. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/partic-encompass.png :exports both
 @startuml

 box "Internal Service" #LightBlue
 participant Bob
 participant Alice
 end box
 participant Other

 Bob -> Alice : hello
 Alice -> Other : hello

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/partic-encompass.png]]


**** Remove foot boxes
       You can use the =hide footbox= keywords to remove the foot boxes of the
       diagram. 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/remove-foot-box.png :exports both
 @startuml

 hide footbox
 title Foot Box removed

 Alice -> Bob: Authentication Request
 Bob --> Alice: Authentication Response

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/remove-foot-box.png]]

**** Style =strictuml=
 To be conform to strict UML (for arrow style: emits triangle rather than sharp
 arrowheads), you can use: 

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/strict-uml.png :exports both
 @startuml
 skinparam style strictuml
 Bob -> Alice : hello
 Alice -> Bob : ok
 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/strict-uml.png]]

**** Color a group message
 It is possible to color a group message: 
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/color-group-msg.png :exports both
 @startuml
 Alice -> Bob: Authentication Request
 alt#Gold #LightBlue Successful case
     Bob -> Alice: Authentication Accepted
 else #Pink Failure
     Bob -> Alice: Authentication Rejected
 end
 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/color-group-msg.png]]

**** Colors
 You can use specify *fill* and *line* colors either:
 1. with its standard name or CSS name
 2. using HEX value (6 digits): #RRGGBB
 3. using HEX value (8 digits) with alpha compositing or RGBA color model:
    #RRGGBBaa
 4. using short HEX value (3 digits): #RGB

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/colors.png :exports both
 @startuml
 actor Bob #Red/Yellow
 actor Alice #FF0000/FFFF00
 Alice -> Bob : hello
 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/colors.png]]
**** All together                                                 :Important:
 This example tries to combine all the most important tips stated previously.

 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/examples/all-together.png :exports both
 @startuml
 ' ---------- SETUP ----------------
 ' strict uml style and hide footboxes
 skinparam style strictuml
 hide footbox
 ' for anchors and duration this may be required (uncomment)
 ' !pragma teoz true


 ' ---------- Declaring participants
 participant Participant as Foo
 actor       Actor       as Foo1
 boundary    Boundary    as Foo2
 control     Control     as Foo3
 entity      Entity      as Foo4
 database    Database    as Foo5
 collections Collections as Foo6
 queue       Queue       as Foo7
 Foo -> Foo1 : To actor 
 Foo -> Foo2 : To boundary
 Foo -> Foo3 : To control
 Foo -> Foo4 : To entity
 Foo -> Foo5 : To database
 Foo -> Foo6 : To collections
 Foo -> Foo7: To queue

 ' -------- Grouping messages ------------------
 ' divider or separator
 ' Encompass actors
 ' add colors to cases
 ' add notes
 == Initialization ==

 box "Internal Service" #LightBlue
 participant Bob
 participant Alice
 end box
 Alice -> Bob: Authentication Request
 alt#Gold #LightBlue Successful case
     Bob -> Alice: Authentication Accepted
     note left: this is a first note
 else #Pink Failure
     Bob -> Alice: Authentication Rejected
     note right: this is a 2nd note
 end

 == Repetition ==

 Alice -> Bob: Another authentication Request
 Alice <-- Bob: another authentication Response


 Alice -> Bob: Authentication Request

 alt successful case

     Bob -> Alice: Authentication Accepted

 else some kind of failure

     Bob -> Alice: Authentication Failure
     group My own label
     Alice -> Log : Log attack start
         loop 1000 times
             Alice -> Bob: DNS Attack
         end
     Alice -> Log : Log attack end
     end

 else Another type of failure

    Bob -> Alice: Please repeat

 ' ---------- Anchors and duration
 {start} Alice -> Bob : start doing things during duration
 Bob -> Max : something
 Max -> Bob : something else
 {end} Bob -> Alice : finish

 {start} <-> {end} : some time

 ' --------- Incoming and outgoing messages
 [-> A: DoWork

 activate A

 A -> A: Internal call
 activate A

 A ->] : << createRequest >>

 A<--] : RequestCreated
 deactivate A
 [<- A: Done
 deactivate A

 ' -------  Participant creation ---------
 Bob -> Alice : hello

 create Other
 Alice -> Other : new

 create control String
 Alice -> String
 note right : You can also put notes!

 Alice --> Bob : ok

 '-------- Lifeline activation/deactivation
 participant User

 User -> A: DoWork
 activate A

 A -> B: << createRequest >>
 activate B

 B -> C: DoWork
 activate C
 C --> B: WorkDone
 destroy C

 B --> A: RequestCreated
 deactivate B

 A -> User: Done
 deactivate A

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/examples/all-together.png]]
**** Mine (to generate report)                           :noexport:Important:
     :PROPERTIES:
     :ID:       6e44c5fa-06a8-40bb-bef2-b1fbca2964fb
     :END:

 *Interaction mode*
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/output/seq-local-interaction-mode.png
   @startuml
   ' ---------- SETUP ----------------
   ' strict uml style and hide footboxes
   skinparam style strictuml
   hide footbox
   ' for anchors and duration this may be required (uncomment)
   ' !pragma teoz true

   ' ---------- Declaring participants
   ' participant Participant as Foo
   actor User
   box "MDO-L" #LightBlue
   boundary "Gesture Recognition Engine" as GRE
   control "UI Engine" as UIE
   actor "Local System \nBack-End" as LS
   endbox
   ' entity      Entity      as Foo4
   ' database    Database    as Foo5
   ' collections Collections as Foo6
   ' queue       Queue       as Foo7
   ' Foo -> Foo1 : To actor 
   ' Foo -> Foo2 : To boundary
   ' Foo -> Foo3 : To control
   ' Foo -> Foo4 : To entity
   ' Foo -> Foo5 : To database
   ' Foo -> Foo6 : To collections
   ' Foo -> Foo7: To queue

   ' async message
   == Activate camera feed ==
   User ->> LS: User in range
   activate User
   activate LS
   LS -> LS: activate camera

   par
     loop while (user in range && ! user_timeout)
	 LS -> UIE: grab frame from camera and display it on window
	 activate UIE
	 UIE -> User: visual feedback
     end
     == Identify User gesture ==
     User ->> GRE: gesture
     activate GRE
     GRE -> LS: gesture recognized
     deactivate GRE
     LS -> LS: process gesture callback
     == Multimedia mode ==
     alt Select Image Filter
     LS -> UIE: show Image Filter view
     UIE -> User: visual feedback
     ref over User, GRE, UIE, LS: Image Filter
 ' -------
     else Take Pic
     LS -> UIE: show Pic view
     UIE -> User: visual feedback
     ref over User, GRE, UIE, LS: Picture mode
 ' -------
     else Create GIF
     LS -> UIE: show GIF view
     UIE -> User: visual feedback
     ref over User, GRE, UIE, LS: GIF mode
     '' LS -> LS: process gesture \ncallback
     '' LS -> UIE: provide output
     '' UIE -> User: visual feedback
     ' end alt
     end 
 ' end par
   end
		

   @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/output/seq-local-interaction-mode.png]]

 *Remote client*
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/output/seq-rc.png
   @startuml
   ' ---------- SETUP ----------------
   ' strict uml style and hide footboxes
   skinparam style strictuml
   hide footbox
   ' for anchors and duration this may be required (uncomment)
   ' !pragma teoz true

   ' ---------- Declaring participants
   ' participant Participant as Foo
  
   actor User
   box "MDO-RC" #LightBlue
   boundary "UI" as UI
   control "UI Engine" as UIE
   actor "Remote Client \nBack-End" as RC
   endbox
   box "MDO-RS" #f9db8f
   actor "Remote Server" as RS
   database "User DB" as UserDB
   endbox
   ' entity      Entity      as Foo4
   ' database    Database    as Foo5
   ' collections Collections as Foo6
   ' queue       Queue       as Foo7

   ' async message
   == Application start ==
   activate User
   User ->> UI: starts app 
   deactivate User
   activate UI
   UI -> User: Show Login view
   deactivate UI
   activate User
 ''
   == Login ==
   activate User
   User ->> UI: input username and password
   UI -> User: visual feedback
   User ->> UI: User presses Login
 ''
   deactivate User
   activate UI
   UI -> UIE: login_btn_pressed
   deactivate UI
   activate UIE
   UIE -> RC : login(username, pass)
   deactivate UIE
   activate RC
   RC -> RC : Encrypt password
   RC ->> RS : send(username, pass_crypt)
   RS -> UserDB : transaction(username, pass_crypt)
 '' DB transaction
   alt transaction success
     UserDB -> RS: User info
     RS ->> RC: User info
     RC -> RC: check type of User
     alt Admin user
     RC -> UIE: admin user
     UIE -> UI: admin_view
     UI -> User: Show admin view
     ref over RC, UIE, UI, User: Admin
     else Brand user
     RC -> UIE: brand user
     UIE -> UI: brand_view
     UI -> User: Show brand view
     ref over RC, UIE, UI, User: Brand
     end
   else failure
   UserDB -> RS: empty
   end
 ''
 ''  == User Authentication ==
 ''  alt Admin
 ''    UIE ->> RC : Send DBs relative to admin
 ''    RC ->> User : Show Main Menu
 ''    alt Users
 ''      User ->> RC : Manage Useres
 ''      RC ->> UIE : Send changes
 ''      UIE ->> UIE : Update data
 ''    else Statistics
 ''      User ->> RC : Watch Statistics
 ''      RC ->> User : Show Statistics
 ''    else Ads To Activate
 ''      User ->> RC : Download Videos, Accept/Deny Ads
 ''      RC ->> UIE : Send Changes
 ''      UIE ->> UIE : Update data
 ''    else Logout
 ''      User ->> RC : Logout
 ''      RC ->> RC : Quit
 ''    end
 '    
 ''  else Brand
 ''    UIE ->> RC : Sends DBs relative to the brand 
 ''    RC ->> User : Show Main Menu
 ''    alt Notifications
 ''      User ->> RC : See notifications
 ''      RC ->> User : Show notifications
 ''    else Rented
 ''      User ->> RC : See Rented Ads
 ''      RC ->> User : Show statistics of Rented Ads
 ''    else To Rent
 ''      User ->> RC : Upload Videos, choose conditions and fragrancy
 ''      RC ->> UIE : Send Changes
 ''      UIE ->> UIE : Update data
 ''    else Logoudat
 ''      User ->> RC : Logout
 ''      RC ->> RC : Quit
 ''    end
 ''  end

 ' =========================== ZE das couves ==============================='
 '  LS -> UIE: show Image Filter view
 '  UIE -> User: visual feedback
 '  ref over User, GRE, UIE, LS: Image Filter
 ' -------
 '    else Take Pic
 '    LS -> UIE: show Pic view
 '    UIE -> User: visual feedback
 '    ref over User, GRE, UIE, LS: Picture mode
 ' -------
 '    else Create GIF
 '    LS -> UIE: show GIF view
 '    UIE -> User: visual feedback
 '    ref over User, GRE, UIE, LS: GIF mode
 '    '' LS -> LS: process gesture \ncallback
 '    '' LS -> UIE: provide output
 '    '' UIE -> User: visual feedback
 '    ' end alt
 '    end 
 ' end par
 '  end
		

   @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/output/seq-rc.png]]

 *Normal mode*
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/output/seq-local-normal-mode.png
   @startuml
   ' ---------- SETUP ----------------
   ' strict uml style and hide footboxes
   skinparam style strictuml
   hide footbox
   ' for anchors and duration this may be required (uncomment)
   ' !pragma teoz true

   ' ---------- Declaring participants
   ' participant Participant as Foo
   ''actor User
   box "MDO-L" #LightBlue
   ''boundary "Gesture Recognition Engine" as GRE
   ''control "UI Engine" as UIE
   actor "Local System Back-End" as LS
   endbox
   ' entity      Entity      as Foo4
   ' database    Database    as Foo5
   ' collections Collections as Foo6
   ' queue       Queue       as Foo7

   ' async message
   activate LS
   LS -> LS: Ads time
   LS -> LS: get video, audio and fragrance from internal DB
   par
   == Video playback ==
     loop while (! ads_time_stop)
	 LS -> LS: get next video from videos playback queue
	 LS -> LS: play video
     end
     == Fragrance diffusion ==
     loop while(1)
	 LS -> LS: diffuse := (get next start and stop times)
	 alt if(! diffuse)
	   break
	   end
	 else diffuse
	   loop while(1)
	     alt if(start_time)
	       LS -> LS: start diffusion
	     else if(stop_time)
	       LS -> LS: stop diffusion
	     else idle
	       LS -> LS: sleep
	       'end alt
	     end
	     ' end while(1)
	   end
	   'end diffuse'
	 end
     end
 ' end par
   end
		

   @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/output/seq-local-normal-mode.png]]

 *Multimedia mode: Select filter*
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/output/seq-local-multimedia-mode-sel-filt.png
   @startuml
   ' ---------- SETUP ----------------
   ' strict uml style and hide footboxes
   skinparam style strictuml
   hide footbox
   ' for anchors and duration this may be required (uncomment)
   ' !pragma teoz true

   ' ---------- Declaring participants
   ' participant Participant as Foo
   actor User
   box "MDO-L" #LightBlue
   boundary "Gesture Recognition Engine" as GRE
   control "UI Engine" as UIE
   actor "Local System Back-End" as LS
   endbox
   actor "Image Filtering APIs" as IFA
   ' entity      Entity      as Foo4
   ' database    Database    as Foo5
   ' collections Collections as Foo6
   ' queue       Queue       as Foo7

   ' async message
 ''== Image filter ==
 activate User
 User ->> GRE: select filter gesture
 deactivate User
 activate GRE
 GRE -> UIE: select filter gesture recognized
 deactivate GRE
 activate UIE
 UIE -> LS: select_filt
 deactivate UIE
 activate LS
 LS -> LS: apply_facial_detection
 LS -> LS: sel_filt(filt)
 group Apply filter
     loop while (! filter_cancel && ! filter_accept)
     ''ref over LS, IFA, UIE, User: apply filter
	 LS -> IFA: filter_selected
	 deactivate LS
	 activate IFA
	 IFA ->> LS: apply filter
	 deactivate IFA
	 activate LS
	 LS -> UIE: filter applied
	 deactivate LS
	 activate UIE
	 UIE -> User: show filter applied
	 deactivate UIE
	 activate User
     end
 end
 alt filter_accept
     activate User
     User ->> GRE: accept filter gesture
     deactivate User
     activate GRE
     GRE -> UIE: accept filter gesture recognized
     deactivate GRE
     activate UIE
     UIE -> LS: filter_accepted
     deactivate UIE
     activate LS
     LS -> LS: apply filter
     deactivate LS
     par
     ref over User, IFA: Interaction mode
     ref over User, IFA: Apply filter
     end
 else filter_cancel
     activate User
     User ->> GRE: cancel filter gesture
     deactivate User
     activate GRE
     GRE -> UIE: cancel filter gesture recognized
     deactivate GRE
     activate UIE
     UIE -> LS: filter_canceled
     deactivate UIE
     activate LS
     LS -> LS: cancel filter
     deactivate LS
     ref over User, LS: Interaction mode
 end
   @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/output/seq-local-multimedia-mode-sel-filt.png]]

 *Multimedia mode: Take pic*
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/output/seq-local-multimedia-mode-take-pic.png
   @startuml
   ' ---------- SETUP ----------------
   ' strict uml style and hide footboxes
   skinparam style strictuml
   hide footbox
   ' for anchors and duration this may be required (uncomment)
   ' !pragma teoz true

   ' ---------- Declaring participants
   ' participant Participant as Foo
   actor User
   box "MDO-L" #LightBlue
   boundary "Gesture Recognition Engine" as GRE
   control "UI Engine" as UIE
   actor "Local System Back-End" as LS
   endbox
 ''  actor "Image Filtering APIs" as IFA
   ' entity      Entity      as Foo4
   ' database    Database    as Foo5
   ' collections Collections as Foo6
   ' queue       Queue       as Foo7

   ' async message
 ''== Take Pic ==
 [->> LS: Picture mode initiated
 activate LS
 LS -> LS: Start pic timer
 loop while (! pic_timer_elapsed)
     LS -> UIE: time_remaining
     deactivate LS
     activate UIE
     UIE -> User: show time remaining
     deactivate UIE
     activate User
 end
 deactivate User
 activate LS
 LS -> LS: store picture
 deactivate LS
   @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/output/seq-local-multimedia-mode-take-pic.png]]

 *Multimedia mode: Create GIF*
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/output/seq-local-multimedia-mode-create-gif.png
   @startuml
   ' ---------- SETUP ----------------
   ' strict uml style and hide footboxes
   skinparam style strictuml
   hide footbox
   ' for anchors and duration this may be required (uncomment)
   ' !pragma teoz true

   ' ---------- Declaring participants
   ' participant Participant as Foo
   actor User
   box "MDO-L" #LightBlue
   boundary "Gesture Recognition Engine" as GRE
   control "UI Engine" as UIE
   actor "Local System Back-End" as LS
   endbox
 ''  actor "Image Filtering APIs" as IFA
   ' entity      Entity      as Foo4
   ' database    Database    as Foo5
   ' collections Collections as Foo6
   ' queue       Queue       as Foo7

   ' async message
 ''== Create GIF ==
 [->> LS: GIF mode initiated
 activate LS
 LS -> LS: Start GIF setup timer
 loop while (! gif_setup_timer_elapsed)
     LS -> UIE: time_remaining
     deactivate LS
     activate UIE
     UIE -> User: show time remaining
     deactivate UIE
     activate User
 end
 deactivate User
 LS -> LS: Start GIF operation timer
 loop while (! gif_oper_timer_elapsed)
     LS -> UIE: time_remaining
     deactivate LS
     activate UIE
     UIE -> User: show time remaining in a dial
     deactivate UIE
     activate User
 end
 deactivate User
 activate LS
 LS -> LS: store GIF
 deactivate LS
   @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/output/seq-local-multimedia-mode-create-gif.png]]

 *Sharing mode*
 #+BEGIN_SRC plantuml :file diags/plantuml/seq-diag/output/seq-local-sharing-mode.png
   @startuml
   ' ---------- SETUP ----------------
   ' strict uml style and hide footboxes
   skinparam style strictuml
   hide footbox
   ' for anchors and duration this may be required (uncomment)
   ' !pragma teoz true

   ' ---------- Declaring participants
   ' participant Participant as Foo
   actor User
   box "MDO-L" #LightBlue
   boundary "Gesture Recognition Engine" as GRE
   control "UI Engine" as UIE
   actor "Local System Back-End" as LS
   endbox
   actor "Social Media Servers" as SMS
 ''  actor "Image Filtering APIs" as IFA
   ' entity      Entity      as Foo4
   ' database    Database    as Foo5
   ' collections Collections as Foo6
   ' queue       Queue       as Foo7

   ' async message
 ''== Sharing mode ==
 == Social media selection ==
 activate User
 User ->> GRE: select SM gesture
 deactivate User
 activate GRE
 GRE -> UIE: select SM gesture recognized
 deactivate GRE
 activate UIE
 UIE -> LS: sm_selected(sm)
 deactivate UIE
 activate LS
 LS -> LS: configure SM platform
 LS -> LS: attachment = last multimedia file
 LS -> UIE: post_edit
 deactivate LS
 activate UIE
 UIE -> User: show Post Edit view
 deactivate UIE
 activate User
 ''deactivate User
 == Post editing ==
 loop while ( !share_post && !share_cancel)
     ''activate User
     User -> GRE: character selected gesture
     deactivate User
     activate GRE
     GRE -> UIE: char selected gesture recognized
     deactivate GRE
     activate UIE
     UIE -> UIE: get_input(char)
     UIE -> User: show feedback
     deactivate UIE
     activate User
 end
 == Share decision ==
 alt share_post
     activate User
     User ->> GRE: share post gesture
     deactivate User
     activate GRE
     GRE -> UIE: share post gesture recognized
     deactivate GRE
     activate UIE
     UIE -> LS: post_share(message)
     deactivate UIE
     ref over User, LS: share_post
 else share_cancel
     activate User
     User ->> GRE: cancel post gesture
     deactivate User
     activate GRE
     GRE -> UIE: cancel post gesture recognized
     deactivate GRE
     activate UIE
     UIE -> LS: cancel_share
     deactivate UIE
     ref over User, LS: interaction mode
     ''activate LS
 end
 ''deactivate User
 == Share Post ==
 group share_post
 ''activate LS
 LS -> LS ++: share_post(SM, message, attachment)
 LS ->> SMS: login
 deactivate LS
 activate SMS
 return login_status
 deactivate SMS
 activate LS
 alt if (login_status == fail)
     ref over LS, SMS: share_fail
     else success
     LS ->> SMS: post_status = send(message, attachment)
     deactivate LS
     activate SMS
     return post_status
     activate LS
     alt if(post_status == fail)
	 ref over LS, SMS: share_fail
	 deactivate LS
     end
 end
 alt share_success
     activate LS
     LS -> UIE: share_success
     deactivate LS
     activate UIE
     UIE -> User: show Share Success view
     deactivate UIE
 else share_fail
     activate LS
     LS -> UIE: share_fail
     deactivate LS
     activate UIE
     UIE -> User: show Share Failure view
     deactivate UIE
     activate User
 end
 end
   @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/seq-diag/output/seq-local-sharing-mode.png]]
*** ✔ DONE Component diagrams
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-13 seg 21:27]
    :END:
You can use component diagrams to model the software architecture of a
system. Component diagrams provide a view of the physical software components in
the system, their interfaces, and their dependencies. [[https://www.ibm.com/docs/en/rsar/9.5?topic=diagrams-creating-component][src]]

**** Components
Components must be bracketed.

You can also use the component keyword to define a component. And you can define
an alias, using the as keyword. This alias will be used later, when defining
relations. 

  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/components.png
@startuml

[First component]
[Another component] as Comp2
component Comp3
component [Last\ncomponent] as Comp4

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/components.png]]

**** UML 2
 UML 2 is the default now.

 #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/uml2-example.png
 @startuml

 interface "Data Access" as DA

 DA - [First Component]
 [First Component] ..> HTTP : use

 @enduml
 #+END_SRC

 #+RESULTS:
 [[file:diags/plantuml/component-diag/output/uml2-example.png]]
**** Skinparam
  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/skinparam1.png
 @startuml

 skinparam interface {
   backgroundColor RosyBrown
   borderColor orange
 }

 skinparam component {
   FontSize 13
   BackgroundColor<<Apache>> Pink
   BorderColor<<Apache>> #FF6655
   FontName Courier
   BorderColor black
   BackgroundColor gold
   ArrowFontName Impact
   ArrowColor #FF6655
   ArrowFontColor #777777
 }

 () "Data Access" as DA
 Component "Web Server" as WS << Apache >>

 DA - [First Component]
 [First Component] ..> () HTTP : use
 HTTP - WS

 @enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/skinparam1.png]]


  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/skinparam2.png
@startuml
[AA] <<static lib>>
[BB] <<shared lib>>
[CC] <<static lib>>

node node1
node node2 <<shared node>>
database Production

skinparam component {
    backgroundColor<<static lib>> DarkKhaki
    backgroundColor<<shared lib>> Green
}

skinparam node {
borderColor Green
backgroundColor Yellow
backgroundColor<<shared node>> Magenta
}
skinparam databaseBackgroundColor Aqua

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/skinparam2.png]]
**** Grouping components
You can use several keywords to group components and interfaces together:
1) package
2) node
3) folder
4) frame
5) cloud
6) database

  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/group-components.png
@startuml

package "Some Group" {
  HTTP - [First Component]
  [Another Component]
}

node "Other Groups" {
  FTP - [Second Component]
  [First Component] --> FTP
}

cloud {
  [Example 1]
}

database "MySql" {
  folder "This is my folder" {
    [Folder 3]
  }
  frame "Foo" {
    [Frame 4]
  }
}

[Another Component] --> [Example 1]
[Example 1] --> [Folder 3]
[Folder 3] --> [Frame 4]

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/group-components.png]]
**** Full example
  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/full-example.png
@startuml

[First component]
[Another component] as Comp2
component Comp3
component [Last\ncomponent] as Comp4

 interface "Data Access" as DA

 DA - [First Component]
 [First Component] ..> HTTP : use


package "Some Group" {
  HTTP - [First Component]
  [Another Component]
}

node "Other Groups" {
  FTP - [Second Component]
  [First Component] --> FTP
}

cloud {
  [Example 1]
}

database "MySql" {
  folder "This is my folder" {
    [Folder 3]
  }
  frame "Foo" {
    [Frame 4]
  }
}

[Another Component] --> [Example 1]
[Example 1] --> [Folder 3]
[Folder 3] --> [Frame 4]

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/full-example.png]]

**** Mine (to generate report)                                    :Important:
You can use several keywords to group components and interfaces together:
1) package
2) node
3) folder
4) frame
5) cloud
6) database

*Full*
  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/component-diag-full.png
@startuml
''left to right direction
skinparam fixCircleLabelOverlapping true
''skinparam linetype ortho
''[First component]
''[Another component] as Comp2
''component Comp3
''component [Last\ncomponent] as Comp4
''
'' interface "Data Access" as DA
''
'' DA - [First Component]
'' [First Component] ..> HTTP : use

''node "Other Groups" {
''  () FTP - [Second Component]
''  [First Component] --> FTP
''}
''
''cloud {
''  [Example 1]
''}
''
''database "MySql" {
''  folder "This is my folder" {
''    [Folder 3]
''  }
''  frame "Foo" {
''    [Frame 4]
''  }
''}

'' participants definition
actor "Brand/Admin" as User

package "MDO-RC: AppManager" {
  ''HTTP - [UI]
  ''[  UI\n << Qt >>] as UI
  frame "User Interface"{
  [UI] as UI
  [UI \nEngine] as UIE
  }
  frame "DB Manager" as DBM{
  [Query\nParser] as QP
  [Response\nParser] as RP
  }
  frame "Comm Manager" as CM{
  [Comm \nStatus] as RCS
  [RC\nTx] as RCT
  [RC\nRx] as RCR
  }
  [RC Rx\nParser] as RCRP
  frame "Remote \nController" as REMCTL{
  [Cmd \nParser] as RCCP
  }
}

'' ----------- INTERACTIONS
'' MDO-RC
[User] ..> [UI]: presses
[UI] ..> [User]: feedback
[UI] --> [UIE]: UI events
[UIE] --> [UI]: updates
[UIE] --> [QP]: build db \nquery
[UIE] --> [RCT]: admin\n cmd
[UIE] --> [RCS]: status
[RCS] --> [UIE]: status
[RCS] --> [RCT]: ping
[QP] --> [UIE]: status
[QP] --> [RCT]: db query \nframe
[RCR] ..> [RCRP]: rx frame
[RCRP] ..> [RP]: db response \nframe
[RCRP] ..> [RCCP]: cmd \nresponse
[RCCP] ..> [UIE]: cmd \nfeedback
[RP] --> [UIE]: db response

package "MDO-RS: AppManager"{
  ''HTTP - [UI]
  frame "Command-Line\nInterface"{
  [CLI] as CLI
  [CLI Engine] as CLIE
  [CLI Parser] as CLIP
  }
  frame "Command Parser"{
  [Client\nParser] as RSCP
  [Local \nParser] as RSLP
  }
  frame "DB Manager" as RSDBM{
  [Query\nParser] as RSQP
  [Response\nParser] as RSRP
  }
  frame "Comm Manager" as CMS{
  [Comm \nStatus] as RSS
  [RS Tx \nClient] as RSTC
  [RS Rx \nClient] as RSRC
  [RS Tx \nLocal] as RSTL
  [RS Rx \nLocal] as RSRL
  }
}

database "MDO-RS: MySQL"{
  [DB Server] as DBS
  [DB Manager] as RSDBM2
}
'' ------------------------------

'' ----------- INTERFACES
() "tcp/ip" as sockRC_RS
() "tcp/ip" as sockRS_RC

'' MDO-RS
'' Client Connections
[RCT] ..> sockRC_RS
sockRC_RS ..> [RSRC]: send server\nframe
[RSTC] ..> sockRS_RC
sockRS_RC ..> [RCR]: send client\n frame
'' Internal logic
[RSRC] --> [RSCP]: server\n frame
[RSCP] --> [RSQP]: db query\n frame
[RSCP] --> [CLIP]: cmd\n frame
[CLIP] --> [RSTL]: local \ncmd frame
[RSQP] --> [DBS]: db query
[DBS] --> [RSRP]: db response
[RSRP] --> [RSTC]: db response\n frame
[RSDBM2] ..> [RSDBM2]: check\ntimestamps
[RSDBM2] ..> [RSRP]: local sys\nupdate
[RSRP] ..> [RSTL]: local sys\nframe 

'' ----------- INTERFACES
'' () "tcp/ip" as sockRS_L
'' () "tcp/ip" as sockL_RS
'' () "tcp/ip" as sockL_Twitter
'' () "tcp/ip" as sockL_Transfer_sh

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/component-diag-full.png]]

***** Client
  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/component-diag-rc.png
@startuml
''left to right direction
skinparam fixCircleLabelOverlapping true
'' participants definition
actor "Brand/Admin" as User

package "MDO-RC: AppManager" #lightgrey{
  ''HTTP - [UI]
  ''[  UI\n << Qt >>] as UI
  frame "User Interface" #bisque{
  [UI] as UI
  [UI \nEngine] as UIE
  }
  frame "DB Manager" as DBM #bisque{
  [Query\nParser] as QP
  [Response\nParser] as RP
  }
  frame "Comm Manager" as CM #bisque{
  [Comm \nStatus] as RCS
  [RC\nTx] as RCT
  [RC\nRx] as RCR
  }
  [RC Rx\nParser] as RCRP
  frame "Remote \nController" as REMCTL #bisque{
  [Cmd \nParser] as RCCP
  }
}

'' ----------- INTERACTIONS
'' MDO-RC
[User] .d..> [UI]: presses
[UI] .u..> [User]: feedback
[UI] --> [UIE]: UI events
[UIE] --> [UI]: updates
[UIE] --> [QP]: build db \nquery
[UIE] --> [RCT]: admin\n cmd
[UIE] --> [RCS]: status
[RCS] --> [UIE]: status
[RCS] --> [RCT]: ping
[QP] --> [UIE]: status
[QP] --> [RCT]: db query \nframe
[RCR] ..> [RCRP]: rx frame
[RCRP] ..> [RP]: db response \nframe
[RCRP] ..> [RCCP]: cmd \nresponse
[RCCP] ..> [UIE]: cmd \nfeedback
[RP] --> [UIE]: db response

package "MDO-RS: AppManager" #lightgrey{
  ''HTTP - [UI]
  frame "Comm Manager" as CMS #bisque{
  ''together{
  [Comm \nStatus] as RSS
  [RS Tx \nClient] as RSTC
  [RS Rx \nClient] as RSRC
  ''[RS Tx \nLocal] as RSTL
  ''[RS Rx \nLocal] as RSRL
  ''}
  }
}
'' ----------- INTERFACES
() "TCP/IP" as sockRC_RS
''() "tcp/ip" as sockRS_RC

'' MDO-RS
'' Client Connections
[RCT] .r.> sockRC_RS #blue
sockRC_RS .d.> [RSRC] #blue: <color:blue>send server\n<color:blue>frame
[RSTC] .u.> sockRC_RS #darkgreen
sockRC_RS .l.> [RCR] #darkgreen: <color:darkgreen>send client\n <color:darkgreen>frame

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/component-diag-rc.png]]

***** Server
  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/component-diag-rs.png
@startuml
''left to right direction
skinparam fixCircleLabelOverlapping true
''skinparam linetype ortho
''[First component]
''[Another component] as Comp2
''component Comp3
''component [Last\ncomponent] as Comp4
''
'' interface "Data Access" as DA
''
'' DA - [First Component]
'' [First Component] ..> HTTP : use

''node "Other Groups" {
''  () FTP - [Second Component]
''  [First Component] --> FTP
''}
''
''cloud {
''  [Example 1]
''}
''
''database "MySql" {
''  folder "This is my folder" {
''    [Folder 3]
''  }
''  frame "Foo" {
''    [Frame 4]
''  }
''}

'' participants definition
''actor "Brand/Admin" as User

package "MDO-RC: AppManager" #lightgrey{
  frame "Comm Manager" as CM #bisque{
  [Comm \nStatus] as RCS
  [RC\nTx] as RCT
  [RC\nRx] as RCR
  }
}

package "MDO-L: AppManager" #lightgrey{
  frame "Comm Manager" as LCM #bisque{
  [Comm \nStatus] as LCS
  [Local\nTx] as LCT
  [Local\nRx] as LCR
  }
}

package "MDO-RS: AppManager" #lightgrey{
  ''HTTP - [UI]
  frame "Command-Line\nInterface" #bisque{
  [CLI] as CLI
  [CLI \nEngine] as CLIE
  [CLI \nParser] as CLIP
  }
  frame "Command Parser" #bisque{
  [Client\nParser] as RSCP
  [Local \nParser] as RSLP
  }
  frame "DB Client" as RSDBM #bisque{
  [Query\nParser] as RSQP
  [Response\nParser] as RSRP
  }
  frame "Comm Manager" as CMS #bisque{
  [Comm \nStatus] as RSS
  [RS Tx \nClient] as RSTC
  [RS Rx \nClient] as RSRC
  [RS Tx \nLocal] as RSTL
  [RS Rx \nLocal] as RSRL
  }
}

package "MDO-RS: DB Server" #lightgrey{
  ''[DB Transaction\n Manager] as DBTM
  [DB Manager] as RSDBM2
  frame "Comm Manager" as CMDB #bisque{
  [Comm \nStatus] as DBSt
  [Server \nTx] as SERVTx
  [Server \nRx] as SERVRx
  }
  database "Databases" as DBs
}
'' ------------------------------

'' ----------- INTERFACES
() "TCP/IP" as sockRC_RS
() "TCP/IP" as sockRS_L
() "TCP/IP" as sockRS_SQL

'' MDO-RS
'' Client Connections
[RCT] ..> sockRC_RS #blue: <color:blue>send server\n<color:blue>frame
sockRC_RS ..> [RSRC] #blue
[RSTC] ..> sockRC_RS #blue: <color:blue>send client\n<color:blue>frame
sockRC_RS ..> [RCR] #blue
'' Local connections
[RSTL] .r.> sockRS_L #darkgreen: <color:darkgreen>send local\n<color:darkgreen>frame
sockRS_L .d.> [LCR] #darkgreen
[LCT] .u.> sockRS_L #darkgreen: <color:darkgreen>send server\n<color:darkgreen>frame
sockRS_L .l.> [RSRL] #darkgreen
'' Database connections
[RSQP] --> sockRS_SQL #indigo: <color:indigo>db\n<color:indigo>query
sockRS_SQL --> [SERVRx] #indigo
[SERVTx] ..> sockRS_SQL #indigo: <color:indigo>db \n<color:indigo>response 
sockRS_SQL ..> [RSRP] #indigo

'' Internal logic
[RSRC] --> [RSCP]: server\n frame
[RSCP] --> [RSQP]: db query\n frame
[RSCP] --> [CLIP]: cmd\n frame
[CLIP] --> [RSTL]: local \ncmd frame
[RSRP] --> [RSTC]: db response\n frame
[RSRP] ..> [RSTL]: local sys\nframe 
[RSRL] --> [RSLP]: local rx\nframe
[RSLP] --> [RSTC]: local cmd\nresponse

'' Server
[RSDBM2] ..> [RSDBM2]: check\ntimestamps
[SERVRx] --> [RSDBM2]: db query
[RSDBM2] --> [DBs]: db \ntransaction
[DBs] --> [RSDBM2]: db \ntransaction
[RSDBM2] --> [SERVTx]: update\n local sys\n
[RSDBM2] --> [SERVTx]: db transaction\n local sys
''[RSDBM2] --> [SERVTx]: update\n local sys\n or \ndb transaction\n local sys
''[DBTM] --> [RSDBM2]: update \nlocal sys

'' ----------- INTERFACES
'' () "tcp/ip" as sockL_Twitter
'' () "tcp/ip" as sockL_Transfer_sh

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/component-diag-rs.png]]

***** Local System
  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/component-diag-local.png
@startuml
left to right direction
skinparam fixCircleLabelOverlapping true

'' participants definition
actor "User" as User

package "MDO-RS: AppManager" #lightgrey{
  ''HTTP - [UI]
  frame "Comm Manager" as CMS #bisque{
  ''[Comm \nStatus] as RSS
  ''[RS Tx \nClient] as RSTC
  ''[RS Rx \nClient] as RSRC
  [RS Tx \nLocal] as RSTL
  [RS Rx \nLocal] as RSRL
  }
}

package "MDO-L: AppManager" as MDOLAPP #lightgrey{
  frame "Comm Manager" as LCM #bisque{
  [Comm \nStatus] as LCS
  [Local\nTx] as LCT
  [Local\nRx] as LCR
  [Web\n Tx] as WEBTx
  [Web\n Rx] as WEBRx
  }
  frame "User Interface" #bisque{
  [UI] as UI
  [UI \nEngine] as UIE
  [Gesture \nRecognition\n Engine] as GRE
  frame "User \nDetection]" as UD{
  [Ultrasonic sensor\n device driver] as USDD
  [Ultrasonic sensor\n daemon] as USD
  }
  }
  frame "Normal mode Manager" as NMM #bisque{
  [Video Manager] as VIDM
  [Audio Manager] as AUDM
  frame "Fragrance Manager" as FRAG_M #bisque{
  [Diffuser actuator\n device driver] as DDD
  [Diffuser actuator\n daemon] as DD
  }
  }
  frame "Multimedia mode Manager" #bisque{
  [Image Filter\nManager] as IFM
  [Camera\nManager] as CAM
  [GIF\nGeneration] as GIFG
  }
  frame "Sharing mode Manager" #bisque{
  [Post\nManager] as PM
  [Twitter\nManager] as TM
  }
  frame "Computer Vision framework" #bisque{
  [Face\n detection] as FD
  [Gesture\n recognition] as GR
  [Image Filter\nOverlay] as IFO
  }
  frame "App Parser" as AP #bisque{
  [Command\n Parser] as CP
  [DB\n Parser] as DP
  }
}

cloud Twitter{
[REST APIs] as REST
}

cloud Transfer.sh{
[URL proxy] as URLP
}
'' ------------------------------

'' ----------- INTERFACES
() "TCP/IP" as sockRS_L
''() "TCP/ip" as sockL_RS
() "TCP/IP" as sockL_Twitter
() "TCP/IP" as sockL_Transfer_sh
''-------------------------------

'' MDO-Local
'' Server connections
[RSTL] .u.> sockRS_L: send local\nframe 
sockRS_L .u.> [LCR]
[LCT] .d.> sockRS_L: send server\nframe 
sockRS_L .d.> [RSRL]
'' Cloud connections
[WEBTx] .d.> sockL_Twitter: twitter\ncmd
sockL_Twitter .d.> REST
[WEBTx] -d-> sockL_Transfer_sh: URL proxy\ncmd
sockL_Transfer_sh -r-> URLP
REST -u-> sockL_Twitter: twitter\nresponse
sockL_Twitter -u-> [WEBRx]
URLP -u-> sockL_Transfer_sh: URL proxy\nresponse
sockL_Transfer_sh -l-> [WEBRx]

'' ------------------ Logic
'' User detected
User ..> USDD: user\ndetected
USDD ..> USD: signal user\n detection
USD ..> UIE: signal \nuser_detected
UIE ..> NMM: stop normal\n mode
UIE ..> CAM: grab\nframe
CAM ..> UIE: camera\nframe
UIE ..> FD: face\ndetected 
FD ..> UIE: face\ndetected
UIE ..> UI: update\n UI
UI ..> User: feedback
'' User gesture
User ..> GRE: gesture
GRE ..> GR: recognize\ngesture
GR ..> GRE: gesture\ndetected
GRE ..> UIE: gesture_type
'' Normal mode
MDOLAPP --> NMM: start\n normal mode
AUDM -d-> AUDM: play\n audio
VIDM -d-> VIDM: play\n video
FRAG_M -> DD: enable\ndiffusion
DD -> DDD: actuate
'' Sharing mode
UIE ..> PM: manage\npost
PM ..> TM: add\npost
TM --> WEBTx: share\npost
WEBRx ..> TM: post\nshared
TM ..> UIE: sharing\nstatus
'' Multimedia mode
UIE ..> CAM: Take\n Pic
CAM ..> UIE: camera\n frame
UIE ..> GIFG: create\n gif
GIFG ..> UIE: gif \ngenerated
UIE ..> IFM: apply\nimg filter
IFM ..> IFO: apply \noverlay
IFO ..> IFM: img with\n overlay
IFM ..> UIE: img with\noverlay
'' Parsing
LCR ..> AP: server\n frame
AP ..> CP: cmd
CP ..> LCT: cmd\nresponse
AP ..> DP: DB\nupdate
DP ..> LCT: DB update\nstatus

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/component-diag-local.png]]

  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/component-diag-local-no-links.png
@startuml
''left to right direction
skinparam fixCircleLabelOverlapping true
skinparam package<<Invisible>> {
verticalLayout true
  borderColor Transparent
  backgroundColor Transparent
  fontColor Transparent
  stereotypeFontColor Transparent
  componentShadowing false
  rectangleShadowing false
}

'' participants definition
actor "User" as User

package "MDO-RS: AppManager"{
  ''HTTP - [UI]
  frame "Comm Manager" as CMS{
  ''[Comm \nStatus] as RSS
  ''[RS Tx \nClient] as RSTC
  ''[RS Rx \nClient] as RSRC
  [RS Tx \nLocal] as RSTL
  [RS Rx \nLocal] as RSRL
  }
}

package "MDO-L: AppManager" as MDOLAPP{
  frame "Comm Manager" as LCM{
  [Comm \nStatus] as LCS
  [Local\nTx] as LCT
  [Local\nRx] as LCR
  [Web\n Tx] as WEBTx
  [Web\n Rx] as WEBRx
  }
  frame "User Interface"{
  [UI] as UI
  [UI \nEngine] as UIE
  [Gesture \nRecognition\n Engine] as GRE
  frame "User \nDetection]" as UD{
  [Ultrasonic sensor\n device driver] as USDD
  [Ultrasonic sensor\n daemon] as USD
  }
  }
  frame "Normal mode Manager" as NMM{
  [Video Manager] as VIDM
  [Audio Manager] as AUDM
  frame "Fragrance Manager" as FRAG_M{
  [Diffuser actuator\n device driver] as DDD
  [Diffuser actuator\n daemon] as DD
  }
  }
  package dummy1 <<Invisible>> {
  frame "Multimedia mode Manager"{
  [Image Filter\nManager] as IFM
  [Camera\nManager] as CAM
  [GIF\nGeneration] as GIFG
  }
  frame "Sharing mode Manager"{
  [Post\nManager] as PM
  [Twitter\nManager] as TM
  }
  frame "Computer Vision framework"{
  [Face\n detection] as FD
  [Gesture\n recognition] as GR
  [Image Filter\nOverlay] as IFO
  }
  frame "App Parser" as AP{
  [Command\n Parser] as CP
  [DB\n Parser] as DP
  }
  }
}

cloud Twitter{
[REST APIs] as REST
}

cloud Transfer.sh{
[URL proxy] as URLP
}
'' ------------------------------

'' ----------- INTERFACES
() "tcp/ip" as sockRS_L
() "tcp/ip" as sockL_RS
() "tcp/ip" as sockL_Twitter
() "tcp/ip" as sockL_Transfer_sh
''-------------------------------

'' MDO-Local
'' Server connections
[RSTL] .u.> sockRS_L: send local\nframe 
sockRS_L .u.> [LCR]
[LCT] .d.> sockL_RS: send server\nframe 
sockL_RS .d.> [RSRL]
'' Cloud connections
[WEBTx] .d.> sockL_Twitter: twitter\ncmd
sockL_Twitter .d.> REST
[WEBTx] -d-> sockL_Transfer_sh: URL proxy\ncmd
sockL_Transfer_sh -r-> URLP
REST -u-> sockL_Twitter: twitter\nresponse
sockL_Twitter -u-> [WEBRx]
URLP -u-> sockL_Transfer_sh: URL proxy\nresponse
sockL_Transfer_sh -l-> [WEBRx]

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/component-diag-local-no-links.png]]

***** Local System Threads
  #+BEGIN_SRC plantuml :file diags/plantuml/component-diag/output/component-diag-local-threads.png
@startuml
''left to right direction
skinparam fixCircleLabelOverlapping true
'' src: https://www.augmentedmind.de/2021/01/17/plantuml-layout-tutorial-styles/
''skinparam nodesep x (where x is an integer > 0) will increase the horizontal margin
''skinparam ranksep x affects the vertical margin
''With skinparam padding x (x also being an integer > 0) you will increase the padding of every kind of element, so use it carefully, with small values.
skinparam nodesep 5
skinparam ranksep 5
skinparam padding 0.1

<style>
 ' scope to sequenceDiagram elements
 ' scope to actor elements
   component {
       FontColor Blue
       FontStyle bold
   }
   rectangle {
       FontColor Blue
       FontStyle bold
   }
</style>

'' colors
''!$highest = darkred
''!$high = %lighten($highest, 40)
''!$medium = %lighten($highest, 80)
''!$low = %lighten($highest, 120)
!$highest = red
!$high = "#orange"
!$medium = "#yellow"
!$low = "#lightgreen"

'' participants definition

  frame "Comm" as LCM #bisque{
  [LocalRx] as LCR #$highest
  [LocalTx] as LCT $high
  together {
  [FileTransfer] as FT $low
  [TwitterShare] as TS $low
  }
  }
  frame "App" as APP #bisque{
  [AppParser] as AP $high
  [CmdHandler] as CH $medium
  }
  frame "User Interface" as USIN #bisque{
  [UserDetection] as UD #$highest
  together {
  [UI] as UI $high
  [FrameGrabber] as FG $high
  }
  together {
  ''[FaceDetection] as FD $medium 
  '' ImgFilterOverlay encapsulates face detection
  [GestureRecognition] as GRE $medium
  [ImgFilterOverlay] as IFO $medium
  }
  }
  frame "Normal mode" as NMM #bisque{
  together {
  [VidMan] as VIDM $high
  [AudioMan] as AUDM $high
  }
  [FragMan] as FRAGM $medium
  }
  frame "Multimedia mode" as MMM #bisque{
  [GIFGenerator] as GIFG $low
  }
  
  rectangle "Legend - Priority" as PRIO{
  rectangle "Highest" as P0 #$highest
  rectangle "High" as P1 $high
  rectangle "Medium" as P2 $medium
  rectangle "Low" as P3 $low
  }
'' ------------------------------

'' ------------ INTERACTIONS
'' (with hidden links)
'' UI
 UD-[hidden]d-UI
 UI-[hidden]l-FG
 ''FG-[hidden]d-FD
 FG-[hidden]d-GRE
 GRE-[hidden]l-IFO
'' Normal mode
 VIDM-[hidden]l-AUDM
 AUDM-[hidden]d-FRAGM
'' Comm
 LCR-[hidden]d-LCT
 LCT-[hidden]d-FT
 FT-[hidden]l-TS
'' App
 AP--[hidden]-CH
'' Normal and Multimedia Mode
 UI-[hidden]u-LCM
 UI--[hidden]-APP
 UI--[hidden]-NMM
 UI--[hidden]-MMM
 NMM-[hidden]-MMM
'' Legend
 APP---[hidden]-PRIO
 P0-[hidden]l-P1
 P1-[hidden]l-P2
 P2-[hidden]l-P3

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/component-diag/output/component-diag-local-threads.png]]

*** ✔ DONE Deployment diagrams
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-15 qua 02:19]
    :END:
In UML, deployment diagrams model the physical architecture of a system. Deployment diagrams show the relationships between the software and hardware components in the system and the physical distribution of the processing.

Deployment diagrams, which you typically prepare during the implementation phase
of development, show the physical arrangement of the nodes in a distributed
system, the artifacts that are stored on each node, and the components and other
elements that the artifacts implement. Nodes represent hardware devices such as
computers, sensors, and printers, as well as other devices that support the
runtime environment of a system. Communication paths and deploy relationships
model the connections in the system. ([[https://www.ibm.com/docs/en/rsar/9.5?topic=diagrams-deployment][src]])

*Types of elements* ([[https://mychartguide.com/deployment-diagram/#What_are_the_elements_involved][src]]): 
1. Artifact: The artifact is the main element in a deployment diagram and
   signifies the product developed by any software. It is also often referred to
   as the information that software generates. It is symbolized with the help of
   a rectangle.
2. Association: This helps signify the correlation and message between two
   different nodes in the diagram.
3. Component: This helps signify the presence of a software element in the
   diagram and is represented with a rectangle with two tabs.
4. Dependency: This signifies the dependency and correlation of one node or
   component with that of the other in a deployment diagram. It is represented
   with dashed lines with arrows.
5. Interface: This signifies the presence of a contractual relationship in a
   diagram, especially when there are obligations involved that needs to be
   completed in a total system. It is represented with a circle.
6. Node: This signifies the main element of any kind of hardware or even
   software object in a deployment diagram that is further shown with a three
   dimensional box.
7. Stereotype: This signifies the element that is trapped inside a node which
   comes with its representation with the name bracketed by double arrows.

**** Declaring element
  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/decl-elem.png
@startuml
actor actor
actor/ "actor/"
agent agent
artifact artifact
boundary boundary
card card
circle circle
cloud cloud
collections collections
component component
control control
database database
entity entity
file file
folder folder
frame frame
hexagon hexagon
interface interface
label label
node node
package package
person person
queue queue
rectangle rectangle
stack stack
storage storage
usecase usecase
usecase/ "usecase/"
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/deploy-diag/output/decl-elem.png]]
**** Linking or arrow

  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/link-arrow.png
@startuml

cloud cloud1
cloud cloud2
cloud cloud3
cloud cloud4
cloud cloud5
cloud1 -0- cloud2
cloud1 -0)- cloud3
cloud1 -(0- cloud4
cloud1 -(0)- cloud5

@enduml
  #+END_SRC
**** Line style
  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/line-style.png
@startuml
title Bracketed line style with label
node foo
foo --> bar          : ∅
foo -[bold]-> bar1   : [bold]
foo -[dashed]-> bar2 : [dashed]
foo -[dotted]-> bar3 : [dotted]
foo -[hidden]-> bar4 : [hidden]
foo -[plain]-> bar5  : [plain]
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/deploy-diag/output/line-style.png]]
**** Nesting
  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/nesting.png
@startuml
artifact Foo1 {
  folder Foo2
}

folder Foo3 {
  artifact Foo4
}

frame Foo5 {
  database Foo6
}

cloud vpc {
  node ec2 {
    stack stack
  }
}

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/deploy-diag/output/nesting.png]]

**** Full nesting
  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/full-nesting.png
@startuml
artifact artifact {
card card {
cloud cloud {
component component {
database database {
file file {
folder folder {
frame frame {
hexagon hexagon {
node node {
package package {
queue queue {
rectangle rectangle {
stack stack {
storage storage {
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/deploy-diag/output/full-nesting.png]]
**** Style for each nested element
  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/style-nested.png
@startuml
<style>
artifact {
  BackGroundColor #ee1100
  LineThickness 1
  LineColor black
}
card {
  BackGroundColor #ff3311
  LineThickness 1
  LineColor black
}
cloud {
  BackGroundColor #ff4422
  LineThickness 1
  LineColor black
}
component {
  BackGroundColor #ff6644
  LineThickness 1
  LineColor black
}
database {
  BackGroundColor #ff9933
  LineThickness 1
  LineColor black
}
file {
  BackGroundColor #feae2d
  LineThickness 1
  LineColor black
}
folder {
  BackGroundColor #ccbb33
  LineThickness 1
  LineColor black
}
frame {
  BackGroundColor #d0c310
  LineThickness 1
  LineColor black
}
hexagon {
  BackGroundColor #aacc22
  LineThickness 1
  LineColor black
}
node {
  BackGroundColor #22ccaa
  LineThickness 1
  LineColor black
}
package {
  BackGroundColor #12bdb9
  LineThickness 1
  LineColor black
}
queue {
  BackGroundColor #11aabb
  LineThickness 1
  LineColor black
}
rectangle {
  BackGroundColor #4444dd
  LineThickness 1
  LineColor black
}
stack {
  BackGroundColor #3311bb
  LineThickness 1
  LineColor black
}
storage {
  BackGroundColor #3b0cbd
  LineThickness 1
  LineColor black
}

</style>
artifact artifact {
}
card card {
}
cloud cloud {
}
component component {
}
database database {
}
file file {
}
folder folder {
}
frame frame {
}
hexagon hexagon {
}
node node {
}
package package {
}
queue queue {
}
rectangle rectangle {
}
stack stack {
}
storage storage {
}
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/deploy-diag/output/style-nested.png]]
**** Style for all elements
  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/style-nested-all.png
@startuml
<style>
.stereo {
  BackgroundColor palegreen
}
</style>
actor actor << stereo >>
actor/ "actor/" << stereo >>
agent agent << stereo >>
artifact artifact << stereo >>
boundary boundary << stereo >>
card card << stereo >>
circle circle << stereo >>
cloud cloud << stereo >>
collections collections << stereo >>
component component << stereo >>
control control << stereo >>
database database << stereo >>
entity entity << stereo >>
file file << stereo >>
folder folder << stereo >>
frame frame << stereo >>
hexagon hexagon << stereo >>
interface interface << stereo >>
label label << stereo >>
node node << stereo >>
package package << stereo >>
person person << stereo >>
queue queue << stereo >>
rectangle rectangle << stereo >>
stack stack << stereo >>
storage storage << stereo >>
usecase usecase << stereo >>
usecase/ "usecase/" << stereo >>
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/deploy-diag/output/style-nested-all.png]]
**** Mine (to generate report)                                    :Important:
  #+BEGIN_SRC plantuml :file diags/plantuml/deploy-diag/output/deploy-diag.png
@startuml
''left to right direction
!theme plain
''<style>
''package{
''  BackgroundColor white
''}
''node{
''  BackGroundColor #lightgrey
''  LineThickness 2
''  LineColor black
''  FontStyle bold
''}
''</style>

node "Linux PC" <<Host device>> #lightblue{
package "MDO-RC: AppManager" as MDORCAPP #lightgrey {
  frame "Comm Manager" as CM #bisque{
  ''[Comm \nStatus] as RCS
  [RC\nTx] as RCT
  [RC\nRx] as RCR
  }
}

package "MDO-RS: AppManager" as MDORSAPP #lightgrey{
  ''HTTP - [UI]
  frame "DB Client" as RSDBM #bisque{
  [Query\nParser] as RSQP
  [Response\nParser] as RSRP
  }
  frame "Comm Manager" as CMS #bisque{
  ''[Comm \nStatus] as RSS
  [RS Tx \nClient] as RSTC
  [RS Rx \nClient] as RSRC
  [RS Tx \nLocal] as RSTL
  [RS Rx \nLocal] as RSRL
  }
}

package "MDO-RS: DB Server" #lightgrey{
  ''[DB Transaction\n Manager] as DBTM
  frame "Comm Manager" as CMDB #bisque{
  ''[Comm \nStatus] as DBSt
  [Server \nTx] as SERVTx
  [Server \nRx] as SERVRx
  }
  [DB Manager] as RSDBM2
  database "Databases" as DBs
}
}

node "Raspberry Pi" <<Embedded device>> #lightblue{
package "MDO-L: AppManager" as MDOLAPP #lightgrey{
  frame "Comm Manager" as LCM #bisque{
  ''[Comm \nStatus] as LCS
  [Local\nTx] as LCT
  [Local\nRx] as LCR
  [Web\n Tx] as WEBTx
  [Web\n Rx] as WEBRx
  }
}
}

node "Web" <<cloud>> #lightblue{
cloud Twitter #plum{
[REST APIs] as REST
}

cloud Transfer.sh #plum{
[URL proxy] as URLP
}
}

'' ----------- INTERFACES
() "TCP/IP" as sockRS_L
() "TCP/IP" as sockL_Twitter
() "TCP/IP" as sockL_Transfer_sh
() "TCP/IP" as sockRC_RS
() "TCP/IP" as sockRS_SQL
''-------------------------------

'' MDO-Local
'' Cloud connections
[WEBTx] .u.> sockL_Twitter #darkred: <color:darkred>twitter\n<color:darkred>cmd
sockL_Twitter .u.> REST #darkred
[WEBTx] -d-> sockL_Transfer_sh: URL proxy\ncmd
sockL_Transfer_sh -d-> URLP
REST --> sockL_Twitter #darkred: <color:darkred>twitter\n<color:darkred>response
sockL_Twitter --> [WEBRx] #darkred
URLP --> sockL_Transfer_sh: URL proxy\nresponse
sockL_Transfer_sh --> [WEBRx]
'' MDO-RS
'' Client Connections
[RCT] ..> sockRC_RS #blue: <color:blue>send server\n<color:blue>frame
sockRC_RS ..> [RSRC] #blue
[RSTC] ..> sockRC_RS #blue: <color:blue>send client\n<color:blue>frame
sockRC_RS ..> [RCR] #blue
'' Local connections
[RSTL] .u.> sockRS_L #darkgreen: <color:darkgreen>send local\n<color:darkgreen>frame
sockRS_L .u.> [LCR] #darkgreen
[LCT] .d.> sockRS_L #darkgreen: <color:darkgreen>send server\n<color:darkgreen>frame
sockRS_L .d.> [RSRL] #darkgreen
'' Database connections
[RSQP] --> sockRS_SQL #indigo: <color:indigo>db\n<color:indigo>query
sockRS_SQL --> [SERVRx] #indigo
[SERVTx] ..> sockRS_SQL #indigo: <color:indigo>db \n<color:indigo>response 
sockRS_SQL ..> [RSRP] #indigo

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/deploy-diag/output/deploy-diag.png]]

*** ▭▭ IN-PROGRESS Class diagrams
    :LOGBOOK:
    - State "▭▭ IN-PROGRESS" from              [2021-12-14 ter 14:42]
    :END:

**** Declaring elements
   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/decl-elems.png
@startuml
abstract        abstract
abstract class  "abstract class"
annotation      annotation
circle          circle
()              circle_short_form
class           class
diamond         diamond
<>              diamond_short_form
entity          entity
enum            enum
interface       interface
''structure struct
class structName << struct >>
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/decl-elems.png]]

**** Relation between classes
   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/class-relations.png
@startuml
'' Types
'' - extension (inheritance):   <|--
'' - composition: *--
'' - Aggregation: o--
'' To have a dotted line, replace -- by ..
Class01 <|-- Class02
Class03 *-- Class04
Class05 o-- Class06
Class07 .. Class08
Class09 -- Class10
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/class-relations.png]]


**** Label on relations
It is possible to add a label on the relation, using :, followed by the text of the label.

For cardinality, you can use double-quotes "" on each side of the relation. 
   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/label-relations.png
@startuml
'' relation syntax
''  <C1Name> "<cardC1>" <relation> "<cardC2>" <C2Name> : <label>

Class01 "1" *-- "many" Class02 : contains
Class03 o-- Class04 : aggregation
Class05 --> "1" Class06

@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/label-relations.png]]

**** Adding methods
To declare fields and methods, you can use the symbol : followed by the field's
or method's name.
- The system checks for parenthesis to choose between methods and fields. 

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/add-methods.png
@startuml
class Dummy {
  String data
  void methods()
}

class Flight {
   flightNumber : Integer
   departureTime : Date
}
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/add-methods.png]]

**** Defining visibility
When you define methods or fields, you can use characters to define the
visibility of the corresponding item: 
1) - : private
2) # : protected
3) ~ : package private
4) + : public 

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/def-visibility.png
@startuml
'' 
'' 1) - : private
'' 2) # : protected
'' 3) ~ : package private
'' 4) + : public 

'' remove attributes and methods icons
skinparam classAttributeIconSize 0
class Dummy {
 -field1
 #field2
 ~method1()
 +method2()
}

@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/def-visibility.png]]

**** Abstract and Static
You can define static or abstract methods or fields using the {static} or
{abstract} modifier.
- These modifiers can be used at the start or at the end of the line.

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/abstract-static.png
@startuml
class Dummy {
  {static} String id
  {abstract} void methods()
}
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/abstract-static.png]]

**** Hide/show elements
You can parameterize the display of classes using the hide/show command.

The basic command is: hide empty members. This command will hide attributes or methods if they are empty.

Instead of empty members, you can use:
- empty fields or empty attributes for empty fields,
- empty methods for empty methods,
- fields or attributes which will hide fields, even if they are described,
- methods which will hide methods, even if they are described,
- members which will hide fields and methods, even if they are described,
- circle for the circled character in front of class name,
- stereotype for the stereotype.

You can also provide, just after the hide or show keyword:
- class for all classes,
- interface for all interfaces,
- enum for all enums,
- <<foo1>> for classes which are stereotyped with foo1,
- an existing class name.

You can use several show/hide commands to define rules and exceptions.

You can also use the show/hide commands to hide classes.
- This may be useful if you define a large !included file, and if you want to
  hide some classes after file inclusion. 

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/hide-show.png
@startuml

class Dummy1 {
  +myMethods()
}

class Dummy2 {
  +hiddenMethod()
}

class Dummy3 <<Serializable>> {
String name
}

hide members
hide <<Serializable>> circle
show Dummy1 methods
show <<Serializable>> fields


class Foo1
class Foo2
Foo2 *-- Foo1
hide Foo2

@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/hide-show.png]]

**** Specific spot
Usually, a spotted character (C, I, E or A) is used for classes, interface, enum and abstract classes.

But you can define your own spot for a class when you define the stereotype,
adding a single character and a color, like in this example: 

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/specific-spot.png
@startuml

class System << (S,#FF7700) Singleton >>
class Date << (D,orchid) >>
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/specific-spot.png]]

**** Changing arrows orientation
By default, links between classes have two dashes -- and are vertically
oriented.
- It is possible to use horizontal link by putting a single dash (or dot).  
- You can also change directions by reversing the link.

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/arrows-orientation.png
Room o- Student
Room *-- Chair

Students -o Rooms
Chairs --* Rooms

   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/arrows-orientation.png]]

**** Help on layout
Sometimes, the default layout is not perfect...

You can use together keyword to group some classes together : the layout engine will try to group them (as if they were in the same package).

You can also use hidden links to force the layout. 

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/layout-help.png
@startuml

class Bar1
class Bar2
together {
  class Together1
  class Together2
  class Together3
}
Together1 - Together2
Together2 - Together3
Together2 -[hidden]--> Bar1
Bar1 -[hidden]> Bar2


@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/layout-help.png]]

**** Splitting large files
Sometimes, you will get some very large image files.

You can use the page (hpages)x(vpages) command to split the generated image into several files :

hpages is a number that indicated the number of horizontal pages, and vpages is a number that indicated the number of vertical pages.

You can also use some specific skinparam settings to put borders on splitted
pages (see example). 

   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/split-pages.png
@startuml
' Split into 4 pages
page 2x2
skinparam pageMargin 10
skinparam pageExternalColor gray
skinparam pageBorderColor black

class BaseClass

namespace net.dummy #DDDDDD {
    .BaseClass <|-- Person
    Meeting o-- Person

    .BaseClass <|- Meeting

}

namespace net.foo {
  net.dummy.Person  <|- Person
  .BaseClass <|-- Person

  net.dummy.Meeting o-- Person
}

BaseClass <|-- net.unused.Person
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/split-pages.png]]

**** Extends and implements
   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/extends-implements.png
@startuml
class ArrayList implements List
class ArrayList extends AbstractList
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/extends-implements.png]]

**** Linking or arrow style
   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/link-arrow-style.png
@startuml
title Bracketed line style mix
class foo
class bar
bar1 : [#red,thickness=1]
bar2 : [#red,dashed,thickness=2]
bar3 : [#green,dashed,thickness=4]
bar4 : [#blue,dotted,thickness=8]
bar5 : [#blue,plain,thickness=16]

foo --> bar                             : ∅
foo -[#red,thickness=1]-> bar1          : [#red,1]
foo -[#red,dashed,thickness=2]-> bar2   : [#red,dashed,2]
foo -[#green,dashed,thickness=4]-> bar3 : [#green,dashed,4]
foo -[#blue,dotted,thickness=8]-> bar4  : [blue,dotted,8]
foo -[#blue,plain,thickness=16]-> bar5  : [blue,plain,16]
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/link-arrow-style.png]]



**** All together
   #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/all-together.png
@startuml
'' -------------------- Splitting into several files -----------
page 2x2
skinparam pageMargin 10
skinparam pageExternalColor gray
skinparam pageBorderColor black

class BaseClass

namespace net.dummy #DDDDDD {
    .BaseClass <|-- Person
    Meeting o-- Person

    .BaseClass <|- Meeting

}

namespace net.foo {
  net.dummy.Person  <|- Person
  .BaseClass <|-- Person

  net.dummy.Meeting o-- Person
}

BaseClass <|-- net.unused.Person
'' -------------------------------------------------------------

'' ------------- Link or arrow style ------------------------
title Bracketed line style mix
class foo
class bar
bar1 : [#red,thickness=1]
bar2 : [#red,dashed,thickness=2]
bar3 : [#green,dashed,thickness=4]
bar4 : [#blue,dotted,thickness=8]
bar5 : [#blue,plain,thickness=16]

foo --> bar                             : ∅
foo -[#red,thickness=1]-> bar1          : [#red,1]
foo -[#red,dashed,thickness=2]-> bar2   : [#red,dashed,2]
foo -[#green,dashed,thickness=4]-> bar3 : [#green,dashed,4]
foo -[#blue,dotted,thickness=8]-> bar4  : [blue,dotted,8]
foo -[#blue,plain,thickness=16]-> bar5  : [blue,plain,16]
'' -------------------------------------------------------------

'' ------- defining elements --------
abstract        abstract
abstract class  "abstract class"
annotation      annotation
circle          circle
()              circle_short_form
class           class
diamond         diamond
<>              diamond_short_form
entity          entity
enum            enum
interface       interface
''structure struct
class structName << struct >>
'' ------------------------------

'' ---------- Relation between classes -----------
'' Types
'' - extension (inheritance):   <|--
'' - composition: *--
'' - Aggregation: o--
'' To have a dotted line, replace -- by ..
'' relation syntax
''  <C1Name> "<cardC1>" <relation> "<cardC2>" <C2Name> : <label>
Class01 "1" *-- "many" Class02 : contains
Class03 o-- Class04 : aggregation
Class05 --> "1" Class06
'' --------------------------------------

'' -------------- Scope ---------------
'' 
'' 1) - : private
'' 2) # : protected
'' 3) ~ : package private
'' 4) + : public 
'' remove attributes and methods icons
skinparam classAttributeIconSize 0
class Dummy {
 -field1
 #field2
 ~method1()
 +method2()
}

'' hide classes if output too long
class Foo2
hide Foo2
'' --------------------------------------

'' -------------- Specific spot -----------
class System << (S,#FF7700) Singleton >>
class Date << (D,orchid) >>
'' --------------------------------------

class ArrayList implements List
class ArrayList extends AbstractList
@enduml
   #+END_SRC

   #+RESULTS:
   [[file:diags/plantuml/class-diag/output/all-together.png]]

**** Mine (to generate report)                                    :Important:

***** Test
  #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/test.png
@startuml
'' -------------------- Splitting into several files -----------
page 2x2
skinparam pageMargin 10
skinparam pageExternalColor gray
skinparam pageBorderColor black
'' --------------------------------------------------------------

'' --------------- Notes ---------------------------------------
note top of Object : In java, every class\nextends this one.

note "This is a floating note" as N1
note "This note is connected\nto several objects." as N2
Object .. N2
N2 .. ArrayList
'' --------------------------------------------------------------

'' ------- defining elements --------
abstract        abstract
abstract class  "abstract class"
annotation      annotation
circle          circle
()              circle_short_form
class           class
diamond         diamond
<>              diamond_short_form
entity          entity
enum            enum
interface       interface
''structure struct
class structName <<(S,#FF7700)struct>> {
+char[] data
}
hide <<struct>> methods
'' ------------------------------

'' ---------- Relation between classes -----------
'' Types
'' - extension (inheritance):   <|--
'' - composition: *--
'' - Aggregation: o--
'' To have a dotted line, replace -- by ..
'' relation syntax
''  <C1Name> "<cardC1>" <relation> "<cardC2>" <C2Name> : <label>
Class01 "1" *-- "many" Class02 : contains
Class03 o-- Class04 : aggregation
Class05 --> "1" Class06
'' --------------------------------------

'' -------------- Scope ---------------
'' 
'' 1) - : private
'' 2) # : protected
'' 3) ~ : package private
'' 4) + : public 
'' remove attributes and methods icons
skinparam classAttributeIconSize 0
class Dummy {
 -field1
 #field2
 ~method1()
 +method2()
}


'' -------------- Specific spot -----------
class System << (S,#FF7700) Singleton >>
class Date << (D,orchid) >>
'' --------------------------------------

class ArrayList implements List
class ArrayList extends AbstractList

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/class-diag/output/test.png]]

***** Client
  #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/class-diag-rc.png
@startuml
'' ---------------- THEMES (some only work on the web server) -----------
''!theme plain
''!theme toy
''!theme spacelab
''!theme mars (does not work)
'' -------------------- Splitting into several files -----------
page 2x2
skinparam pageMargin 10
skinparam pageExternalColor gray
skinparam pageBorderColor black
'' --------------------------------------------------------------
'' remove attributes and methods icons
skinparam classAttributeIconSize 0
'' --------------------------------------------------------------

'' ---------------------- NOTES --------------------------------
''note "This is a floating note" as N1
''note "This note is connected\nto several objects." as N2
''Object .. N2
''N2 .. ArrayList
'' --------------------------------------------------------------

'' ------- defining elements --------
''abstract        abstract
''abstract class  "abstract class"
''annotation      annotation
''circle          circle
''()              circle_short_form
''class           class
''diamond         diamond
''<>              diamond_short_form
''entity          entity
''enum            enum
''interface       interface
''structure struct
''class structName <<(S,#FF7700)struct>> {
''+char[] data
''}
''hide <<struct>> methods
'' ------------------------------
''
'' ---------- Relation between classes -----------
'' Types
'' - extension (inheritance):   <|--
'' - composition: *--
'' - Aggregation: o--
'' To have a dotted line, replace -- by ..
'' relation syntax
''  <C1Name> "<cardC1>" <relation> "<cardC2>" <C2Name> : <label>
''Class01 "1" *-- "many" Class02 : contains
''Class03 o-- Class04 : aggregation
''Class05 --> "1" Class06
'' --------------------------------------
''
''newpage
''
'' -------------- Scope ---------------
'' 
'' 1) - : private
'' 2) # : protected
'' 3) ~ : package private
'' 4) + : public 
''class Dummy {
'' -field1
'' #field2
'' ~method1()
'' +method2()
''}
''
''class ArrayList implements List
''class ArrayList extends AbstractList
'' --------------------------------------------------------------

'' ------- defining elements --------
''interface       interface
'' ------- structures
''class Ad_DB <<(S,#FF7700)struct>>
''class User_DB <<(S,#FF7700)struct>>
''class Station <<(S,#FF7700)struct>>
hide <<struct>> methods
hide enum methods
''hide <<struct>> circle
''class Ad_DB {
''+FILE vid_f
''+fragType frag
''+bool rented
''}
''
''class User_DB{
''+bool admin
''+char* name
''+char* encrypted_pass
''}
class rxFrame <<(S,lightgrey)struct>>{
enum frameT header
size_t len
void *data
}
class dbRsp <<(S,lightgrey)struct>>{
size_t len
void *data
}
class cmd <<(S,lightgrey)struct>>{
size_t len
void *data
}
' ---------

'' ------- enums
enum fragType{
COFFEE
BURGER
CHICKEN
CALM
...
}
''
enum ConnStatus{
CREATED
ONLINE
CLOSED
ERROR
}

enum Role{
BRAND
ADMIN
}

enum frameT{
DB
CMD
}
'' ---------


'' -------- Classes 
'' APP
''class AppManager{
''    - UIApp uiApp()
''    - UIWidget win()
''}
class UIApp{
    == constr/destr ==
    +UIApp()
    ~UIApp()
    == mutators ==
    void exec()
}
    note right of UIApp::exec
    executes the 
    UI event loop
    end note
class UIWindow{
    == constr/destr ==
    +UIWindow(string title)
    ~UIWindow()
    == mutators ==
    void show()
}
''class UIView{
''}
''note top of UIView
''Specific for 
''each UI view
''end note

'' AD
class Ad {
  -FILE* _vid
  -fragType _frag
  -bool _rented
  == constr/destr ==
  +Ad()
  +~Ad()
  == accessors ==
  + bool rented() const
  + enum fragType frag() const
  + FILE* video() const
  + boold compareAd(const Ad &otherAd) const
  == mutators ==
  + void rent(bool r)
  + void modifyFrag(enum fragType f)
  + void modifyVideo(FILE* f)
  == helpers ==
  + void print() const
  + void serialize(map<string col, string val> &m) 
  + void deserialize(map<string col, string val> &m) 
}
    note right of Ad::serialize
	serializes 
	object 
	for DB 
	management
    end note
'' STATION
class Station{
  -int _id
  -string _name
  -string _addr
  -list<ad> _adsToActivate
  -TimeTable* _tt
  == constr/destr ==
  +Station(string name, string addr)
  +~Station()
  == accessors ==
  + int id() const
  + string name() const
  + string addr() const
  + TimeTable* getTimeTable() const
  == mutators ==
  + void setName(string name)
  + bool setAddr(Ad ad)
  == helpers ==
  + void print() const
  + void serialize(map<string col, string val> &m) 
  + void deserialize(map<string col, string val> &m) 
}
'' Timetable (to check ad slots)
class TimeTable{
  -Date _startDate
  -Date _endDate
  -Date _curDate
  -list<ad> _adsToActivate
  == constr/destr ==
  +TimeTable(Date start, Date end)
  +TimeTable()
  +~TimeTable()
  == accessors ==
  + Date start() const
  + Date end() const
  + list<Ad> adsToActivate() const
  + list<Ad> ads() const
  == mutators ==
  + void addAd(Ad ad)
  + bool removeAd(Ad ad)
  + bool editAd(Ad ad)
  + bool findAd(Ad ad)
  == helpers ==
  + void print() const
  + void serialize(map<string col, string val> &m) 
  + void deserialize(map<string col, string val> &m) 
}
'' Date
class Date{
  - int _year
  - int _month
  - int _day
  - int _week
  - int _hour
  == constr/destr ==
  +Date(int year, int month, int day, int hour)
  +~Date()
  == accessors ==
  + Date year() const
  + Date month() const
  + Date day() const
  + Date week() const
  + Date hour() const
  == mutators ==
  + bool addDay(int day)
  + bool addYear(int year)
  + bool addMonth(int month)
  + bool addWeek(int week)
  + bool addHour(int hour)
  == helpers ==
  + void print() const
  + void serialize(map<string col, string val> &m) 
  + void deserialize(map<string col, string val> &m) 
}
'' User
class User{
  -int _id
  -string _name
  -string _email
  -string _encryptedPass
  -list<Station> _stations
  -enum Role _role;
  == constr/destr ==
  +User()
  +User(string name, string email, string pass)
  +~User()
  == accessors ==
  + int id() const
  + string name() const
  + string email() const
  + enum Role role() const
  + string encryptedPass() const
  + list<Ad> ads() const
  + list<Station> stations() const
  == mutators ==
  + bool setName(string name)
  + bool setEmail(string email)
  + bool setPass(string password)
  + bool login(string password)
  + bool logout()
  == helpers ==
  + void print() const
  + void serialize(map<string col, string val> &m) 
  + void deserialize(map<string col, string val> &m) 
  + Station findStation(const Station &otherStat)
}
class Admin{
  -list<User> _users
  == constr/destr ==
  +Admin()
  +Admin(string name, string email, string pass)
  +~Admin()
  == accessors ==
  + list<User> Users() const
  == mutators ==
  + bool setRole()
  + bool addUser(User *user)
  == helpers ==
  + bool testOper(string cmdMsg)
}
'' Crypt
class Crypt{
  -int _key
  == constr/destr ==
  +Crypt()
  +~Crypt()
  == mutators ==
  + string encrypt(string msg)
  + string decrypt(string msg)
}
note top of Crypt
Encrypts strings
requested by
multiple classes
end note
'' COMM MANAGER
class CommManager{
  -ConnStatus _status
  -int _cliSock
  -string servAddr
  -int servPort
  -mutex sockAccess
  -mutex RxAccess
  -condition_variable msgRx
  == constr/destr ==
  +CommManager(string servAddr, int servPort)
  +~CommManager()
  == accessors ==
  + ConnStatus status() const
  - string servAddr() const
  - int servPort() const
  == mutators ==
  + int run()
  - static void send_th_fcn( void *)
  - static void recv_th_fcn( void *)
  + int Send(const void *obj, size_t len);
  + int Recv(void *obj, size_t len);
  == helpers ==
  + void print() const
}
'' DB Manager
class DBManager{
  -mutex RxExtract
  -condition_variable extract
  == constr/destr ==
  +DBManager()
  +~DBManager()
  == helpers ==
  + string validateQuery(string q)
  + void extract(rxFrame *f)
}
class Parser{
  -mutex parseExec
  -condition_variable parsing
  == constr/destr ==
  +DBManager()
  +~DBManager()
  == helpers ==
  + bool exec(list<rxFrame> *l, 
      list<cmd> *c, list<dbRsp> *d)
}

'' ----------------

'' ----------- Containers

'' ----------------

'' --------------------------------------------------------------


'' ---------- Relation between classes -----------
'' Types
'' - extension (inheritance):   <|--
'' - composition: *--
'' - Aggregation: o--
Ad "1" *-u- "1" fragType: contains
UIApp "1" *- "1" UIWindow: contains
UIWindow "1" *- "1" CommManager: contains
UIWindow "1" *-- "1" DBManager: contains
UIWindow "1" *-- "1" User: contains
UIWindow "1" *-- "1" Admin: contains
CommManager "1" *- "1" ConnStatus: contains
User "1" *-- "many" Station: contains
Station "1" *- "1" TimeTable: contains
TimeTable "1" *- "many" Ad: contains
TimeTable "1" *-- "3" Date: contains
Admin -l-|> User
User *-l- Role: contains
User "1" - "1" Crypt
UIWindow "1" *-- "1" Parser: contains
rxFrame "1" *- "1" frameT: contains
UIWindow "1" *-u- "many" rxFrame: contains
UIWindow "1" *-u- "many" dbRsp: contains
UIWindow "1" *-u- "many" cmd: contains
''UIView <|-- UIWindow: inherits

'' ------------------------------

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/class-diag/output/class-diag-rc.png]]

***** Server
  #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/class-diag-rs.png
@startuml
'' ---------------- THEMES (some only work on the web server) -----------
''!theme plain
''!theme toy
''!theme spacelab
''!theme mars (does not work)
'' -------------------- Splitting into several files -----------
page 2x2
skinparam pageMargin 10
skinparam pageExternalColor gray
skinparam pageBorderColor black
'' --------------------------------------------------------------
'' remove attributes and methods icons
skinparam classAttributeIconSize 0
'' --------------------------------------------------------------
'' ------- defining elements --------
''interface       interface
'' ------- structures
''class Ad_DB <<(S,#FF7700)struct>>
''class User_DB <<(S,#FF7700)struct>>
''class Station <<(S,#FF7700)struct>>
hide <<struct>> methods
hide enum methods
''hide <<struct>> circle
''class Ad_DB {
''+FILE vid_f
''+fragType frag
''+bool rented
''}
''
''class User_DB{
''+bool admin
''+char* name
''+char* encrypted_pass
''}
class rxFrame <<(S,lightgrey)struct>>{
enum frameT header
size_t len
void *data
}
class dbRsp <<(S,lightgrey)struct>>{
size_t len
void *data
}
class cmd <<(S,lightgrey)struct>>{
size_t len
void *data
}
' ---------

enum ConnStatus{
CREATED
ONLINE
CLOSED
ERROR
}

enum Role{
BRAND
ADMIN
}

enum frameT{
DB
CMD
}
'' ---------


'' -------- Classes 
'' APP
''class AppManager{
''    - UIApp uiApp()
''    - UIWidget win()
''}
class CLIApp{
    == constr/destr ==
    +CLIApp()
    ~CLIApp()
    == mutators ==
    void exec()
}
    note right of CLIApp::exec
    executes the 
    UI event loop
    end note
class CLI{
  - vector<rxFrame> cliRxVec
  - vector<rxFrame> cliTxVec
  - vector<rxFrame> LocRxVec
  - vector<rxFrame> LocTxVec
  - vector<string> userInput
  - mutex _sockCliAccess
  - condition_variable _msgRxCli
  - mutex _sockLocAccess
  - condition_variable _msgLocCli
  == constr/destr ==
  +CLI()
  ~CLI()
  == mutators ==
  bool init()
  void show()
}

'' User
class User{
  -int _id
  -string _name
  -string _email
  -string _encryptedPass
  -enum Role _role;
  == constr/destr ==
  +User()
  +User(string name, string email, string pass)
  +~User()
  == accessors ==
  + int id() const
  + string name() const
  + string email() const
  + enum Role role() const
  + string encryptedPass() const
  == mutators ==
  + bool setName(string name)
  + bool setEmail(string email)
  + bool setPass(string password)
  + bool login(string password)
  + bool logout()
  == helpers ==
  + void print() const
}
class Admin{
  -list<User> _users
  == constr/destr ==
  +Admin()
  +Admin(string name, string email, string pass)
  +~Admin()
  == accessors ==
  + list<User> Users() const
  == mutators ==
  + bool setRole()
  + bool addUser(User *user)
  == helpers ==
  + bool enableTest(bool enable)
}
'' Crypt
class Crypt{
  -int _key
  == constr/destr ==
  +Crypt()
  +~Crypt()
  == mutators ==
  + string encrypt(string msg)
  + string decrypt(string msg)
}
note top of Crypt
Encrypts strings
requested by
multiple classes
end note
'' COMM MANAGER
class CommManager{
  -ConnStatus _status
  -int _cliSock
  -string _cliAddr
  -int _cliPort
  -int _localSock
  -string _localAddr
  -int _localPort
  -mutex _sockCliAccess
  -mutex _RxCliAccess
  -condition_variable _msgRxCli
  -mutex _sockLocAccess
  -mutex _RxLocAccess
  -condition_variable _msgRxLoc
  == constr/destr ==
  +CommManager(string addr)
  +~CommManager()
  == accessors ==
  + ConnStatus status() const
  - string cliAddr() const
  - int cliPort() const
  - string localAddr() const
  - int localPort() const
  == mutators ==
  + int run()
  - static void send_Cli_th_fcn( void* arg)
  - static void recv_Cli_th_fcn( void* arg)
  - static void send_Loc_th_fcn( void* arg) 
  - static void recv_Loc_th_fcn( void* arg)
''  + int Send(int sd, const void *obj, size_t len);
''  + int Recv(int sd, void *obj, size_t len);
}
'' DB Client
class DBClient{
  -mutex RxExtract
  -condition_variable extract
  -ConnStatus _status
  -int _dbSock
  -string _dbAddr
  -int _dbPort
  == constr/destr ==
  +DBClient()
  +~DBClient()
  == accessors ==
  + ConnStatus status() const
  - string dbAddr() const
  - int dbPort() const
  == mutators ==
  + bool connect()
  == helpers ==
  + string validateQuery(string q)
  + void extract(rxFrame *f)
}
class Parser{
  -mutex parseExec
  -condition_variable parsing
  == constr/destr ==
  +Parser()
  +~Parser()
  == helpers ==
  + bool exec(Vector<rxFrame> *l)
}
class CliParser{
  == constr/destr ==
  +CliParser()
  +~CliParser()
  == helpers ==
  + bool exec(Vector<rxFrame> *cli_l)
}
class LocalParser{
  == constr/destr ==
  +LocalParser()
  +~LocalParser()
  == helpers ==
  + bool exec(Vector<rxFrame> *loc_l)
}

'' ----------------

'' ----------- Containers

'' ----------------

'' --------------------------------------------------------------


'' ---------- Relation between classes -----------
'' Types
'' - extension (inheritance):   <|--
'' - composition: *--
'' - Aggregation: o--
CLIApp "1" *- "1" CLI: contains
CLI "1" *- "1" CommManager: contains
CLI "1" *-- "1" DBClient: contains
CLI "1" *-- "1" User: contains
CLI "1" *-- "1" Admin: contains
CommManager "1" *- "1" ConnStatus: contains
Admin -l-|> User
User *-l- Role: contains
User "1" - "1" Crypt
CLI "1" *-- "1" Parser: contains
rxFrame "1" *- "1" frameT: contains
CLI "1" *-u- "many" rxFrame: contains
CLI "1" *-u- "many" dbRsp: contains
CLI "1" *-u- "many" cmd: contains
CliParser --|> Parser
LocalParser --|> Parser
''UIView <|-- UIWindow: inherits

'' ------------------------------

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/class-diag/output/class-diag-rs.png]]

***** Local System
  #+BEGIN_SRC plantuml :file diags/plantuml/class-diag/output/class-diag-local.png
@startuml
'' ---------------- THEMES (some only work on the web server) -----------
''!theme plain
''!theme toy
''!theme spacelab
''!theme mars (does not work)
'' -------------------- Splitting into several files -----------
page 2x2
skinparam pageMargin 10
skinparam pageExternalColor gray
skinparam pageBorderColor black
'' --------------------------------------------------------------
'' remove attributes and methods icons
skinparam classAttributeIconSize 0
'' --------------------------------------------------------------
'' ------- defining elements --------
''interface       interface
'' ------- structures
''class Ad_DB <<(S,#FF7700)struct>>
''class User_DB <<(S,#FF7700)struct>>
''class Station <<(S,#FF7700)struct>>
hide <<struct>> methods
hide enum methods
''hide <<struct>> circle
''class Ad_DB {
''+FILE vid_f
''+fragType frag
''+bool rented
''}
''
''class User_DB{
''+bool admin
''+char* name
''+char* encrypted_pass
''}
''class rxFrame <<(S,lightgrey)struct>>{
''enum frameT header
''size_t len
''void *data
''}
''class dbRsp <<(S,lightgrey)struct>>{
''size_t len
''void *data
''}
''class cmd <<(S,lightgrey)struct>>{
''size_t len
''void *data
''}
' ---------
''
''enum ConnStatus{
''CREATED
''ONLINE
''CLOSED
''ERROR
''}
''
''
''enum frameT{
''DB
''CMD
''}
'' ---------
''
''
'' -------- Classes 
'' APP
''class AppManager{
''    - UIApp uiApp()
''    - UIWidget win()
''}
''class UIApp{
''    == constr/destr ==
''    +UIApp()
''    ~UIApp()
''    == mutators ==
''    void exec()
''}
''    note left of UIApp::exec
''    executes the 
''    UI event loop
''    end note
''class UI{
''  - vector<rxFrame> servRxVec
''  - vector<rxFrame> servTxVec
''  - vector<rxFrame> WebRxVec
''  - vector<rxFrame> WebTxVec
''  - mutex _sockCliAccess
''  - condition_variable _msgRxCli
''  - mutex _sockWebAccess
''  - condition_variable _msgWebCli
''  == constr/destr ==
''  +UI()
''  ~UI()
''  == mutators ==
''  bool init()
''  void show()
''}
''
'' User
''class User{
''  -int _id
''  -string _name
''  -string _email
''  -string _encryptedPass
''  -enum Role _role;
''  == constr/destr ==
''  +User()
''  +User(string name, string email, string pass)
''  +~User()
''  == accessors ==
''  + int id() const
''  + string name() const
''  + string email() const
''  + enum Role role() const
''  + string encryptedPass() const
''  == mutators ==
''  + bool setName(string name)
''  + bool setEmail(string email)
''  + bool setPass(string password)
''  + bool login(string password)
''  + bool logout()
''  == helpers ==
''  + void print() const
''}
''class Admin{
''  -list<User> _users
''  == constr/destr ==
''  +Admin()
''  +Admin(string name, string email, string pass)
''  +~Admin()
''  == accessors ==
''  + list<User> Users() const
''  == mutators ==
''  + bool setRole()
''  + bool addUser(User *user)
''  == helpers ==
''  + bool enableTest(bool enable)
''}
'' Crypt
''class Crypt{
''  -int _key
''  == constr/destr ==
''  +Crypt()
''  +~Crypt()
''  == mutators ==
''  + string encrypt(string msg)
''  + string decrypt(string msg)
''}
''note top of Crypt
''Encrypts strings
''requested by
''multiple classes
''end note
'' COMM MANAGER
''class CommManager{
''  -ConnStatus _status
''  -int _servSock
''  -string _servAddr
''  -int _servPort
''  -int _webSock
''  -string _webAddr
''  -int _webPort
''  -mutex _sockCliAccess
''  -mutex _RxCliAccess
''  -condition_variable _msgRxCli
''  -mutex _sockWebAccess
''  -mutex _RxWebAccess
''  -condition_variable _msgWebLoc
''  == constr/destr ==
''  +CommManager(string addr)
''  +~CommManager()
''  == accessors ==
''  + ConnStatus status() const
''  - string cliAddr() const
''  - int cliPort() const
''  - string webAddr() const
''  - int webPort() const
''  == mutators ==
''  + int run()
''  - static void send_Cli_th_fcn( void* arg)
''  - static void recv_Cli_th_fcn( void* arg)
''  - static void send_Web_th_fcn( void* arg) 
''  - static void recv_Web_th_fcn( void* arg)
''  + int Send(int sd, const void *obj, size_t len);
''  + int Recv(int sd, void *obj, size_t len);
''}
''abstract class Parser{
''  -mutex parseExec
''  -condition_variable parsing
''  == constr/destr ==
''  +Parser()
''  +~Parser()
''  == helpers ==
''  + bool exec(Vector<rxFrame> *v)
''}
''class DBParser{
''  == constr/destr ==
''  +CliParser()
''  +~CliParser()
''  == helpers ==
''  + bool exec(Vector<rxFrame> *db_v)
''}
''class CmdParser{
''  == constr/destr ==
''  +LocalParser()
''  +~LocalParser()
''  == helpers ==
''  + bool exec(Vector<rxFrame> *cmd_v)
''}
''class TwitterParser{
''  == constr/destr ==
''  +LocalParser()
''  +~LocalParser()
''  == helpers ==
''  + bool exec(Vector<rxFrame> *twit_v)
''}
''
''class Post{
''  -int _id
''  -string _msg
''  -FILE* _attach
''  == constr/destr ==
''  +Post()
''  +Post(string msg, FILE* attach)
''  +~Post()
''  == accessors ==
''  + int id() const
''  + string msg() const
''  + FILE* attachment() const
''  == mutators ==
''  + bool setMsg(string msg)
''  + bool attach(FILE* f)
''  == helpers ==
''  + void print() const
''}
''
''abstract class SocialMedia{
''  == constr/destr ==
''  +SocialMedia()
''  +~SocialMedia()
''  == mutators ==
''  + string addPost(Post* p)
''}
''
''class Twitter{
''  -string _consumerKey
''  -string _consumerSecret
''  -string _accessToken
''  -string _tokenSecret
''  ''-Post *_post
''  == constr/destr ==
''  +Twitter()
''  +~Twitter()
''  ''== accessors ==
''  ''+ Post* getPost() const
''  == mutators ==
''  + string addPost(Post* p)
''}
''
''class GestureRecognitionEngine{
''  -int _status
''  -Vector<Gesture> gestures
''  -mutex _gestAcess
''  -condition_variable _gestRx
''  == constr/destr ==
''  +GestureRecognitionEngine()
''  +~GestureRecognitionEngine()
''  == mutators ==
''  + int run()
''  - static void run_th_fcn( void* arg)
''}
''class UserDetectionEngine{
''  -int _status
''  -bool detected
''  -mutex _UDAcess
''  -condition_variable _UDRx
''  == constr/destr ==
''  +UserDetectionEngine()
''  +~UserDetectionEngine()
''  == mutators ==
''  + int init()
''  + int run()
''  - static void run_th_fcn( void* arg)
''}
''
''
'' ----------------
''
'' ----------- Containers
''
'' ----------------
''
'' --------------------------------------------------------------
''
''
'' ---------- Relation between classes -----------
'' Types
'' - extension (inheritance):   <|--
'' - composition: *--
'' - Aggregation: o--
''UIApp "1" *- "1" UI: contains
''UI "1" *- "1" CommManager: contains
''UI "1" *-- "1" User: contains
''UI "1" *-- "1" Admin: contains
''UI "1" *-- "1" Twitter: contains
''UI "1" *-- "1" Post: contains
''CommManager "1" *- "1" ConnStatus: contains
''Admin -l-|> User
''User *-l- Role: contains
''User "1" - "1" Crypt
''UI "1" *-- "1" DBParser: contains
''UI "1" *-- "1" CmdParser: contains
''UI "1" *-- "1" TwitterParser: contains
''UI "1" *-- "1" GestureRecognitionEngine: contains
''UI "1" *-- "1" UserDetectionEngine: contains
''rxFrame "1" *- "1" frameT: contains
''UI "1" *-u- "many" rxFrame: contains
''UI "1" *-u- "many" dbRsp: contains
''UI "1" *-u- "many" cmd: contains
''DBParser --|> Parser
''CmdParser --|> Parser
''TwitterParser --|> Parser
''Twitter --|> SocialMedia

''newpage

enum ImgFilterType{
GLASSES
CIGAR
RED_EYES
...
}
class ImageFilterEngine{
  -int _status
  -enum ImgFilterType _filt
  + vector<ImgFilterType> *_filts
  == constr/destr ==
  +ImageFilterEngine()
  +~ImageFilterEngine()
  == mutators ==
  + int init()
  + int apply()
  + bool selectFilter()
  + vector<ImgFilterType>* listFilter()
  - static void run_th_fcn( void* arg)
}

class GIFGenerator{
  -int _status
  -mutex _started
  -condition_variable _start
  -mutex _ended
  -condition_variable _end
  -string fname
  == constr/destr ==
  +GIFGenerator()
  +~GIFGenerator()
  == mutators ==
  + int init()
  + bool run(FILE* f)
  - static void run_th_fcn( void* arg)
}

class CameraEngine{
  -int _status
  -mutex _running
  -condition_variable _run
  == constr/destr ==
  +CameraEngine()
  +~CameraEngine()
  == mutators ==
  + int init()
  + int run()
  - static void run_th_fcn( void* arg)
}

class FragranceManager{
  -int _status
  -mutex _running
  -condition_variable _run
  -int _timeToRun
  -enum FragType _fragT
  == constr/destr ==
  +FragranceManager()
  +~FragranceManager()
  == mutators ==
  + int start()
  + int stop()
  + bool add(enum FragType f)
  - static void run_th_fcn( void* arg)
}

class VideoManager{
  -int _status
  -mutex _running
  -condition_variable _run
  -Vector<FILE *> _vids
  == constr/destr ==
  +VideoManager()
  +~VideoManager()
  == mutators ==
  + int start()
  + int stop()
  + bool add(FILE* vid)
  + bool clear()
  - static void run_th_fcn( void* arg)
}

class AudioManager{
  -int _status
  -mutex _running
  -condition_variable _run
  -Vector<FILE *> _audios
  == constr/destr ==
  +AudioManager()
  +~AudioManager()
  == mutators ==
  + int start()
  + int stop()
  + bool add(FILE* audio)
  + bool clear()
  - static void run_th_fcn( void* arg)
}

enum State{
NORMAL
INTERACTION
MULTIMEDIA
SHARING
}

class FSM{
  -int _status
  -enum State _curState
  -enum State _nextState
  == constr/destr ==
  +FSM()
  +~FSM()
  == mutators ==
  + int init()
  + int run(enum State s)
}

enum fragType{
COFFEE
BURGER
CHICKEN
CALM
...
}

ImageFilterEngine "1" *- "1" ImgFilterType: contains
FragranceManager "1" *- "1" fragType: contains
FSM "1" *- "1" State: contains
UI "1" *-- "1" FSM: contains
UI "1" *-- "1" ImageFilterEngine: contains
UI "1" *-- "1" CameraEngine: contains
UI "1" *-u- "1" GIFGenerator: contains
UI "1" *-u- "1" VideoManager: contains
UI "1" *-u- "1" AudioManager: contains
UI "1" *-u- "1" FragranceManager: contains
''UIView <|-- UIWindow: inherits

'' ------------------------------

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/class-diag/output/class-diag-local.png]]

*** ✔ DONE Entity-Relationship diagram
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-22 qua 03:26]
    :END:
[[https://plantuml.com/ie-diagram][src]]

Based on the Information Engineering notation.
This is an extension to the existing Class Diagram. This extension adds:
- Additional relations for the Information Engineering notation.   
- An entity alias that maps to the class diagram class.   
- An additional visibility modifier * to identify mandatory attributes. 

Otherwise, the syntax for drawing diagrams is the same as for  class
diagrams. All other features of class diagrams are also supported.


  #+BEGIN_SRC plantuml :file diags/plantuml/erd/output/complete-example.png
@startuml

' hide the spot
hide circle

' avoid problems with angled crows feet
skinparam linetype ortho

entity "Entity01" as e01 {
  *e1_id : number <<generated>>
  --
  *name : text
  description : text
}

entity "Entity02" as e02 {
  *e2_id : number <<generated>>
  --
  *e1_id : number <<FK>>
  other_details : text
}

entity "Entity03" as e03 {
  *e3_id : number <<generated>>
  --
  e1_id : number <<FK>>
  other_details : text
}

e01 ||..o{ e02
e01 |o..o{ e03

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/erd/output/complete-example.png]]

**** Mine (to generate report)                                    :Important:
  #+BEGIN_SRC plantuml :file diags/plantuml/erd/output/erd.png
@startuml

' hide the spot
hide circle

' avoid problems with angled crows feet
skinparam linetype ortho


''entity "Entity01" as e01 {
''  *e1_id : number <<generated>>
''  --
''  *name : text
''  description : text
''}
''
''entity "Entity02" as e02 {
''  *e2_id : number <<generated>>
''  --
''  *e1_id : number <<FK>>
''  other_details : text
''}
''
''entity "Entity03" as e03 {
''  *e3_id : number <<generated>>
''  --
''  e1_id : number <<FK>>
''  other_details : text
''}
''
''e01 ||..o{ e02
''e01 |o..o{ e03

'' ENTITIES DEFINITION
'' - *: indicates mandatory fields 
entity "User" as USER{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  *role : number
  *name : text
  *email : text
  *pass : text
}

entity "UserStations" as SPU{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  * <color:blue>User.id : number <<FK>>
}

entity "Station" as ST{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  *<color:blue>UserStations.id : number <<FK>>
  *name : text
  *location : text
  *IP : text
}

entity "TimeTable" as TT{
  * <color:darkred>week : number <<generated>> <<PK>>
  --
  *<color:blue>Station.id : number <<FK>>
}

entity "TimeSlot" as TS{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  *<color:blue>TimeTable.id : number <<FK>>
  *duration : int
  *cost : int
  *rented : bool
}

entity "Ad" as AD{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  *<color:blue>Fragrance.id : number <<FK>>
  *<color:blue>User.id : number <<FK>>
  *<color:blue>TimeSlot.id : number <<FK>>
}

''entity "MediaFileList" as MFL{
''  *id : number <<generated>> <<PK>>
''  --
''  *Ad.id : number <<FK>>
''}

entity "MediaFile" as MF{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  *<color:blue>Ad.id : number <<FK>> 
  *filename : text
  *filesize : text
  *filetype : text
  *mdata : blob
  description : text
}

entity "Fragrance" as FRAG{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  *<color:blue>FragranceList.id : number <<FK>>
  *name : text
  *intensity : text
  *vol_ml_max: int
  *vol_ml_level: int
  description : text
}

entity "FragranceList" as FL{
  * <color:darkred>id : number <<generated>> <<PK>>
  --
  *<color:blue>Station.id : number <<FK>>
}


'' RELATIONSHIPS
USER ||--o{ SPU
USER ||--o{ AD
SPU ||-|{ ST 
ST ||-u-|| TT 
TT ||-u-|{ TS  
TS }|-l|| AD  
''AD ||--|| MFL  
''MFL ||--|{ MF
AD ||--|{ MF
ST ||--|| FL 
FL ||-u-|{ FRAG
AD }o--|| FRAG


@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/erd/output/erd.png]]

**** Plantuml to MySQL
[[https://github.com/grafov/plantuml2mysql][src]]

I liked plantuml tool for UML diagrams but use it also for visualizing structure of relational database. This script loads plantuml class diagram and generates DDL for MySQL SQL dialect. You may define primary keys with # prefix in field name (it means protected field in PlantUML) and define index fields with + (public field in PlantUML) prefix.

Field type noted after field name as is. Also you may use comments after --.

For example class definition:
  #+BEGIN_SRC plantuml :file diags/plantuml/erd/output/plantuml2sql-example.png
@startuml

class dummy {
  Sample table.
  ==
  #id int(10) -- A comment
  field1 int(10)
  .. Comment line, ignored ..
  field2 varchar(128)
}

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/erd/output/plantuml2sql-example.png]]

will be converted to SQL:
#+BEGIN_SRC sql
CREATE TABLE IF NOT EXISTS `dummy` (
  id               INT(10) COMMENT 'A comment',
  field1           INT(10),
  field2           VARCHAR(128),
  PRIMARY KEY (id));
#+END_SRC

Text between class name and == is table description. The description of the table is mandatory. I was too lazy to check for absence of descriptions but not lazy to write them in each table of my databases.

A line starting with .. or __, used as a separator into a class definition, will be ignored.

The HTML markup in comments (after --) is stripped.

**** [[file:db.org][DB design and implementation using SQL]]                       :Important:

*** ▭▭ IN-PROGRESS Flowcharts (activity diagram)
    :LOGBOOK:
    - State "▭▭ IN-PROGRESS" from              [2021-12-22 qua 03:26]
    :END:

**** Conditionals
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/cond.png
    start      
'' You can use the command !pragma useVerticalIf on to have the tests in vertical mode:     
''  *  !pragma useVerticalIf on      

  '' style 1        
    if (Graphviz installed?) then (yes)      
      :process all\ndiagrams;      
    else (no)      
      :process only      
      __sequence__ and __activity__ diagrams;      
    endif      
    
  '' style 2
    if (color?) is (<color:red>red) then      
    :print red;      
    else       
    :print not red;      

  '' style 3
    if (counter?) equals (5) then      
    :print 5;      
    else       
    :print not 5;      

'' Elseif
    if (condition A) then (yes)      
      :Text 1;      
    elseif (condition B) then (yes)      
      :Text 2;      
      stop      
    (no) elseif (condition C) then (yes)      
      :Text 3;      
    (no) elseif (condition D) then (yes)      
      :Text 4;      
    else (nothing)      
      :Text else;      
    endif      



          
    stop      
          
    @enduml      
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/cond.png]]

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/switch-case.png
  @startuml      
  start      
    switch (test?)      
    case ( condition A )      
      :Text 1;      
    case ( condition B )       
      :Text 2;      
    case ( condition C )      
      :Text 3;      
    case ( condition D )      
      :Text 4;      
    case ( condition E )      
      :Text 5;      
    endswitch      
    stop      
    @enduml      
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/switch-case.png]]

**** Connector
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/connector.png
@startuml
start
:Some activity;
(A)
detach
(A)
:Other activity;
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/connector.png]]

**** Specification and Description Language (SDL)
By changing the final ; separator, you can set different rendering for the activity:
- |
- <
- >
- /
- \\
- ]
- }

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/sdl.png
@startuml
:Ready;
:next(o)|
:Receiving;
split
 :nak(i)<
 :ack(o)>
split again
 :ack(i)<
 :next(o)
 on several lines|
 :i := i + 1]
 :ack(o)>
split again
 :err(i)<
 :nak(o)>
split again
 :foo/
split again
 :bar\\
split again
 :i > 5}
stop
end split
:finish;
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/sdl.png]]

**** Full example

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/full-example.png
@startuml

start
:ClickServlet.handleRequest();
:new page;
if (Page.onSecurityCheck) then (true)
  :Page.onInit();
  if (isForward?) then (no)
    :Process controls;
    if (continue processing?) then (no)
      stop
    endif

    if (isPost?) then (yes)
      :Page.onPost();
    else (no)
      :Page.onGet();
    endif
    :Page.onRender();
  endif
else (false)
endif

if (do redirect?) then (yes)
  :redirect process;
else
  if (do forward?) then (yes)
    :Forward request;
  else (no)
    :Render page template;
  endif
endif

stop

@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/full-example.png]]

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/test1.png
@startuml
'-----------------------
  note right
    Login menu
    (Verifying Credentials)
  end note
'-----------------------
  start
    repeat
    repeat while(login_btn pressed?) is (No) not (Yes)
    
    if (All fields are filled?) then (Yes)  
    
        :pass_encrypt = crypt.encrypt(password)
        db_query = "select count (*), role from user where
        (username = " + username + " and pass_encrypt = " + pass_encrypt + ");";

        note left
            Encrypt the pass and 
            then query to the db
            in order to receive 
            the count of 1
        end note

        :rxFrame.header = DB
        rxFrame.len = db_query.length
        rxFrame.data = db_query
        server.send(rxFrame, rxFrame.len);

        :wait for server response (async);
      
        if(Count == 1?) then (Yes)
          if (role == ADMIN) then (Yes)
              :Show ADMIN view;
          (No) elseif (role == BRAND) then (Yes)
              :Show BRAND view;
          else (No)
            :Error logging in!;
            stop
          endif

        else (No)
            :Error logging in! Credentials are wrong;
        endif
    else (No)
        :Error! Fill all the fields;
    endif   
  stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/test1.png]]

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/test2.png
@startuml
'-----------------------
  note right
    Thread to Receive 
    from local system
  end note
'-----------------------
start
    repeat
        :nbytes = recv(sd, size, tmp_buff, 0);
        note left
            It only exits from here
            when it receives something
        end note

        if (nbytes > 0) then (Yes)
            :frame = (rxFrame*) tmp_buff;
            note right
                cast the received data
            end note
            if (nbytes == MAX_BYTES) then (Yes)
                if(Last byte is end of string?) then (No)
                    :receive rest of the data and
                    add that data to frame.data;
                else (Yes)
                endif
            else (No)
            endif
            :cli.LocRxVec.push(frame);
            note right
                push to the rx vector
            end note

            :msgLocCli = 1; 
            note right
                notify the reception
                of the data
            end note
        else (No)
        endif
        'backward
    repeat while()
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/test2.png]]


**** Mine (to generate report)                                    :Important:

***** ✔ DONE User Detection
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 20:24]
	  :END:

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-user-detect.png
@startuml
!$end_bracket = "<b> </b>"
'-----------------------
  note right
    <b>UserDetection Thread</b>
	Executed periodically to check 
	if an User was detected
  end note
'-----------------------
  start
  repeat
  (1)
  :unsigned int sender
  char msg[ MSG_MAX ]$end_bracket
  ud_rx = mq_receive(mq_id, msg, MSG_MAX, sender);
''
  note right
	consuming msg queue
	user detection events
  end note
''
  if(ud_rx == -1?) then (Y)
	(1)
	''
	note right
		error reading from 
		msg queue
		go back to start
	end note
	''
	detach
  else (N)
	:detected = calc_sliding_window(ud_rx)|
	''
	note right
		Use a sliding window
		(moving average)
		to determine if a user
		was detected
	end note
	''
	if(detected?) then (Y)
		:pthread_mutex_lock( &user_detect_mut )
		pthread_cond_signal( &user_detect_cond )
		pthread_mutex_unlock( &user_detect_mut );
	''
	note right
		Signal that the user 
		was detected to
		other threads
	end note
	''
	else (N)
	(1)
	detach
	endif
  endif
  repeat while(execution aborted?) is (N)
  ->Y;

  stop
  
'-----------------------
@enduml
  #+END_SRC
   #+RESULTS:
   [[file:diags/plantuml/flowchart/output/local-user-detect.png]]

***** ✔ DONE FrameGrabber
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 18:07]
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-frame-grabber.png
@startuml
!$end_bracket = "<b> </b>"
'-----------------------
  note right
    <b>FrameGrabber Thread</b>
	Executed periodically, accordingly to the frame rate,
	to grab frames from camera and store them
  end note
'-----------------------
  start
  repeat
	''(1)
	:cam_frame = camera_capture()
	pthread_mutex_lock (&cam_fifo_mutex)
	push cam_frame to cam_fifo
	sem_post (&cam_fifo_count)
	pthread_mutex_unlock (&cam_fifo_mutex);
	''
	note right
		producing camera frames 
		into shared FIFO and using 
		a semaphore to handle 
		multiple threads access
	end note
  repeat while(execution aborted?) is (N)
  ->Y;

  stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/local-frame-grabber.png]]

***** ✔ DONE LocalRx
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 18:07]
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-rx.png
@startuml
'-----------------------
  note right
  <b>LocalRx</b>
    Thread to receive 
    from Remote Server
  end note
'-----------------------
start
(1)
:frame_ended = 0;
    repeat
        repeat
			:nbytes = recv(sd, size, tmp_buff, 0);
			''
			note right
				blocking call
			end note

			if (nbytes > 0) then (Yes)
				:frame = (rxFrame*) tmp_buff;
				''
				note right
					cast the received data
				end note
				if (tmp_buffer[nbytes - sizeof(int)] == ACK) then (Yes)
						:frame_ended = 1;
				else (Yes)
					:receive rest of the data and
					add that data to frame.data;
				endif
			else (No)
			(1)
			detach
			endif
		repeat while(frame ended?) is (N)
		->Y;
		''if(arroz) then(Y)
		:pthread_mutex_lock( &rx_avail_mut )
		RxVec.push(frame)
		pthread_cond_signal( &rx_avail_cond )
		pthread_mutex_unlock( &rx_avail_mut );
			''
            note right
				push to the rx vector
				and notify the reception
				of the data
            end note
			''
        ''else (No)
        ''endif
        'backward
    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/local-rx.png]]

***** ✔ DONE LocalTx
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 18:07]
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/flow-local-tx.png
@startuml
'-----------------------
  note right
        <b>LocalTx</b>
  Thread to send
  to Remote Server
  end note
'-----------------------
start
(1)
	:rxFrame* frame;
    repeat
		:pthread_mutex_lock( &tx_avail_mut )
		pthread_cond_wait( &tx_avail_cond, &tx_avail_mut )
		TxVec.pop( &frame )
		pthread_mutex_unlock( &tx_avail_mut );
			''
            note right
				wait for tx_avail 
				event and pop data 
				frame from tx FIFO
            end note
			''
		(2)
		:nbytes = send(sd, size, frame, 0);
		''
		note right
			blocking call
		end note

		if (nbytes > 0) then (Yes)
		else (No)
		(2)
		detach
		endif
    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/flow-local-tx.png]]

***** ✔ DONE AppParser
      :LOGBOOK:
      - State "✔ DONE"     from              [2021-12-23 qui 22:25]
      :END:

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-app-parser.png
@startuml
'-----------------------
  note right
  <b>AppParser</b>
    Local System 
    parser execution
  end note
'-----------------------
start
	(1)
	:pthread_mutex_lock( &msgRx_mut )
	pthread_cond_wait( &msgRx_cond, &msgRx_mut )
	pthread_mutex_unlock( &msgRx_mut );
	repeat
		:pthread_mutex_lock( &RxVec_mut )
		empty = RxVec.empty()
		pthread_mutex_unlock( &RxVec_mut );
	
            if (empty) then (No)
		:pthread_mutex_lock( &RxVec_mut )
                tmp_rx = RxVec->pop()
		pthread_mutex_unlock( &RxVec_mut );
                note left
                    pop the rx frame from the
                    vector and process it
                end note
                :transform the tmp_rx->data into
                string in order to be processed;
                if(tmp_rx->header == CMD?) then (Yes)
		:cmdFrame *cmdF = (cmdFrame *) tmp_rx->data;
		    note right
			cast the rxFrame 
			data to an adFrame
		    end note
		:pthread_mutex_lock( &cmd_avail_mut )
		cmdVec.push( &adF )
		pthread_cond_signal( &cmd_avail_cond )
		pthread_mutex_unlock( &cmd_avail_mut );
                note right
		    push the cmd frame to the
		    vector so it can be processed
		    later
                end note
                (No) elseif (tmp_buff->header == AD?)
		:adFrame *adF = (adFrame *) tmp_rx->data;
		    note right
			cast the rxFrame 
			data to an cmdFrame
		    end note
		:pthread_mutex_lock( &ad_frame_avail_mut )
		adFrameVec.push( &adF )
		pthread_cond_signal( &ad_frame_avail_cond )
		pthread_mutex_unlock( &ad_frame_avail_mut );
                note right
		    push the Ad frame to the
		    vector so FileTransfer thread
		    can download media files
                end note
                else (No)
                    :Ignore data;
                endif
        else (Yes)
        endif
   repeat while()
 
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/local-app-parser.png]]

***** ✔ DONE CmdHandler
      :LOGBOOK:
      - State "✔ DONE"     from              [2021-12-24 sex 01:02]
      :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-cmd-handler.png
@startuml
'-----------------------
  note right
  <b>CmdHandler</b>
  Thread to handle commands
  end note
'-----------------------
start
	(1)
	repeat
	:pthread_mutex_lock( &cmd_avail_mut )
	pthread_cond_wait( &cmd_avail_cond, &cmd_avail_mut )
	pthread_mutex_unlock( &cmd_avail_mut );
	(2)
	''repeat
		:pthread_mutex_lock( &CmdVec_mut )
		empty = CmdVec.empty()
		pthread_mutex_unlock( &CmdVec_mut );
	
            if (empty) then (No)
		:pthread_mutex_lock( &cmdVec_mut )
                tmp_cmd = cmdVec->pop()
		pthread_mutex_unlock( &cmdVec_mut );
                note left
		pop the cmd frame from 
		the vector and process it
                end note
		:process tmp_cmd->data;
		    note left
		    process 
		    the command
		    end note
		(2)
		detach
		else (Yes)
		''break
		endif
	''repeat while()
   repeat while(execution aborted?) is (N)
   ->Y;
 
stop
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/local-cmd-handler.png]]

***** ✔ DONE TwitterShare
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 18:07]
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-twitter-share.png
@startuml
'-----------------------
  note right
        <b>TwitterShare</b>
  Thread to share
  posts on Twitter
  end note
'-----------------------
start
(1)
	:post* Post;
    repeat
		:pthread_mutex_lock( &post_avail_mut )
		pthread_cond_wait( &post_avail_cond, &post_avail_mut )
		PostVec.pop( &post )
		pthread_mutex_unlock( &post_avail_mut );
			''
            note right
				wait for pop_avail 
				event and pop data 
				frame from post FIFO
            end note
			''
		(2)
		:resp = twitter.share(post)|
		''
		note right
			<b>share post on Twitter</b>
			it uses curl and HTTP 
			POST under the hood
		end note

		:pthread_mutex_lock( &share_status_mut )
		twitter.shareStatus = resp
		pthread_cond_signal( &share_status_cond )
		pthread_mutex_unlock( &share_status_mut );

		note right
			<b>Share post status</b>
			store it and warn UI
			it is available for 
			processing
		end note

    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/local-twitter-share.png]]

***** ✔ DONE FileTransfer
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 19:11]
	  :END:

  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-file-transfer.png
@startuml
!$end_bracket = "<b> </b>"
'-----------------------
  note right
        <b>FileTransfer</b>
  Thread to download files
  from WebProxy using the
  AdFrame data
  end note
'-----------------------
start
(1)
	:adFrame* adF
	Ad *ad;
    repeat
		:pthread_mutex_lock( &ad_frame_avail_mut )
		pthread_cond_wait( &ad_frame_avail_cond, &ad_frame_avail_mut )
		adFrameVec.pop( &adF )
		pthread_mutex_unlock( &ad_frame_avail_mut );
			''
            note right
				wait for ad_frame_avail 
				event and pop data 
				frame from adFrame FIFO
            end note
			''
		(2)
		:ad = new Ad(adF->frag, adF->inten, 
                       adF->startTime, adF->duration);
			''
            note right
				construct an Ad
            end note
			''
		:construct mediaURLs string from
		mediaLen;
		:cnt = 0;
		''repeat
		:url = tokenize(mediaURLs, ',')|
		''
		note right
			<b>Tokenize</b>
			obtain individual URL 
			from CSV list
		end note
		repeat
		if(url empty?) then (Y)
		(1)
		detach
		else (N)
		(3)
		:char fname[24]$end_bracket
		char fext [ ] = ".mp4"
		fname = sprintf(fname, "ad_%d_%d.%s", 
                           Ad.id(), cnt, fext)
		res = curl.save(fname, url);
			note right
				<b>Download file</b>
				from URL into 
				local storage
			end note
			if(res == OK) then (N)
			(3)
			note right
				try again
			end note
			detach
			else (Y)
			:ad->fifo->push(strlen(fname), fname)
			cnt++;
			note right
				<b>Save reference to file</b>
				in a media FIFO
			end note
			endif

		endif
		repeat while()

    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/local-file-transfer.png]]

***** ✘ CANCELED AudioMan
	  :LOGBOOK:
	  - State "✘ CANCELED" from              [2021-12-23 qui 20:24] \\
		The VidMan thread may handle this
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-audio-man.png
@startuml
!$end_bracket = "<b> </b>"
'-----------------------
  note right
        <b>AudioMan</b>
  Thread to manage
  from WebProxy using the
  AdFrame data
  end note
'-----------------------
start
(1)
	:adFrame* adF
	Ad *ad;
    repeat
		:pthread_mutex_lock( &ad_frame_avail_mut )
		pthread_cond_wait( &ad_frame_avail_cond, &ad_frame_avail_mut )
		adFrameVec.pop( &adF )
		pthread_mutex_unlock( &ad_frame_avail_mut );
			''
            note right
				wait for ad_frame_avail 
				event and pop data 
				frame from adFrame FIFO
            end note
			''
		(2)
		:ad = new Ad(adF->frag, adF->inten, 
                       adF->startTime, adF->duration);
			''
            note right
				construct an Ad
            end note
			''
		:construct mediaURLs string from
		mediaLen;
		:cnt = 0;
		''repeat
		:url = tokenize(mediaURLs, ',')|
		''
		note right
			<b>Tokenize</b>
			obtain individual URL 
			from CSV list
		end note
		repeat
		if(url empty?) then (Y)
		(1)
		detach
		else (N)
		(3)
		:char fname[24]$end_bracket
		char fext [ ] = ".mp4"
		fname = sprintf(fname, "ad_%d_%d.%s", 
                           Ad.id(), cnt, fext)
		res = curl.save(fname, url);
			note right
				<b>Download file</b>
				from URL into 
				local storage
			end note
			if(res == OK) then (N)
			(3)
			note right
				try again
			end note
			detach
			else (Y)
			:ad->fifo->push(strlen(fname), fname)
			cnt++;
			note right
				<b>Save reference to file</b>
				in a media FIFO
			end note
			endif

		endif
		repeat while()

    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC
***** ✔ DONE VideoMan
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 20:05]
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/flow-local-video-man.png
@startuml
!$end_bracket = "<b> </b>"
'-----------------------
  note right
        <b>VideoMan</b>
  Thread to manage video
  reproduction in normal mode
  end note
'-----------------------
start
	:Ad *curAd;
	(1)
    repeat
		:sem_wait(&user_detect_status_mut)
		mode = app.Mode()
		sem_post(&app_mode_count);
''		:pthread_mutex_lock( &user_detect_not_mut )
''		pthread_cond_wait( &user_detect_not_cond, &user_detect_not_mut )
''		adFrameVec.pop( &adF )
''		pthread_mutex_unlock( &ad_frame_avail_mut );
			''
            note right
				check start 
				condition
			end note
			''
		(2)
		if(mode == NORMAL) then (N)
			'' go back to start
			(1)
			detach
		else (Y)
			'' play video
		endif
		:curAd = this->ad; 
			''
            note right
		check if there is 
		an ad to reproduce
            end note
			''
		if(curAd == NULL) then (Y)
		(1)
		detach
		else (N)
		endif
		:cnt = 0
		start_timer(&tim_on, curAd->duration);
		repeat 
		:sem_wait(&user_detect_status_mut)
		mode = app.Mode()
		sem_post(&app_mode_count);
            note right
				check stop
				condition
			end note
		if(mode == NORMAL && tim_on != elapsed ?) then (N)
			'' go back to start
			(1)
			''
			detach
		else (Y)
			'' play video
		endif
		:playVideo(ad->fifo->buff[cnt])
		cnt++;
			''
            note right
				play video
				from media list
            end note
			''
		repeat while()
    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/flow-local-video-man.png]]

***** ✔ DONE FragMan
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 20:24]
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/flow-local-frag-man.png
@startuml
!$end_bracket = "<b> </b>"
'-----------------------
  note right
        <b>FragMan</b>
  Thread to manage fragrance
  diffusion in normal mode
  end note
'-----------------------
start
	:Ad *curAd;
	(1)
    repeat
		:sem_wait(&user_detect_status_mut)
		mode = app.Mode()
		sem_post(&app_mode_count);
''		:pthread_mutex_lock( &user_detect_not_mut )
''		pthread_cond_wait( &user_detect_not_cond, &user_detect_not_mut )
''		adFrameVec.pop( &adF )
''		pthread_mutex_unlock( &ad_frame_avail_mut );
			''
            note right
				check start 
				condition
			end note
			''
		(2)
		if(mode == NORMAL) then (N)
			'' go back to start
			(1)
			detach
		else (Y)
			'' play video
		endif
		:curAd = this->ad; 
			''
            note right
		check if there is 
		an ad to reproduce
            end note
			''
		if(curAd == NULL) then (Y)
		(1)
		detach
		else (N)
		endif
			''
		:frag.calc_times(curAd->intens(), 
		                 &duration_on, &duration_off);
			''
            note right
				calculate times for
				fragrance diffusion
            end note
			''
		:start timer(&tim_on, duration_on)
		frag.enable(ON);
			''
            note right
				Start ON timer and
				actuate fragrance
            end note
			''
		repeat 
		:sem_wait(&user_detect_status_mut)
		mode = app.Mode()
		sem_post(&app_mode_count);
            note right
				check stop
				condition
			end note
		if(mode == NORMAL) then (N)
			'' go back to start
			(1)
			''
			detach
		else (Y)
			'' diffuse fragrance
		endif
		if(tim_on == elapsed?) then (Y)
		:frag.enable(OFF)
		start timer(&tim_off, duration_off);
			''
            note right
			Turn off diffusion and
			start OFF timer 
            end note
			''
		else (N)
		endif
		if(tim_off == elapsed?) then (Y)
		:start timer(&tim_on, duration_on)
		frag.enable(ON);
			''
            note right
				Start ON timer and
				actuate fragrance
            end note
			''
		else (N)
		endif
		repeat while()
    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/flow-local-frag-man.png]]

***** ✔ DONE GIFGenerator
	  :LOGBOOK:
	  - State "✔ DONE"     from              [2021-12-23 qui 20:52]
	  :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/flowchart/output/local-gif-gen.png
@startuml
!$end_bracket = "<b> </b>"
'-----------------------
  note right
        <b>GIFGenerator</b>
  Thread to generate GIFs
  end note
'-----------------------
start
	(1)
	:pthread_mutex_lock( &gif_generate_mut )
	pthread_cond_wait( &gif_generate_cond, &gif_generate_mut )
	pthread_mutex_unlock( &gif_generate_mut );
			''
            note right
				check start 
				condition
			end note
			''
    repeat
		:start timer(&tim_on, duration_on);
			''
            note right
				Start ON timer
            end note
			''
		if(tim_on == elapsed?) then (Y)
			:fname = 'gif.gif'
			:gif.save(fname,'w')
			:pthread_mutex_lock( &gif_done_mut )
			pthread_cond_wait( &gif_done_cond, &gif_done_mut )
			pthread_mutex_unlock( &gif_done_mut );
            note left
				Save gif file
				and signal that
				GIF is done
            end note
			''
			(1)
			detach
		else (N)
		endif
		:pthread_mutex_lock (&cam_fifo_mutex)
		sem_wait (&cam_fifo_count)
		pop cam_frame from cam_fifo
		pthread_mutex_unlock (&cam_fifo_mutex);
			''
            note right
				get camera frame
            end note
			''
		repeat 
		:gif.push(cam_frame);
			''
            note right
				store image
				in GIF buffer
            end note
			''
		repeat while()
    repeat while(execution aborted?) is (N)
	->Y;
	stop
  
'-----------------------
@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/flowchart/output/local-gif-gen.png]]

*** ✔ DONE Data formats
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-22 qua 23:46]
    :END:
  #+BEGIN_SRC plantuml :file diags/plantuml/misc/output/data-formats.png
@startuml
''left to right direction
skinparam fixCircleLabelOverlapping true
'' src: https://www.augmentedmind.de/2021/01/17/plantuml-layout-tutorial-styles/
''skinparam nodesep x (where x is an integer > 0) will increase the horizontal margin
''skinparam ranksep x affects the vertical margin
''With skinparam padding x (x also being an integer > 0) you will increase the padding of every kind of element, so use it carefully, with small values.
skinparam nodesep 1
skinparam ranksep 1
skinparam padding 0.0

<style>
 ' scope to sequenceDiagram elements
 ' scope to actor elements
   component {
       FontColor Blue
       FontStyle bold
   }
   rectangle {
       FontColor black
       FontStyle bold
       ''BackGroundColor lightgrey
       Shadowing 0.0
   }
   frame {
       BackGroundColor bisque
   }
   package {
       BackGroundColor lightgrey
       FontStyle bold
   }
''   .legStyle{
''       BackGroundColor lightgrey
''       Shadowing 0.0
''   }
</style>

''skinparam RectangleBackgroundColor<<legStyle>> black
skinparam RectangleBorderColor<<legStyle>> black
''skinparam handwritten <<legStyle>> true

'' colors
''!$highest = darkred
''!$high = %lighten($highest, 40)
''!$medium = %lighten($highest, 80)
''!$low = %lighten($highest, 120)
!$highest = red
!$high = "#orange"
!$medium = "#yellow"
!$low = "#lightgreen"
!$basic = "#yellow"
!$local = "#orange"
!$fragTClr = "#lightgreen"
!$fragIntTClr = "#bisque"
!$mediaClr = "#tan"
!$ackClr = "#pink"
!$sdClr = "#greenyellow"
!$rxClr = "#white"

'' participants definition
package "MDO-RC: AppManager" as MDORC #lightgrey{ 
''
  rectangle "rxFrame" as rxFrameRC{
  rectangle "header\nenum frameT" as P0 $basic
  rectangle "len\nsize_t" as P1 
  rectangle "data\nvoid*" as P2
  rectangle "int\nACK" as P3 $ackClr
  }
}

package "MDO-RS: AppManager" as MDORS #lightgrey{ 
  rectangle "rxFrame" as rxFrameRS{
  rectangle "header\nenum frameT" as P01 $basic
  rectangle "len\nsize_t" as P11 
  rectangle "data\nvoid*" as P21
  rectangle "int\nACK" as P31 $ackClr
  }
  rectangle "serverFrame" as serverFrame{
  rectangle "sd\nint" as PS1 $sdClr
  rectangle "rx\nrxFrame" as PS2 $rxClr
  }
}

package "MDO-L: AppManager" as MDOL #lightgrey{
  rectangle "rxFrame" as rxFrameL{
  rectangle "header\nenum frameT" as P02 $local
  rectangle "len\nsize_t" as P12
  rectangle "data\nvoid*" as P22
  rectangle "int\nACK" as P32 $ackClr
  }
  rectangle "AdFrame" as adFrame{
  rectangle "fragrance\nenum fragT" as P03 $fragTClr
  rectangle "intensity\nenum fragIntT" as P13 $fragIntTClr
  rectangle "startTime\nlong int" as P23
  rectangle "duration\nint" as P33
  rectangle "mediaLen\nsize_t" as P43
  rectangle "mediaURLs\nvoid*" as P53 $mediaClr
  rectangle "int\nACK" as P63 $ackClr
  }
}

  rectangle "Legend" <<legStyle>> as LEG #lightblue {
  rectangle "DB\nCMD\n\n\n" as P04 $basic
  rectangle "DB\nCMD\nAD\n\n" as P14 $local
  rectangle "NONE\nCOFFEE\n...\nCHICKEN\n" as P24 $fragTClr
  rectangle "LOW\nMEDIUM\nMEDIUM_HIGH\nHIGH\nTOP" as P34 $fragIntTClr
  rectangle "Comma\nSeparated\nValues\n(CSV)\n" as P44 $mediaClr
  rectangle "Acknowledge\nSignal\n\n\n" as P54 $ackClr
  rectangle "Socket\ndescriptor\n\n\n" as P64 $sdClr
  }

  hide <<legStyle>> stereotype

'' ------------------------------

'' ------------ INTERACTIONS
 P0-[hidden]r-P1
 P1-[hidden]r-P2
 P2-[hidden]r-P3
''
 P01-[hidden]r-P11
 P11-[hidden]r-P21
 P21-[hidden]r-P31

 PS1-[hidden]r-PS2
''
 P02-[hidden]r-P12
 P12-[hidden]r-P22
 P22-[hidden]r-P32
''
 P03-[hidden]r-P13
 P13-[hidden]r-P23
 P23-[hidden]r-P33
 P33-[hidden]r-P43
 P43-[hidden]r-P53
 P53-[hidden]r-P63
''
 rxFrameRS--[hidden]-serverFrame
 rxFrameL--[hidden]-adFrame

'' Legend
 P04-[hidden]r-P14
 P14-[hidden]r-P24
 P24-[hidden]r-P34
 P34-[hidden]r-P44
 P44-[hidden]r-P54
 P54-[hidden]r-P64
 MDORS---[hidden]d-LEG


@enduml
  #+END_SRC

  #+RESULTS:
  [[file:diags/plantuml/misc/output/data-formats.png]]

* Gesture recognition                                              :noexport:
** Research [0/7]
1) [ ] [[https://techvidvan.com/tutorials/hand-gesture-recognition-tensorflow-opencv/][Real-time Hand Gesture Recognition using TensorFlow & OpenCV]]
   1) MediaPipe: a customizable ML frameworks developed by Google which
      comes with some pre-trained models such as face detection and object
      recognition
      1) Recognize hand and the hand key points
   2) TensorFlow: neural networks for ML and DL
      1) These keypoints are fed into a pre-trained gesture recognizer network
	 to recognize the hand pose
   3) Steps:
      1) Import necessary packages.
      2) Initialize models.
      3) Read frames from a webcam.
      4) Detect hand keypoints.
      5) Recognize hand gestures.
2) [ ] [[https://gogul.dev/software/hand-gesture-recognition-p1][Hand gesture recognition using Python and OpenCV]]
   1) Background segmentation
   2) Motion detection and thresholding
   3) Contour extraction
3) [ ] [[file:~/OneDrive%20-%20Universidade%20do%20Minho/Univ/MI_Electro/Sem7/SEC/2021-22/repo/research/gesture-recognition/Hand_gesture_recognition_on_python_and_opencv.pdf][Hand gesture recognition on python and openCV (overview)]]
   1) Hand segmentation
   2) Track gesture of hand using Haar-Cascade Classifier
   3) Region of Interest
   4) Convex-Hull Transform
4) [ ] [[https://github.com/mahaveerverma/hand-gesture-recognition-opencv][Hand gesture recognition in Python using openCV]]
   1) Background subtraction
   2) Histogram
   3) Threshold
   4) Contour and convex hull
      #+BEGIN_EXAMPLE
During setup, first a background model is generated when the user presses 'b'. Then, a histogram is generated when the user provides his hand as a sample by pressing 'c'. When the setup is completed, the program goes into an infinite while loop which does as follows.

Camera input frame is saved to a numpy array. A mask is generated based on background model and applied on the frame. This removes background from the captured frame. Now the frame containing only the foreground is converted to HSV color space, followed by histogram comparison (generating back projection). This leaves us with the detected hand. Morphology and smoothening is applied to get a proper hand shape out of the frame. A threshold converts this into a binary image.

Next, we find contours of the binary image obtained, look for the largest contour and find its convex hull.

Using points from the largest contour we determine center of the palm by finding the largest circle inscribed inside the contour and then the dimension of palm. Using the center of palm as reference, we eliminate all points from the convex hull which do not seem to be part of hand. Also, nearby convex hull points are eliminated so that we are left with exactly only those many points as the number of fingers stretched out.

Using the positions of fingers and palm dimensions, we model our hand.

Then we compare the model with a dictionary of Gestures defined in GestureAPI.py to determine presence of gestures.
      #+END_EXAMPLE
5) [ ] [[https://towardsdatascience.com/training-a-neural-network-to-detect-gestures-with-opencv-in-python-e09b0a12bdf1][Training a Neural Network to Detect Gestures with OpenCV in Python]]
   1) [[https://docs.google.com/presentation/d/1UY3uWE5sUjKRfV7u9DXqY0Cwk6sDNSalZoI2hbSD1o8/edit#slide=id.g49b784d7df_0_2205][Presentation (Summary)]]
   2) Notes:
      1) Train your model on images that are as close as possible to the images
         it is likely to see in the real world - build your own dataset
      2) Extract the gesture: background subtraction + thresholding
      3) Build new dataset: 550 silhouette images for each of the 5 gestures
         using openCV to capture it from webcam and save it with unique
         filenames
      4) Training the new model: built a CNN using Keras & TensorFlow - started
         out with VGG-16 pre-trained model, and added 4 dense layers along with
         a dropout layer on top.
      5) Validation: cross-validation applying same gesture extraction to images
         from Kaggle - results: 98% F1 score, 98% precision and accuracy
6) [ ] [[https://circuitdigest.com/microcontroller-projects/hand-gesture-recognition-using-raspberry-pi-and-opencv][Hand Gesture Recognition using Raspberry Pi and OpenCV]]
   1) OpenCV + TensorFlow
   2) Steps:
      1) Data gathering: 800 images belonging to 4 classes (200 each) 
	 1) Rock
	 2) Paper
	 3) Nothing: so that the Raspberry Pi doesnt make unnecessary gestures
	 4) Scissors
	    #+BEGIN_SRC bash
	    python3 image.py 200
	    # press:
	    # - a: to start the process
	    # - r: rock
	    # - p: paper
	    # - s: scissors
	    # - n: nothing
	    #+END_SRC

	    #+BEGIN_SRC python
	      import cv2
	      import os
	      import sys
	      num_samples = int(sys.argv[1])
	      IMG_SAVE_PATH = 'images'
	      try:
		  os.mkdir(IMG_SAVE_PATH)
	      except FileExistsError:
		  pass
	      # place hands inside rectangle
	      cv2.rectangle(frame, (10, 30), (310, 330), (0, 255, 0), 2)
	      k = cv2.waitKey (1)
	      if k == ord('r'):
		  name = 'rock'
		  IMG_CLASS_PATH = os.path.join(IMG_SAVE_PATH, name)
		  os.mkdir(IMG_CLASS_PATH)               
	      if k == ord('p'):
		  name = 'paper'
		  IMG_CLASS_PATH = os.path.join(IMG_SAVE_PATH, name)
		  os.mkdir(IMG_CLASS_PATH)
	      if k == ord('s'):
		  name = 'scissors'
		  IMG_CLASS_PATH = os.path.join(IMG_SAVE_PATH, name)
		  os.mkdir(IMG_CLASS_PATH)
	      if k == ord('n'):
		  name = 'nothing'
		  IMG_CLASS_PATH = os.path.join(IMG_SAVE_PATH, name)
		  os.mkdir(IMG_CLASS_PATH)
	      # create a ROI around the rectangle we created earlier and save to designated path
	      roi = frame[25:335, 8:315]
	      save_path = os.path.join(IMG_CLASS_PATH, '{}.jpg'.format(counter + 1))
	      print(save_path)
	      cv2.imwrite(save_path, roi)
	      counter += 1
	      font = cv2.FONT_HERSHEY_SIMPLEX
	      cv2.putText(frame,"Collecting {}".format(counter),
			  (10, 20), font, 0.7, (0, 255, 255), 2, cv2.LINE_AA)
	      cv2.imshow("Collecting images", frame)
	    #+END_SRC
      2) Training the model
	 #+BEGIN_SRC bash
	 python3 training.py 
	 #+END_SRC
	 #+BEGIN_SRC python
	   IMG_SAVE_PATH = 'images'
	   CLASS_MAP = {
	       "rock": 0,
	       "paper": 1,
	       "scissors": 2,
	       "nothing": 3
	   }

	   #Now in the next lines construct the head of the model that will be placed on top of the base model. The
	   #AveragePooling2D layer calculates the average output of each feature map in the previous layer. To prevent
	   #over-feeding, we have a 50% dropout rate.
	   def get_model():
	       model = Sequential([
		   SqueezeNet(input_shape=(227, 227, 3), include_top=False),
		   Dropout(0.5),
		   Convolution2D(NUM_CLASSES, (1, 1), padding='valid'),
		   Activation('relu'),
		   GlobalAveragePooling2D(),
		   Activation('softmax')
	       ])

	   #Now, Loop over the image directory and load all the images to python script so that we can begin the
	   #training. Pre-processing steps include converting the images to RGB from BGR, resizing the images to
	   #227×227 pixels, converting them to array format.
	   for directory in os.listdir(IMG_SAVE_PATH):
	       path = os.path.join(IMG_SAVE_PATH, directory)
	       if not os.path.isdir(path):
		   continue
	       for item in os.listdir(path):
		   if item.startswith("."):
		       continue
		   img = cv2.imread(os.path.join(path, item))
		   img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
		   img = cv2.resize(img, (227, 227))
		   dataset.append([img, directory])

	   # Now, we will call the get model function and compile the model with the Adam optimizer.
	   model = get_model()
	   model.compile(
	       optimizer=Adam(lr=0.0001),
	       loss='categorical_crossentropy',
	       metrics=['accuracy']
	   )

	   #After this, start training the model and once training is finished save the model as “game-model.h5”.
	   model.fit(np.array(data), np.array(labels), epochs=15)
	   model.save("game-model.h5")
	 #+END_SRC
      3) Gesture detection
	 #+BEGIN_SRC bash
	 python3 game.py 
	 #+END_SRC
	 #+BEGIN_SRC python
	   #Same as the training.py script this script also starts with importing the required packages. The next lines
	   #after importing the packages are used to create a reverse class-map function.
	   REV_CLASS_MAP = {
	       0: "rock",
	       1: "paper",
	       2: "scissors",
	       3: "nothing"
	   }

	   #The calculate_winner function takes user’s move and Pi’s move as input and then decides the winner out
	   #of it.  It uses Rock, Paper, and Scissors game rules to decide the winner.
	   def calculate_winner(user_move, Pi_move):
	       if user_move == Pi_move:
		   return "Tie"
	       elif user_move == "rock" and Pi_move == "scissors":
		   return "You"
	       elif user_move == "rock" and Pi_move == "paper":
		   return "Pi"
	       elif user_move == "scissors" and Pi_move == "rock":
		   return "Pi"
	       elif user_move == "scissors" and Pi_move == "paper":
		   return "You"
	       elif user_move == "paper" and Pi_move == "rock":
		   return "You"
	       elif user_move == "paper" and Pi_move == "scissors":
		   return "Pi"

	   # Then in the next line load the trained model and start the video stream
	   model = load_model("game-model.h5")
	   cap = cv2.VideoCapture(0)

	   #Inside the loop, create two rectangles on the left and right sides of the frame. The left side rectangle is for the
	   #user’s move and the right side rectangle is for Pi’s move. After that extract, the region of the image within
	   #the user rectangle converts it to RGB format and resizes it to 227×227.
	   cv2.rectangle(frame, (10, 70), (300, 340), (0, 255, 0), 2)
	   cv2.rectangle(frame, (330, 70), (630, 370), (255, 0, 0), 2)
	   roi = frame[70:300, 10:340]
	   img = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
	   img = cv2.resize(img, (227, 227))

	   #Now, use the model that we trained earlier and predict the gesture. Compare the predicted code with the class map and get the user move name.
	   pred = model.predict(np.array([img]))
	   move_code = np.argmax(pred[0])
	   user_move_name = mapper(move_code)

	   #In the next line check if users move is nothing or not. If not then allow Raspberry Pi to make its own random
	   #move and then use calculate_winner function to decide the winner. If the user move is ‘nothing’ then wait for the user to make a move.
	   if prev_move != user_move_name:
	       if user_move_name != "nothing":
		   computer_move_name = choice(['rock', 'paper', 'scissors'])
		   winner = calculate_winner(user_move_name, computer_move_name)
	       else:
		   computer_move_name = "nothing"
		   winner = "Waiting..."
	       prev_move = user_move_name


	   #In these lines, we have used cv2.putText function to display Users move, Pi’s move, and the winner on the frame.
	   font = cv2.FONT_HERSHEY_SIMPLEX
	   cv2.putText(frame, "Your Move: " + user_move_name,
		       (10, 50), font, 1, (255, 255, 255), 2, cv2.LINE_AA)
	   cv2.putText(frame, "Pi's Move: " + computer_move_name,
		       (330, 50), font, 1, (255, 255, 255), 2, cv2.LINE_AA)
	   cv2.putText(frame, "Winner: " + winner,
		       (100, 450), font, 2, (0, 255, 0), 4, cv2.LINE_AA)

	   #Now, we will display the Pi’s move inside the rectangle that we created earlier. Pi will randomly choose an image stored in the ‘test_img’ directory.
	   if computer_move_name != "nothing":
	       icon = cv2.imread("test_img/{}.png".format(computer_move_name))
	       icon = cv2.resize(icon, (300, 300))
	       frame[70:370, 330:630] = icon
	 #+END_SRC
7) [ ] [[pdfview:/home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/biblio/A-systematic-review-on-hand-gesture-recognition-techniques,-challenges-and-applications.pdf::1][A systematic review on hand gesture recognition techniques, challenges
   and applications]]
   1) Scholar overview about hand gesture recognition
* RDBMS
** SQLite vs MySQL
1) https://www.hostinger.com/tutorials/sqlite-vs-mysql-whats-the-difference/
** mySQL
*** Installation [1/1]
1) [X] Add APT repository (see [[id:8fe4ac7c-a9d3-4841-aba2-27b609f6b29c][here]])
2) [ ] Install server (see [[id:e9eeae7e-2396-41c6-925a-5ad2fa36acf0][here]])
3) [X] Install client (see [[id:9cd276fc-a3d9-4826-8c8d-929055b3c3ef][here]])
**** ✔ DONE Add APT repository [4/4]
     :PROPERTIES:
     :ID:       8fe4ac7c-a9d3-4841-aba2-27b609f6b29c
     :END:
     :LOGBOOK:
     - State "✔ DONE"     from              [2021-11-30 ter 01:58]
     :END:
  1) [X] Download APT repository: from [[https://dev.mysql.com/downloads/repo/apt/][here]]
  2) [X] Install APT repository using ([[https://dev.mysql.com/doc/mysql-apt-repo-quick-guide/en/#apt-repo-setup][src]]):
     #+BEGIN_SRC bash
     sudo dpkg -i /PATH/version-specific-package-name.deb
     # example
     # sudo dpkg -i /PATH/version-specific-package-name.deb
     #+END_SRC
  3) [X] During the installation, you'll be asked what components to install.
     1) Choose =none=
     2) Press =ok= to finish
  4) [X] Update package info from APT repository (this step is mandatory):
     #+BEGIN_SRC bash
     sudo apt get update
     #+END_SRC
*** Server
**** Installation [2/5]
     :PROPERTIES:
     :ID:       e9eeae7e-2396-41c6-925a-5ad2fa36acf0
     :END:
  1) [X] Install mySQL server
     #+BEGIN_SRC bash
    sudo apt install mysql-server 
     #+END_SRC
  2) [X] Protect the server before deploying: perform secure installation
     #+BEGIN_SRC bash
     sudo mysql_secure_installation
     # say yes to validate with password
     y
     # select security level of password: medium
     1
     # input password
     ESRG-MDO-Hugo-Ze@2021
     # continue with the password provided: yes
     y
     # Remove annonymous users from DB: yes (they can be added back later)
     y
     # disable root login remotely: yes
     y
     # Remove test database: yes
     y
     # Reload privilege tables: yes
     y
     #+END_SRC
  3) [ ] Enable MySQL to run at boot
     #+BEGIN_SRC bash
    sudo systemctl enable --now mysql.service 
     #+END_SRC
  4) [ ] Check the status of the service
     #+BEGIN_SRC bash
    systemctl status mysql.service 
     #+END_SRC
  5) [ ] Whitelist mySQL service through firewall
     1) Check firewall status
	#+BEGIN_SRC bash
       sudo ufw status 
	#+END_SRC
	1) No rule should be active, and so, it should only show =status: active=
     2) Whitelist it
	#+BEGIN_SRC bash
       sudo ufw allow mysql
	#+END_SRC
     3) Check firewall status
	#+BEGIN_SRC bash
       sudo ufw status 
	#+END_SRC
	1) A rule should be active (mySQL), and available at port =3306= (the
           default for mySQL)
*** Shell
 MySQL Shell provides an interactive code execution mode, where you type code at
 the MySQL Shell prompt and each entered statement is processed, with the result
 of the processing printed onscreen. Unicode text input is supported if the
 terminal in use supports it. Color terminals are supported. ([[https://dev.mysql.com/doc/mysql-shell/8.0/en/mysql-shell-features.html][src]])

Multiple-line code can be written using a command, enabling MySQL Shell to cache
multiple lines and then execute them as a single statement. For more information
see Multiple-line Support. 
**** ✔ DONE Installation [1/1]
     :PROPERTIES:
     :ID:       9cd276fc-a3d9-4826-8c8d-929055b3c3ef
     :END:
     :LOGBOOK:
     - State "✔ DONE"     from              [2021-11-30 ter 01:58]
     :END:
1) [X] Install using terminal ([[https://dev.mysql.com/doc/mysql-shell/8.0/en/mysql-shell-install-linux-quick.html][src]])
   #+BEGIN_SRC bash
   sudo apt install mysql-shell
   #+END_SRC
*** C++ connector
**** Introduction
[[https://dev.mysql.com/doc/connector-cpp/8.0/en/connector-cpp-introduction.html][src]]
**** Usage examples
src
* Doxygen                                                          :noexport:
** ☛ TODO Installation
   :LOGBOOK:
   - State "☛ TODO"     from              [2021-12-04 sáb 23:33]
   :END:
** Doxyfile
[[file:~/OneDrive-UM/Univ/MI_Electro/Sem3/ATC1/Trab/code/Doxyfile][src]]
* Fragrance diffusion                                              :noexport:
** Research
[[file:~/OneDrive-UM/Work/Ambientador][Ambientador]] 
*** [[pdfview:/home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/biblio/fragrance-diffusion/Development_of_a_Piezoelectric-Based_Odor_Reproduc.pdf::1][Development of a Piezoelectric-Based Odor Reproduction system]]
**** Intro
 - *Odor reproduction*: technology through which a machine represents various odors
   by blending several odor sources in different proportions and releases them.
 - *proposed system*: atomization-based odor dispenser using 16 micro-porous
   piezoelectric transducers.
 - *machine olfaction*: technology with which an intelligent machine detects and
   identifies gases or odors.
 - *electronic nose*: gas/odor identification machine which is comprised of a gas
   sensor array and an intelligent identification block [1].
 - Atomization and thermalization are two solutions for building an odor
   reproduction instrument or an olfactory display.
 - *Atomization*: is an approach to dispense odorants with the advantages that
   the dispensing process is fast and the dispensing quantity is controllable.
   - _Surface acoustic wave (SAW)_ are a way of atomizing odorants.
     - Liquid on a piezoelectric substrate is vibrated and atomized by a spurred
       wave generated by an interdigital transducer electrode [9].
     - Several SAW-based olfactory displays have been reported [7, 10, 11].
   - _Micro-porous piezoelectric transducers_ are the other way for atomizing
     odorants.
     - It atomizes liquid by means of a metal mesh smashing the liquid.
     - The smash of the metal mesh is driven by the vibration of a piezoelectric
       substrate.
     - Piezoelectric transducers are suitable for commercial uses, due to being
       low cost.
 - *Thermalization*: is the other way of dispensing odors, by vaporizing odor
   sources in the liquid state of the solid state using PWM heaters [8,12].
   - A temperature controller for odor dispensers should be embedded in a
     thermalization-based odor reproduction instrument or olfactory display to
     avoid the possibility of scorching odor sources.
 - The obstacles to building an odor reproduction system are as follows:
   - Firstly, odor blending should be controlled precisely, because a mixture of
     various odorants in different proportions may change the odor.
   - Secondly, the scheme of how to reproduce odors according to odor information
     is ambiguous.
   - Thirdly, the biological, neurological, and psychological mechanisms of odor
     perception are unclear as of yet.
 - Perfumery is assumed to be the most similar technique to odor reproduction,
   although various aspects differ from odor reproduction, such as aroma
   duration, stability, and so on.
   - The scent of perfume can be divided into three stages: Top note, body note,
     and basic note, since a bottle of perfume is a combination of fragrance
     sources which possess different fragrance retention times.
   - However, an odor reproduction system reproduces different odors
     dynamically. This means that those fragrance sources which have shorter
     retention times may be selected.
   - Besides, perfumery is an art, to some extent, so flavorists may incorporate
     their background knowledge into perfume formulas; however, an odor
     reproduction system needs a more regularized set of rules for blending odor
     sources.
 - The potential applications of odor reproduction technology are manifold:
   - virtual and augmented reality
   - smart homes
   - electronic commerce
   - medical uses, such as first-diagnosis of Parkinson’s disease, as the
     degradation of olfaction is a representative characteristic of the disease
     [20]
 - It is reasonable that an odor reproduction system should have the following
   features:
   1) Reproduces odors in real-time;
   2) A limited number of odor sources are used;
   3) Capacity for reproducing a large number of odors; and
   4) Has a user-friendly interface.
**** Odor Reproduction System
***** Architecture
      :PROPERTIES:
      :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
      :END:
- Controller unit + odor blender + UI application
- Odor blender = odor dispenser + 2 fans + chamber
  - It blends several odor sources
  - odor dispenser: array of 16 odor atomizers, based on micro-porous
    Piezoelectric transducers, to atomize the odor source liquid.
  - The 2 fans are used for:
    - blending odors
    - releasing odors
    - cleaning residual gases in the chamber
- Controller:
  - ready-to-use development platform designed by Digilent Inc
  - 2 peripheral devices: a gas sensor component and a Bluetooth component
  - Self-designed circuit for driving the piezoelectric transducers and fans
- UI application: developed in HTML5
[[file:sec/img/odor-reproduction-sys-overview.png]]
***** Odor dispenser
      :PROPERTIES:
      :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
      :END:
- function: capable of releasing odorous compounds in certain proportions
- contains:
  1) 4x4 array of odor atomizers (micro-porous piezoelectric transducer):
     - rubber gasket: used to isolate electric conduction from other conducting
       materials and as cushion against vibration
     - metal substrate:
       - has a micro-porous metal mesh in the center with a diameter of 35 mm
       - the nr of micro-porous is circa 700
       - each micro-pore is a trumpet-shaped cylinder in which the upper
         cylindrical surface is smaller than the bottom, as shown in Figure 2b.
     - ring-shaped piezoelectric plate
     - A contact is attached to the piezoelectric plate, so that the power wire
       and ground wire can be connected between the piezoelectric plate and the
       metal substrate.
  2) plastic tube: stores an odor source
  3) cotton core: applied for absorption of the odor source liquid
- operation:
  - A micro-porous piezoelectric transducer is driven by a wave with a frequency
    of around 113 kHz and converts electric energy into kinetic energy due to
    inverse-piezoelectricity.
  - The metal substrate vibrates along with the vibration of the ring-shaped
    piezoelectric plate, and the mesh in the center of the metal substrate
    smashes the liquid beneath the transducer. 
  - Some liquid flows through those micro-pores and is emitted in micro-droplet
    form
[[file:sec/img/odor-reproduction-dispenser.png]]

[[file:sec/img/odor-reproduction-transducer.png]]
***** Controller Unit
- consists of:
  - Digilent Arty Z7 development board with a SoC by Xilinx
  - Self-designed board
    - The driver circuit is comprised of 16 amplifiers for the 16 atomizers, a
      fan driving circuit for the two fans, and a connect board. Each amplifier
      contains a transformer with 38 times amplification. The schematic of the
      amplifier circuit is shown in Figure 3b.
  - sensor
  - Bluetooth module
- Multiple channels (16 channels) of driving wave signals, generated by the Arty
  Z7 development board, are amplified first by the self-designed circuit and,
  then, transmitted to the 16 micro-porous piezoelectric transducers.
***** Firmware
***** Software
*** PCB for ultrasonic diffusion
    :PROPERTIES:
    :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
    :END:
[[file:sec/img/pcb-diffusion.png]]
**** Product
 - [[https://i.ebayimg.com/images/g/fRkAAOSwmglgpkut/s-l1600.jpg][img]]
 - [[https://pg-cdn-a2.datacaciques.com/00/NDAy/21/05/20/pet8ub8udrf456m1/357cc097418e66bc.jpg][detailed explanation about PCB]]


*Product introduction*:
- Circuit board parameters:
- Circuit board product voltage: DV 5V
- Current: 300mA
- Power: 2W
- Frequency 108KHz, fixed frequency single-chip microcomputer
- Driver board size: 35*20*17mm (L*W*H) (there will be slight errors)
- Strong versatility, large amount of fog, stable performance, the chip has an
  automatic timing shutdown function (4 hours of continuous work will
  automatically shut down protection, if you need to turn on again, press the
  power on again).
- 5V USB power supply mode, can be powered by MICRO charging cable.
 
*Atomizing film parameters*:
- The net diameter of the atomized steel sheet is 16mm, the outer diameter of
  the silicone ring is 20mm, and the wire length is 8cm.
- Product voltage DV 5V, current 300mA, power 2W, frequency 108KHz, hole number
  740, hole diameter 5um
- The microporous atomized sheet produced by us is coated with a special glaze
  protective layer on the surface of the ceramic sheet, which is resistant to
  acid and alkali and prevents the silver layer from being corroded and oxidized
  by the liquid.
 
*Package Included*:
- Circuit board X1
- Atomization film X1
- Absorbent cotton core X1
**** Electronics
- [[https://www.one-tab.com/page/zX4Un1IBQjG93WQ-tEbv-Q][Research]]
**** Theory
- [[https://www.one-tab.com/page/bRupBzoYSFq4NClOIc90iw][Research]]
*** Ultrasonic diffusion
**** Theory explanation
     :PROPERTIES:
     :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
     :END:
***** Great Scott ([[https://www.youtube.com/watch?v=aKhPj7uFD0Y][link]])
 Piezoelectric actuators are reciprocating actuators as they: 
 - respond to electric stimuli by generating mechanical displacement which, in
   turn, produces waves. 
 - respond to mechanical displacement (applied force) by generating an electrical
   voltage signal.

 In the present case, one is more interested in the first phenomena.
 - Piezoelectric actuators have a ressonant frequency, which means they will go
   into a natural oscillation state.
 - Thus, stimulating the piezoelectric actuator with a AC signal at the ressonant
   frequency produces a strong mechanical oscillation, generating waves.
 - For small ressonant frequencies, the liquid, e.g. water, can easily follow the
   mechanical oscillation produced.
 - However, when the ressonant frequency is about 1.7 MHz, the water particles
   cannot follow the oscillating surface, thus creating momentary vaccum, due to
   the negative amplitudes, which therefore creates air bubbles. Then, on
   positive amplitudes these air bubbles are pushed across the surface,
   catapulting water dropplets into the air, quickly dissipating and turning into
   vapor form.
     [[file:sec/img/diffuser-theory-explained.png]]
 - Thus, one needs to generate an AC signal at the ressonant frequency (1.7 MHz).
**** Reverse engineering PCB 
***** Great scott (link)
      :PROPERTIES:
      :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
      :END:
[[file:sec/img/pcb-reverse-eng.png]]

List part: Ebay:
- 1x NE555: http://rover.ebay.com/rover/1/711-53200-19255-0/1?...
- 1x IRLZ44N: http://rover.ebay.com/rover/1/711-53200-19255-0/1?...
- 1x 113kHz Piezoelectric Disc: http://rover.ebay.com/rover/1/711-53200-19255-0/1?...
- 1x 5kΩTrimmer: http://rover.ebay.com/rover/1/711-53200-19255-0/1?...
- 1x 10Ω Resistor: http://rover.ebay.com/rover/1/711-53200-19255-0/1?...
- 1x 220µH Inductor: http://rover.ebay.com/rover/1/711-53200-19255-0/1?...
- 2x 100nF, 1x 10nF Capacitor: http://rover.ebay.com/rover/1/711-53200-19255-0/1?...
*** [[pdfview:/home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/biblio/fragrance-diffusion/A-Novel-Olfactory-Displays'-Scent-Dispersing-Module.pdf::2][A novel olfactory displays' scent dispersing module]]
**** Intro
- Focus on controlled atomization of a liquid based scent while guaranteeing:
  speed, intensity, durability, compactenss and scalability
- olfactory display should adjust the scent intensity according to the distance
  to the viewer and the amount of scent sources to produce a more immersive
  interaction between the user and the computer.
**** Related works
- Several technologies were applied to disperse the scented data to the target
  user:
  - In [1] and [2] a projection device were proposed to eject the scent based on
    nose tracking feature, the ejection is done using a proposed air cannon that
    contracts upon delivery. The scent in delivered inside the cannon and
    ejected afterwards,
  - while in [3-6] the same olfactory display were used to disperse the scented
    material towards the user, it uses solenoids valves and an air compressor to
    create an air flow. 
    - The concept is based on bubbling the air into the liquid scented material
      to create an evaporated scent mixed with air (air born scent) and
      controlling the flow of that air/scent mixture among other types of scents
      produced in the same way to be directed to the user's nose.
  - Furthermore in [7] and [8] another approach were applied using an
    electro-osmotic micro pump to deliver a predefined amount of scented
    material to be dispersed using and SAW (surface acoustic wave) device that
    is capable of producing a vapor rapidly,
  - other techniques such as [9] using a solid (gel) scent material and a
    peltier module, the module produces heat to transform the gel material to
    vapor and deliver it to the user.
  - In the previously mentioned techniques the olfactory display device is
    basically a desktop device connected to a personal computer and other
    supportive devices such as high voltage supply, air pump and frequency
    generators etc. while in [10-12] the olfactory display were worn by the user
    and the scent ejection point is placed near the user's nose, this approach
    is suitable for virtual reality application when the user is need to be
    mobile rather than sitting at a desk. The mentioned techniques solved some
    problems in terms of scent delver but still have limitations in terms of
    intensity control.
  - In [13] a more precise technique is used based on inkjet cartridge
    technology. An ink cartridge has been modified to eject liquid scent rather
    than ink using the same principles of ink amount control, mixture and
    evaporation.

**** Materials and methods
     :PROPERTIES:
     :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
     :END:

comparison between the proposed scent element and the performance and issues of
other olfactory displays that are the most popular and recent in the literature:

[[file:sec/img/novel-olfactory-comparison-tab.png]]
[[file:sec/img/novel-olfactory-comparison-tab2.png]]

***** Scent element design
      :PROPERTIES:
      :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
      :END:
The design of the scent element has several advantages such as:
- compactness,
- spill proof,
- easy to refill
- and printable by an 3D printer. 

It contains: 
- a reservoir for the liquid scented material (about 10cc)
- and a top opening to insert the high absorbent cotton core
- and a 2mm edge to hold the piezoelectric micro porous film.
- The inside base of the scent element is taper shaped to collect all the scent
  material to bottom of the cotton core,
- the top contains a hole for refilling the scent element with proper scent
  material, see Figure 1 (a) and (b).

[[file:sec/img/novel-olfactory-scent-elem.png]]

The described design represents the shell, chassis or housing of the scent
element, other components are placed on this shell such as:
- the circuit board providing the sufficient frequency to drive the ultrasonic
  piezoelectric film
- and the piezoelectric film itself using upper and lower silicone washers to
  hold it in place and providing seal and vibration freedom at the same time.

***** Piezoelectric film
      :PROPERTIES:
      :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
      :END:
The piezoelectric film used is a recently emerging type of film, called
micro-porous piezoelectric film, where its center is a metal mesh with micro
sized holes to extrude the scented material with micro sized droplets (Figure
2). 
- Placing the film directly above the cotton core will enable it to atomize the
  scent material absorbed by the cotton core by capillary action,
- the continuous vibration will atomize the material at the top
- and the cotton core will absorb more to complete the cycle (Figure 3 shows the
  scent element assembly in design and actual picture).

[[file:sec/img/novel-olfactory-scent-piezo-film.png]]

[[file:sec/img/novel-olfactory-scent-mech-design.png]]

The reason behind choosing the micro porous piezoelectric film is due to its
versatility regarding voltage supply, size and other characteristics that illustrated as follows:
- Properties of piezoelectric film
  1. Diameter: 13.8mm.
  2. Low driving voltage :3-12V.
  3. High conversion efficiency, spray volume.
  4. Exit aperture is very small 4µm.
  5. Frequency: 113khz ± 5khz.
  6. Capacitance: 2700PF ± 15%.
  7. Power: 1.5-2.0W.
  8. Spray volume 30 ml / h.
  9. Can atomize essential oils, perfume, water based perfumes or even mixture
     of the mentioned materials.
  10. life of more than 3000 hours.


***** Speed of delivery
- measured using a tin dioxide SnO2 sensor with aluminum ceramic tube AL2O3
  which is highly responsive to alcohol placed 100cm away from the scent
  element, the latter is filled with isopropyl alcohol (rubbing alcohol) diluted
  in water with 10% concentration, the distance is chosen to be the maximum
  distance to be set between a user and an interface device (peripheral device).
In a
controlled environment some experimental results were obtained regarding the response
time and delivery, the ambient temperature was 23-25 degrees Celsius and
humidity of

In initial tests a range of intensity values were sent consecutively to the
scent element from the PC via a microcontroller and the response values of ppm
(particles per million) were registered.
- Figure 4 shows the relation between the ejected alcohol and response ppm
  values by the sensor. 
- When the microcontroller receives an ejection order from the PC it starts a
  timer
- and the response of the sensor stops that timer indicating detection has
  occurred.

***** Intensity
The scent element is controlled using timed pulses, each pulse duration
correspond to a different intensity of scent to be delivered to the user. 
- By experimentation five distinctive levels of intensities can be produced by
  the scent element,
- the intensities ranges from 16 to 255 each of which can be represented by a
  byte of data sent to the microcontroller from the PC, being 16 as the lowest
  and 255 as the highest intensity.
- To measure these intensities a series of consecutive intensities are sent from
  the PC to the microcontroller from 16 to 255, measuring the response of the
  sensor for the same amount of alcohol to water dilution.
- Figure 5 shows the response values of the sensor with respect to the
  intensities delivered by the scent element.

***** Durability
The scent's element container is:
- made from PLA plastic by 3D printing technology,
- it can be redesigned to have larger reservoir or larger piezoelectric film for
  applications requiring higher intensities of an olfactory display such as
  simulating fog particles, water splash or rain fall which requires higher
  volumes of water to be dispersed in the air in shorter time.
- The scent cartridge can be refilled easily by the upper refill hole after
  removing the cap and filling the desired scent.

***** Scalability
As mentioned earlier having the scent element to be such compact sized, it is
possible to arrange more of these scent element into a small space having the
advantage of size over the number of scent elements is decreased significantly,
- furthermore, the scent element has its own 3 wire connection 5V, GND and
  Pulse.
- The pulse is connected to a controlling microcontroller requiring on digital
  I/O pin per scent element.
- The scent element dose not draw current from the microcontroller just needs a
  starting pulse to start activating and another pulse to declare the end of
  activation.
- To attach scent elements more than the actual number of digital I/O pins of
  the micro controller a shift register, a decoder or a de-multiplexer or any
  pin expansion approach.
- Figure 6 shows an olfactory display with 4×5 scent elements aligns as a matrix
  to provide scent material to the upper level of the olfactory display

** Report [0/3]
1) [ ] Fragrance diffusion technologies
   1) [[pdfview:/home/zmpl/OneDrive-UM/Work/Ambientador/Handbook%20of%20Atomization%20and%20Sprays%20-%20Theory%20and%20Applications%20(Nasser%20Ashgriz,%20(Ed.),%202011)%20-%20Book.pdf::567][Ultrasonic nozzles]]
2) [ ] Electronic nose (E-nose)
3) [ ] Ultrasonic atomization
   1) [ ] Theory
   2) [ ] Circuit
   3) [ ] Conclusions
* GIF generation
** C++ libraries and APIs [0/1]
1) [ ] [[https://www.imagemagick.org/Magick++/Documentation.html][Magick++]]:
   - C++ wrapper for ImageMagick C library
   - One can user openCV to read frames and convert them to Magick++ structures
     and them save them as GIFs (see [[https://stackoverflow.com/questions/41841553/convert-magickimage-to-cvmat][here]] and [[https://gist.github.com/AlphaNext/20a74d872505a36d514778b9b0719e92][here]])
   - [[https://www.one-tab.com/page/fuqT5AkaRZGbzBcehZlXVQ][research]]
* Qt                                                               :noexport:
** ✔ DONE Installation
   :LOGBOOK:
   - State "✔ DONE"     from              [2021-12-27 seg 23:07]
   :END:
[[https://stackoverflow.com/a/48147461][src]]:
#+BEGIN_SRC bash
sudo apt-get install build-essential
sudo apt-get install qt5-default
sudo apt-get install qtcreator
sudo apt-get install qt5-doc
sudo apt-get install qt5-doc-html qtbase5-doc-html
sudo apt-get install qtbase5-examples
#+END_SRC
*** No kits detected                                               :Solution:
[[https://stackoverflow.com/questions/48147356/install-qt-on-ubuntu][src]]
(Ubuntu 20.04): installing qtcreator before qt5-default (qt5-qmake) prevented
qtcreator from automatically detecting qmake. This had to be manually added
via; 1) qtcreator - tools - options - Qt Versions - Manual - Add - qmake
location: /usr/lib/x86_64-linux-gnu/qt5/bin/qmake, 2) qtcreator - tools -
options - Manual - Desktop (default) - Qt version - Qt 5.xx
** ✔ DONE Run qmake externally
   :LOGBOOK:
   - State "✔ DONE"     from              [2021-12-27 seg 23:29]
   :END:
[[https://stackoverflow.com/a/19214180][src]]
#+BEGIN_SRC bash
cd /path/to/project
qmake -project # if the project is not yet created, otherwise
qmake -makefile
make -j$(nproc) #
#+END_SRC
** ✔ DONE Build with CMake
   :LOGBOOK:
   - State "✔ DONE"     from              [2021-12-31 sex 20:54]
   :END:
*** ✔ DONE Test project [3/3]
    :PROPERTIES:
    :ID:       8387549a-aa60-47d8-ab41-245bd8be131e
    :END:
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-31 sex 20:52]
    :END:
1) [X] Create a new Qt Widget app: see [[file:~/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/Qt/testCmake][here]]
   1) Select =CMake= as the build system
   2) A =CMakeLists.txt= is automatically created for the project: see [[file:~/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/Qt/testCmake/CMakeLists.txt][here]]
2) [X] Build the project in QtCreator to check if everything is OK
   1) Status: compilation and execution successfull
3) [X] Build manually using =cmake= with LSP integration
     #+BEGIN_SRC bash
     mkdir build && cd build
     cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=1 ..
     make -j11
     ./execName
     #+END_SRC
** ✔ DONE Creating multiple UI screens
   :PROPERTIES:
   :ID:       ba04076c-5bcb-496c-adbe-eccb7fc6106f
   :END:
   :LOGBOOK:
   - State "✔ DONE"     from "☛ TODO"     [2022-01-01 sáb 03:13]
   :END:
Two methods can be used [1/2]:
1) [ ] [[id:82b9b4cd-bcc6-4931-a62c-067c7eeeb752][QStackedWidget and 1 form only]]
2) [X] [[id:9fbc087a-a9db-49d1-83ff-f16804ee72dc][QStackedWidget and several forms (classes)]]: this was the preferred one
*** Method 1: QStackedWidget and 1 form only
    :PROPERTIES:
    :ID:       82b9b4cd-bcc6-4931-a62c-067c7eeeb752
    :END:
[[https://www.youtube.com/watch?v=oFw-erSp3W8][src]]

*Notes*:
- Easiest method
- Straightforward for simple UIs
- It tends to became messy for more complex UIs

*Steps* [0/5]:
1) [ ] Add a =QStackedWidget= to the UI form
2) [ ] Customize it: resize, set bg color, etc
3) [ ] Add page: in the Form Designer, pages can be added in the objects
   hierarchy
4) [ ] Populate pages with items
5) [ ] Implement pages transitions: most easily done with =on_button_pressed=
*** ✔ DONE Method 2: QStackedWidget and several forms (classes)
    :PROPERTIES:
    :ID:       9fbc087a-a9db-49d1-83ff-f16804ee72dc
    :END:
    :LOGBOOK:
    - State "✔ DONE"     from              [2022-01-01 sáb 03:13]
    :END:
[[https://www.youtube.com/watch?v=27PvtxWlV-o][src]]

*Notes*:
- More complex method
- It pays off for larger applications
- It can be used to grouped related info, such as, UserInfo, AdminInfo,
  ImageFilter, etc.

*Steps* [9/9]:
1) [X] Start a [[file:code/qt/multipleViews/multViews/][Qt Widgets application]] on Qt
2) [X] Add a =QStackedWidget= to the main UI form
3) [X] Customize it: resize, set bg color, etc
4) [X] Remove additional pages: in the Form Designer, pages can be removed in
   the objects hierarchy
5) [X] Populate the page with items: a label and pushbuttons for other forms
6) [X] Add a new UI form [4/4]: 
   1) [X] =File->New file->Qt->Qt Designer form class->Widget= and name it:
      e.g., =MachineInfo=
   2) [X] Customize it: resize, set bg color, etc
   3) [X] Populate the page with items: a label and pushbuttons for other forms
   4) [X] If it has a =QStackedWidget= don't forget to initialize it in the
      constructor:
      #+BEGIN_SRC cpp
      MachineInfo::MachineInfo(QWidget *parent)
      : QMainWindow(parent)
      , ui(new Ui::MachineInfo)
      {
      ui->setupUi(this);

      // Initialize to first page
      ui->stackedWidget->setCurrentIndex(0);
      }
      #+END_SRC
7) [X] Declare pointers for each UI form in the main UI form header file
   (=mainwindow.h=) [2/2]
   1) [X] include the headers for each UI form class added
      #+BEGIN_SRC cpp
      #include "MachineInfo.h"
      #+END_SRC
   2) [X] add the private member
      #+BEGIN_SRC cpp
      private:
      MachineInfo *_machInfo; // as a pointer
      //MachineInfo _machInfo; // as a static object
      #+END_SRC
      1) Probably creating the static object is better for performance, since a
         new object is not created each time.
      2) But one could maintain the pointer, as just make sure we initialize it
         in the =mainwindow= constructor
8) [X] Add each UI form as a page to the =QStackedWidget= of the main UI form
   1) =mainwindow.cpp=
      #+BEGIN_SRC cpp
      MainWindow::MainWindow(QWidget *parent)
      : QMainWindow(parent)
      , ui(new Ui::MainWindow)
      {
      ui->setupUi(this);

      // Initialize to first page
      ui->stackedWidget->setCurrentIndex(0);

      ui->stackedWidget->insertWidget(1, _machInfo);
      ui->stackedWidget->insertWidget(2, _settingsInfo);
      }
      #+END_SRC
9) [X] Implement pages transitions: most easily done with =on_button_pressed=
   1) [X] for =mainwindow=: in =mainwindow.cpp=
      #+BEGIN_SRC cpp
      void MainWindow::on_pushbutton_clicked(){
      // using index
      ui->stackedWidget->setCurrentIndex(1);
      }
      void MainWindow::on_pushbutton2_clicked(){
      // using pointer
      ui->stackedWidget->setCurrentWidget(_settingsInfo);
      }
      #+END_SRC
   2) [X] for other forms: one must avoid circular dependency. Thus, signals and
      slots are preferred. The child form sents a signal and the main UI form
      handles that signal, by connecting a slot to it.
      1) [X] =MachineInfo.h=: declare a signal
	 #+BEGIN_SRC cpp
	 signals:
	 void MI_home_pressed();
	 #+END_SRC
      2) [X] =MachineInfo.cpp=: emit the signal from this UI pushbutton
	 #+BEGIN_SRC cpp
	 void MachineInfo::on_pushbutton_clicked(){
	 // using index
	 emit MI_home_pressed();
	 }
	 #+END_SRC
      3) [X] Create a slot to handle the signal
	 1) [X] =mainwindow.h=
	    #+BEGIN_SRC cpp
	   slots:
	   void on_MI_home_pressed();
	    #+END_SRC
	 2) [X] =mainwindow.cpp=
	    #+BEGIN_SRC cpp
	    MainWindow::on_MI_home_pressed(){
	    ui->stackedWidget->setCurrentIndex(0);
	    }
	    #+END_SRC
      4) [X] =MainWindow.cpp=: connect the signal and the slot together
	 #+BEGIN_SRC cpp
	 MainWindow::MainWindow(QWidget *parent)
	 : QMainWindow(parent)
	 , ui(new Ui::MainWindow)
	 {
	 ui->setupUi(this);

	 ui->stackedWidget->insertWidget(1, _machInfo);
	 ui->stackedWidget->insertWidget(2, _settingsInfo);

	 connect(_machInfo, SIGNAL(MI_home_pressed()) , 
	 this, SLOT(on_MI_home_pressed()) );
	 }
	 #+END_SRC

** Design UI for Local System
** Setup for cross-compilation and deployment for Raspberry
* CMake
- [[https://gitlab.kitware.com/cmake/community/-/wikis/doc/cmake/CrossCompiling][Cross compilation]]
* OpenCV
  *Examples* [5/6]:
1) [X] [[file:code/computer-vision/test-opencv/display_img.cpp][display_img.cpp]] (src):
   1) About: displays an image on screen
   2) Status: compiled but did not run, cuz its required to
       set up the path for samples (see [[https://forum.opencv.org/t/why-does-the-tutorial-code-from-opencv-give-unspecified-error/5104/3][here]])
2) [X] [[file:code/computer-vision/test-opencv/basic-draw/src/basic_draw.cpp][basic_draw.cpp]] ([[https://docs.opencv.org/4.x/d3/d96/tutorial_basic_geometric_drawing.html][src]]):
   1) About: draws basic shapes on screen
   2) Status: compiled and runned successfully
3) [X] [[file:code/computer-vision/test-opencv/find-contours/src/find_contours.cpp][find_contours.cpp]] ([[https://docs.opencv.org/4.x/df/d0d/tutorial_find_contours.html][src]]):
   1) About: finds contours on a image
   2) Status: compiled but did not run, cuz of the same
4) [X] [[file:code/computer-vision/test-opencv/cmake-example/example.cpp][example.cpp]] ([[https://github.com/opencv/opencv/tree/master/samples/cpp/example_cmake][src]]):
   1) About: Grabs frames from the camera and adds text to it
   2) Status: compiled and runned successfully
5) [ ] [[file:code/computer-vision/qt-test/videoprocessor][videoprocessor]] ([[https://amin-ahmadi.com/2018/03/29/how-to-read-process-and-display-videos-using-qt-and-opencv/][src]]):
   1) About: it grabs frames from camera using opencv and displays it on Qt
   2) Status: compiled and runned successfully
6) [X] [[file:code/computer-vision/filters/filter/main.cpp][filter]] (src)
   1) About: it overlays a transparent png image file over a live video feed
   2) Status: compiled and runned successfully

** Build and integrate with Qt5
[[https://lordsoftech.com/open-source/adding-opencv-libraries-to-a-qt5-project-in-ubuntu-20-04/][src]]
*** ✔ DONE Build opencv [7/7]
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-31 sex 01:54]
    :END:
1) [X] Install dependencies ([[https://www.itsfoss.net/how-to-install-and-configure-opencv-on-ubuntu-20-04/][src]])
   #+BEGIN_SRC bash
sudo apt install build-essential cmake git pkg-config libgtk- 3 -dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev libjpeg-dev libpng-dev libtiff-dev gfortran openexr libatlas-base-dev python3-dev python3-numpy libtbb2 libtbb-dev libdc1394- 22 -dev
   #+END_SRC
2) [X] Clone the repository containing the last release and create a build
   folder
   #+BEGIN_SRC bash
mkdir ~/opencv_build && cd ~/opencv_build
git clone https://github.com/opencv/opencv.git && git clone https://github.com/opencv/opencv_contrib.git
cd opencv && mkdir build && cd build
   #+END_SRC
3) [X] Set the flags
   1) WORK: it compiled
	 #+BEGIN_SRC bash
	 cmake -D CMAKE_BUILD_TYPE=RELEASE \ -D CMAKE_INSTALL_PREFIX=/usr/local \ -D INSTALL_PYTHON_EXAMPLES=ON \ -D BUILD_EXAMPLES=ON \ -D OPENCV_GENERATE_PKGCONFIG=ON \ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_build/opencv_contrib/modules \ -D INSTALL_C_EXAMPLES=ON  .. 
	 #+END_SRC
   2) WORK: failed to compile after
	#+BEGIN_SRC bash
	cmake -D CMAKE_BUILD_TYPE=RELEASE \ -D CMAKE_INSTALL_PREFIX=/usr/local \ -D INSTALL_PYTHON_EXAMPLES=ON \ -D BUILD_EXAMPLES=ON \ -D OPENCV_GENERATE_PKGCONFIG=ON \ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_build/opencv_contrib/modules \ -D INSTALL_C_EXAMPLES=ON -D OPENCV_ENABLE_NONFREE=ON -D CMAKE_C_compiler=gcc -D CMAKE_C++_compiler=g++ -D WITH_QT=ON -D BUILD_PROTOBUF=OFF -D WITH_LAPACK=OFF -D WITH_VTK=OFF -D BUILD_opencv_viz=OFF .. 
	#+END_SRC
4) [X] Compile it: it may take a while
   #+BEGIN_SRC bash
   make -j$((`nproc`-1))
   #+END_SRC
5) [X] Install
   #+BEGIN_SRC bash
   sudo make install 
   #+END_SRC
6) [X] Check installation ([[https://medium.com/@pokhrelsuruchi/setting-up-opencv-for-python-and-c-in-ubuntu-20-04-6b0331e37437][src]])
   #+BEGIN_SRC bash
   ls /usr/local/lib
   #+END_SRC
   If the terminal shows list of files =libopencv*=, then the installation went
   successfully
7) [X] Configuring paths to link libraries and include directories ([[https://medium.com/@pokhrelsuruchi/setting-up-opencv-for-python-and-c-in-ubuntu-20-04-6b0331e37437][src]]):
   1) For the new opencv4, the file path for the actual installation and that
      which is mentioned in the header files is different. This is the reason
      why despite following all the proper steps of installation, your project
      will not execute due to library linking errors. The following command
      should be entered in order to fix this issue.
      #+BEGIN_SRC bash
      sudo ln -s /usr/local/include/opencv4/opencv2 /usr/local/include/opencv2
      #+END_SRC
   2) For linking the “.so” libraries, we create a new directory named opencv2
      at /usr/local/lib and move all the .so files relating to the opencv at the
      ~/build/lib to /usr/local/lib/opencv2.
      #+BEGIN_SRC bash
      sudo mkdir /usr/local/lib/opencv2
      sudo cp ~/build/lib /usr/local/lib/opencv2
      #+END_SRC
*** ✔ DONE Compile a simple standlone project
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-31 sex 01:54]
    :END:
**** ✔ DONE Makefile [3/3]
     :LOGBOOK:
     - State "✔ DONE"     from              [2021-12-31 sex 01:54]
     :END:
 1) [X] [[file:code/computer-vision/test-opencv/src/makefile][create a makefile]]: it must define =INC= and =LIBS=
    #+BEGIN_SRC makefile
     INC=-I/usr/local/include/opencv4
     LIBS += -L/usr/local/lib -lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_objdetect -lopencv_imgcodecs -lopencv_videoio -lopencv_features2d
     #+END_SRC
 2) [X] run =bear make= to obtain the =compile_commands.json= and setup the LSP
    mode helper
 3) [X] Run sample codes [3/3]
    1) [X] [[file:code/computer-vision/test-opencv/display_img.cpp][display_img.cpp]] (src): compiled but did not run, cuz its required to
       set up the path for samples (see [[https://forum.opencv.org/t/why-does-the-tutorial-code-from-opencv-give-unspecified-error/5104/3][here]])
    2) [X] [[file:code/computer-vision/test-opencv/basic-draw/src/basic_draw.cpp][basic_draw.cpp]] ([[https://docs.opencv.org/4.x/d3/d96/tutorial_basic_geometric_drawing.html][src]]): compiled and runned successfully
    3) [X] [[file:code/computer-vision/test-opencv/find-contours/src/find_contours.cpp][find_contours.cpp]] ([[https://docs.opencv.org/4.x/df/d0d/tutorial_find_contours.html][src]]): compiled but did not run, cuz of the same
       problem of 1)
**** ✔ DONE Cmake
     :LOGBOOK:
     - State "✔ DONE"     from              [2021-12-31 sex 01:48]
     :END:
1) [X] create a =CMakeLists.txt= file
   1) [[https://github.com/opencv/opencv/blob/master/samples/cpp/example_cmake/CMakeLists.txt][src]]
   2) [[file:code/computer-vision/test-opencv/cmake-example/CMakeLists.txt][example]]
2) [X] create a build dir and navigate there
   #+BEGIN_SRC bash
   mkdir build && cd build
   #+END_SRC
3) [X] run cmake with the following flag:
   #+BEGIN_SRC bash
   cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=1 ..
    #+END_SRC
   - It generates the =compile_commands.json= file for LSP and the =makefile=
4) [X] Run make
   #+BEGIN_SRC bash
   make 
   #+END_SRC
5) [X] Run executable
   #+END_SRC
*** ✔ DONE Integrate with Qt [2/2]
    :LOGBOOK:
    - State "✔ DONE"     from              [2021-12-31 sex 03:19]
    :END:
**** ✔ DONE Qmake [6/6]
     :LOGBOOK:
     - State "✔ DONE"     from              [2021-12-31 sex 20:58]
     :END:
 1) [X] [[file:code/computer-vision/qt-test][Create a Qt Widget project]]
    1) Add a button
    2) Implement =on_button_pressed= event handler to open the camera and overlay
       text (see [[file:code/computer-vision/test-opencv/cmake-example/example.cpp][example.cpp]])
 2) [X] Add this to qtcreator =.pro= file
    #+BEGIN_SRC c
 INCLUDEPATH += /usr/local/include/opencv4

 LIBS += -L/usr/local/lib -lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_objdetect -lopencv_imgcodecs -lopencv_videoio
    #+END_SRC
 3) [X] Use the build system to externally to obtain a makefile: [[id:7e7c3a09-af4f-4395-a7f8-681dab1b95c1][see Qt workflow]]
 4) [X] Generate =compile_commands.json= for LSP integration [0/0]: [[id:7e7c3a09-af4f-4395-a7f8-681dab1b95c1][see Qt
    workflow]]
 5) [X] Run make
 6) [X] Check the application output
    1) A GTK window should be generated when the push button is pressed
**** ✔ DONE Cmake
     :LOGBOOK:
     - State "✔ DONE"     from              [2021-12-31 sex 21:28]
     :END:
 1) [X] [[file:code/computer-vision/qt-test/opencv-cmake-test][Create a Qt Widget project]]
    1) Add a button
    2) Implement =on_button_pressed= event handler to open the camera and overlay
       text (see [[file:code/computer-vision/test-opencv/cmake-example/example.cpp][example.cpp]])
 2) [X] Add this to =CMakeLists.txt= file ([[http://amin-ahmadi.com/2018/02/23/how-to-use-cmake-with-qt5-and-opencv-3-projects/][src]])
    #+BEGIN_SRC cmake
set(OpenCV_DIR "usr/local/lib/cmake/opencv4")
find_package(OpenCV REQUIRED)
include_directories(${OpenCV_INCLUDE_DIRS})

target_link_libraries(${PROJECT_NAME} PRIVATE Qt5::Widgets ${OpenCV_LIBS})
    #+END_SRC
 3) [X] Use the build system to externally to obtain a makefile with LSP
    integration [[id:7e7c3a09-af4f-4395-a7f8-681dab1b95c1][see Qt workflow]]
     #+BEGIN_SRC bash
     mkdir build && cd build
     cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=1 ..
     #+END_SRC
 4) [X] Run make
    #+BEGIN_SRC c
    make -j11
    #+END_SRC
 5) [X] Check the application output
    #+BEGIN_SRC 
     ./execName
    #+END_SRC
    1) A GTK window should be generated when the push button is pressed
*** Read, display and process videos with Qt and OpenCV
[[https://amin-ahmadi.com/2018/03/29/how-to-read-process-and-display-videos-using-qt-and-opencv/][src]]
- [[https://bitbucket.org/amahta/videoprocessor/src/master/][Repo]]
- [[file:code/computer-vision/qt-test/videoprocessor][Local repo]]

*Steps* [0/8]
1) [ ] Create a Qt Widget app using Qt Creator
   1) In the =.pro= file add the include and libs path
2) [ ] Design the UI
   1) [ ] =QGraphicsView= widget: display video frames reas using OpenCV
      =VideoCapture= class
   2) [ ] =QLineEdit= widget: get the user entered camera index, file path or
      RTSP feed URL
3) [ ] =mainwindow.h=
   1) [ ] Add includes and definitions
      #+BEGIN_SRC cpp
#include <QMainWindow>
#include <QDebug>
#include <QGraphicsScene>
#include <QGraphicsPixmapItem>
#include <QImage>
#include <QPixmap>
#include <QCloseEvent>
#include <QMessageBox>

#include "opencv2/opencv.hpp"
      
      #+END_SRC
   2) [ ] Add protected and privated entries
      #+BEGIN_SRC cpp
protected:
    void closeEvent(QCloseEvent *event);

private:
    Ui::MainWindow *ui;

    QGraphicsPixmapItem pixmap;
    cv::VideoCapture video;
      #+END_SRC
      1) We'll be using the =closeEvent= to prevent closing the UI before
         stopping the video
4) [ ] Prepare the graphics viewer
   1) [ ] =mainwindow.cpp= constructor:
      #+BEGIN_SRC cpp
ui->graphicsView->setScene(new QGraphicsScene(this));
ui->graphicsView->scene()->addItem(&pixmap);
      #+END_SRC
5) [ ] Prevent closing the window:
   1) while the video is being read and processed or the camera is being
      accessed, we need to make sure =MainWindow= can't be closed
      #+BEGIN_SRC cpp
void MainWindow::closeEvent(QCloseEvent* event)
{
	if (video.isOpened())
	{
		QMessageBox::warning(this,
			"Warning",
			"Stop the video before closing the application!");
		event->ignore();
	}
	else
	{
		event->accept();
	}
}
      #+END_SRC
   2) In general, if you want to prevent closing a Qt window you have to
      override closeEvent as seen in the preceding code, and accept() or
      ignore() the event based on a condition.
6) [ ] Opening the video or camera
   1) [ ] Choose between camera, video file or RSTP feed URL: try the entered
      text in =QLineEdit= field
   2) [ ] use to =open()= function of =VideoCapture= class
   3) [ ] display relevant messages using QMessageBox in case of a wrong camera
      index, unsupported file or a RTSP feed URL that doesn’t work.
      #+BEGIN_SRC cpp
      using namespace cv;
bool isCamera;
int cameraIndex = ui->videoEdit->text().toInt(&isCamera);
if (isCamera)
{
	if (!video.open(cameraIndex))
	{
		QMessageBox::critical(this,
			"Camera Error",
			"Make sure you entered a correct camera index,"
			"<br>or that the camera is not being accessed by another program!");
		return;
	}
}
else
{
	if (!video.open(ui->videoEdit->text().trimmed().toStdString()))
	{
		QMessageBox::critical(this,
			"Video Error",
			"Make sure you entered a correct and supported video file path,"
			"<br>or a correct RTSP feed URL!");
		return;
	}
}
      #+END_SRC
7) [ ] Reading and processing frames
   1) Reading video frames using OpenCV’s VideoCapture class is quite
      simple.
      1) The trick here is that you need to put in =qApp->processEvents()=
	 into each iteration of your loop to make sure the GUI thread and other
	 events are processed
      2) otherwise you video will be stuck, your screen will
	 not be refreshed with new frames and most probably you’ll need to crash
	 your app to be able to stop or quit it.
      3) This is basically the most simple way of making sure your user
         interface is responsive while you read and process video frames.
      4) For more tutorials about this topic, and especially to learn how this
         is done using QThread classes and proper multi-threading, you can refer
         to my book [[file:biblio/computer-vision/OpenCV3&Qt5][Computer Vision with OpenCV 3 and Qt5]], which also contains
         tons of examples to boost your Qt and OpenCV skills.
	 #+BEGIN_SRC cpp
Mat frame;
while (video.isOpened())
{
	video >> frame;
	if (!frame.empty())
	{
		// process and display frames
	}
	qApp->processEvents();
}
	 #+END_SRC
8) [ ] Display video frames (and images): to be able to properly display OpenCV
   Mat images on a Qt QGraphicsView, you need to do the following:
   1) [ ] Convert Mat to QImage (This obviously means convert OpenCV Mat class
      to Qt QImage class)
   2) [ ] Since default OpenCV color space is BGR (Blue-Green-Red), you need to
      swap Red and Blue channels so that it’s a standards RGB image. (Miss this
      and you’ll get all funny colors 🙂 )
   3) [ ] Convert QImage to QPixmap (image to pixmap)
   4) [ ] Create a QGraphicsPixmapItem using the QPixmap (create an item using
      the pixmap)
   5) [ ] Add the QGraphicsPixmapItem to the QGraphicsScene of the view (set the
      item to the scene of the view)
      #+BEGIN_SRC cpp
      QImage qimg(frame.data, frame.cols, frame.rows, frame.step, QImage::Format_RGB888);
      pixmap.setPixmap( QPixmap::fromImage(qimg.rgbSwapped()) );
      #+END_SRC
      1) Using =Format_RGB888= ensures that image data is converted as it is
         supposed to,
      2) and =rgbSwapped()= takes care of swapping the Blue and Red channels in
         a QImage.
      3) Note that the item was added to the scene at the initialization,
         remember the following in MainWindow constructor:
	 #+BEGIN_SRC cpp
	 ui->graphicsView->scene()->addItem(&pixmap);
	 #+END_SRC
      4) Optionally you can use the following to make sure the image always fits
         the view no matter the window size of your application:
	 #+BEGIN_SRC cpp
	 ui->graphicsView->fitInView(&pixmap, Qt::KeepAspectRatio);
	 #+END_SRC
	 1) Note that you can replace =KeepAspectRatio= with any of the
            following to change how your images and video frames are resize to
            fit the view:
	    1) =IgnoreAspectRatio=
	    2) =KeepAspectRatio=
	    3) =KeepAspectRatioByExpanding=
9) Example using =copyMakeBorder= process ([[https://amin-ahmadi.com/2018/03/29/how-to-read-process-and-display-videos-using-qt-and-opencv/][src]]):
   1) it mirrors multiple images to fill the view
* Twitter sharing
** ✔ DONE Account
   :PROPERTIES:
   :ATTACH_DIR: /home/zmpl/OneDrive-UM/Univ/MI_Electro/Sem7/SEC/2021-22/repo/Proj/sec/img/
   :END:
   :LOGBOOK:
   - State "✔ DONE"     from              [2022-01-19 qua 13:04]
   :END:
 1) [X] Create account:
    1) username: @MDO_ESRG
    2) pass: MDO-Ze-Hugo@2021
    3) email: id6892@alunos.uminho.pt
    4) *Developer keys*:
       1) *API key*: =kmCBLr8hgj5I5KYqAQt2CDSRS=
	  1) Think of the API key as the user name that represents your App when
             making API requests. It helps us verify who you are.
       2) *API key secret*: =1f0Bs8RqV8TvtecYU4sD6Jpzz0pnYtdCW1k8gMKDxmbhczWSw1=
	  1) Your API Key Secret is like a password and helps verify your API
             Key. This will be one of the last times you'll see it displayed, so
             remember to save it in a safe place.
       3) *Bearer token*:
	  =AAAAAAAAAAAAAAAAAAAAAKj8YAEAAAAAeVpmEQUg1VD2W%2BTAH%2BVGpMX2vFg%3DyUEBsVOBN9tn1XzHf9a2uhU1pjgEXEH6IGfBbKhBlJaEYjU38q=
	  1) An Access Token used in authentication that allows you to pull
             specific data.
[[file:sec/img/twitter-keys.png]]

#+BEGIN_EXAMPLE
API key:
kmCBLr8hgj5I5KYqAQt2CDSRS

API key secret:
1f0Bs8RqV8TvtecYU4sD6Jpzz0pnYtdCW1k8gMKDxmbhczWSw1

Bearer token:
AAAAAAAAAAAAAAAAAAAAAKj8YAEAAAAAeVpmEQUg1VD2W%2BTAH%2BVGpMX2vFg%3DyUEBsVOBN9tn1XzHf9a2uhU1pjgEXEH6IGfBbKhBlJaEYjU38q
#+END_EXAMPLE

Other app: v1.1
#+BEGIN_EXAMPLE
API key:
rVgHlnUTgSQlaIOq5O1Es63QG

API key secret:
0jchuatg6RYldjsdQ8jrzmigiZbmsBhTSJkG8huMuQm352NnWO

Bearer token:
AAAAAAAAAAAAAAAAAAAAAFIAYQEAAAAAseeLeNxMedbnoWhp0D9QmAr8iE0%3D4XygdbEdebzhIsuiPThOIqU3IJIiQqzocR75vHaMTmw1HMmJt0

Access Token:
1483403576558895106-YOcwI1mbBMBqVZoiGMtB6taancaAKO

Access Token secret
oEwtGWK0DxwD8puE7PI2fhcelcGUa0sFWJB2W2Nj6Xlu3

#+END_EXAMPLE
** ✔ DONE Access to Twitter API
   :LOGBOOK:
   - State "✔ DONE"     from              [2022-01-19 qua 13:04]
   :END:
 1) [X] Access to Twitter API: [[https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api][src]]
    1) [X] Sign up for a developer account
    2) [X] Save the application's keys and tokens and keep them secure
       1) API Key and secret
       2) A set of user Access tokens
       3) Bearer token
    3) [X] Make the first request: test an endpoint
       1) Select a sample tweet: ([[https://developer.twitter.com/en/portal/register/playground][src]]) 
       2) I've created the following shell variables:
	  #+BEGIN_SRC bash
export TWIT_BEARER_TOKEN="AAAAAAAAAAAAAAAAAAAAAKj8YAEAAAAAeVpmEQUg1VD2W%2BTAH%2BVGpMX2vFg%3DyUEBsVOBN9tn1XzHf9a2uhU1pjgEXEH6IGfBbKhBlJaEYjU38q"
export TWIT_API_KEY="1f0Bs8RqV8TvtecYU4sD6Jpzz0pnYtdCW1k8gMKDxmbhczWSw1"
export TWIT_API_KEY_SECRET="kmCBLr8hgj5I5KYqAQt2CDSRS"
	  #+END_SRC
       3) Run the command:
	  #+BEGIN_SRC bash
	  curl -X GET -H "Authorization: Bearer $TWIT_BEARER TOKEN" "https://api.twitter.com/2/tweets/20"
	  #+END_SRC
       4) Received the following response:
	  #+BEGIN_SRC bash
	  {"data":{"id":"20","text":"just setting up my twttr"}}
	  #+END_SRC
    4) [X] Apply for additional access (optional)
** ✔ DONE Example program
   :LOGBOOK:
   - State "✔ DONE"     from              [2022-01-19 qua 13:04]
   :END:
[[file:code/twitter/twitcurl/twitterClient][src]]
TwitterClient is a client for twitter using the =twitcurl= library (C++).

*Setup*: [8/8]
1) [X] Create an App in the Twitter developer portal
   1) The app must be standlone to use the v1.1. API
2) [X] Give the app the correct permissions
   1) In the twitter developer portal, select the App
   2) Select Settings
   3) Select User Authentication Settings
   4) Enable OAuth 1.0a
   5) In the oauth 1.0a settings, select the permissions:
      1) Read and write and direct messages
   6) Fill the General Authentication Settings:
      1) Callback URI / Redirect URL: =https://mydomain.pt/logout=
      2) Website URL: =https://mydomain.pt=
   7) Save it
3) [X] Generate the tokens
   1) Select the App
   2) Select =Keys and Tokens=
   3) Select =Generate Access Token and Secret= and copy them
   4) The consumer keys should already been generated too (they are used to
      authenticate the user/developer) and copied
4) [X] Copy the =Access token and secret= to:
   1) [X] =twitterClient_token_key.txt=: Access token
   2) [X] =twitterClient_token_secret.txt=: Access token secret
5) [X] Copy the =Consumer key and Secret= to =main.cpp= as follows:
   #+BEGIN_SRC c++
#define API_KEY "rVgHlnUTgSQlaIOq5O1Es63QG"
#define API_KEY_SECRET "0jchuatg6RYldjsdQ8jrzmigiZbmsBhTSJkG8huMuQm352NnWO"
   #+END_SRC
6) [X] Install =libcurldev=: =sudo apt-get install libcurl4-gnutls-dev=
7) [X] Run =sudo make=
8) [X] Run the program: =./twitterClient -u @MDO_ESRG -p MDO-Ze-Hugo@2021=
   1) [X] It will prompt for proxy server: say no =0=
   2) [X] Answer the prompts interactively to post on twitter and get answers

*Output*:
#+BEGIN_EXAMPLE
twitterClient:: twitCurl::accountVerifyCredGet web response:
{"id":1483403576558895106,"id_str":"1483403576558895106","name":"MDO","screen_name":"MDO_ESRG2
021","location":"","description":"Marketing Digital Outdoor: ESRG 2021\/22","url":null,"entiti
es":{"description":{"urls":[]}},"protected":false,"followers_count":0,"friends_count":0,"liste
d_count":0,"created_at":"Tue Jan 18 11:39:27 +0000 2022","favourites_count":0,"utc_offset":nul
l,"time_zone":null,"geo_enabled":false,"verified":false,"statuses_count":0,"lang":null,"contri
butors_enabled":false,"is_translator":false,"is_translation_enabled":false,"profile_background
_color":"F5F8FA","profile_background_image_url":null,"profile_background_image_url_https":null
,"profile_background_tile":false,"profile_image_url":"http:\/\/pbs.twimg.com\/profile_images\/
1483405696058200065\/i1bfwFgo_normal.jpg","profile_image_url_https":"https:\/\/pbs.twimg.com\/
profile_images\/1483405696058200065\/i1bfwFgo_normal.jpg","profile_banner_url":"https:\/\/pbs.
twimg.com\/profile_banners\/1483403576558895106\/1642506478","profile_link_color":"1DA1F2","pr
ofile_sidebar_border_color":"C0DEED","profile_sidebar_fill_color":"DDEEF6","profile_text_color
":"333333","profile_use_background_image":true,"has_extended_profile":true,"default_profile":t
rue,"default_profile_image":false,"following":false,"follow_request_sent":false,"notifications
":false,"translator_type":"none","withheld_in_countries":[],"suspended":false,"needs_phone_ver
ification":false}  


Enter a new status message: Hello

twitterClient:: twitCurl::statusUpdate web response:
{"created_at":"Wed Jan 19 00:34:43 +0000 2022","id":1483598763251929093,"id_str":"148359876325
1929093","text":"Hello","truncated":false,"entities":{"hashtags":[],"symbols":[],"user_mention
s":[],"urls":[]},"source":"\u003ca href=\"https:\/\/mydomain.pt\" rel=\"nofollow\"\u003eLSAppE
SRG2021\u003c\/a\u003e","in_reply_to_status_id":null,"in_reply_to_status_id_str":null,"in_repl
y_to_user_id":null,"in_reply_to_user_id_str":null,"in_reply_to_screen_name":null,"user":{"id":
1483403576558895106,"id_str":"1483403576558895106","name":"MDO","screen_name":"MDO_ESRG2021","
location":"","description":"Marketing Digital Outdoor: ESRG 2021\/22","url":null,"entities":{"
description":{"urls":[]}},"protected":false,"followers_count":0,"friends_count":0,"listed_coun
t":0,"created_at":"Tue Jan 18 11:39:27 +0000 2022","favourites_count":0,"utc_offset":null,"tim
e_zone":null,"geo_enabled":false,"verified":false,"statuses_count":1,"lang":null,"contributors
_enabled":false,"is_translator":false,"is_translation_enabled":false,"profile_background_color
":"F5F8FA","profile_background_image_url":null,"profile_background_image_url_https":null,"prof
ile_background_tile":false,"profile_image_url":"http:\/\/pbs.twimg.com\/profile_images\/148340
5696058200065\/i1bfwFgo_normal.jpg","profile_image_url_https":"https:\/\/pbs.twimg.com\/profil
e_images\/1483405696058200065\/i1bfwFgo_normal.jpg","profile_banner_url":"https:\/\/pbs.twimg.
com\/profile_banners\/1483403576558895106\/1642506478","profile_link_color":"1DA1F2","profile_
sidebar_border_color":"C0DEED","profile_sidebar_fill_color":"DDEEF6","profile_text_color":"333
333","profile_use_background_image":true,"has_extended_profile":true,"default_profile":true,"d
efault_profile_image":false,"following":false,"follow_request_sent":false,"notifications":fals
e,"translator_type":"none","withheld_in_countries":[]},"geo":null,"coordinates":null,"place":n
ull,"contributors":null,"is_quote_status":false,"retweet_count":0,"favorite_count":0,"favorite
d":false,"retweeted":false,"lang":"en"}                                                      

Enter message id to reply to : 1483598763251929093

Enter a reply message: Hello Twitcurl

twitterClient:: twitCurl::statusUpdate web response:
{"created_at":"Wed Jan 19 00:35:44 +0000 2022","id":1483599016738885632,"id_str":"148359901673
8885632","text":"Hello Twitcurl","truncated":false,"entities":{"hashtags":[],"symbols":[],"use
r_mentions":[],"urls":[]},"source":"\u003ca href=\"https:\/\/mydomain.pt\" rel=\"nofollow\"\u0
03eLSAppESRG2021\u003c\/a\u003e","in_reply_to_status_id":1483598763251929093,"in_reply_to_stat
us_id_str":"1483598763251929093","in_reply_to_user_id":1483403576558895106,"in_reply_to_user_i
d_str":"1483403576558895106","in_reply_to_screen_name":"MDO_ESRG2021","user":{"id":14834035765
58895106,"id_str":"1483403576558895106","name":"MDO","screen_name":"MDO_ESRG2021","location":"
","description":"Marketing Digital Outdoor: ESRG 2021\/22","url":null,"entities":{"description
":{"urls":[]}},"protected":false,"followers_count":0,"friends_count":0,"listed_count":0,"creat
ed_at":"Tue Jan 18 11:39:27 +0000 2022","favourites_count":0,"utc_offset":null,"time_zone":nul
l,"geo_enabled":false,"verified":false,"statuses_count":2,"lang":null,"contributors_enabled":f
alse,"is_translator":false,"is_translation_enabled":false,"profile_background_color":"F5F8FA",
"profile_background_image_url":null,"profile_background_image_url_https":null,"profile_backgro
und_tile":false,"profile_image_url":"http:\/\/pbs.twimg.com\/profile_images\/14834056960582000
65\/i1bfwFgo_normal.jpg","profile_image_url_https":"https:\/\/pbs.twimg.com\/profile_images\/1
483405696058200065\/i1bfwFgo_normal.jpg","profile_banner_url":"https:\/\/pbs.twimg.com\/profil
e_banners\/1483403576558895106\/1642506478","profile_link_color":"1DA1F2","profile_sidebar_bor
der_color":"C0DEED","profile_sidebar_fill_color":"DDEEF6","profile_text_color":"333333","profi
le_use_background_image":true,"has_extended_profile":true,"default_profile":true,"default_prof
ile_image":false,"following":false,"follow_request_sent":false,"notifications":false,"translat
or_type":"none","withheld_in_countries":[]},"geo":null,"coordinates":null,"place":null,"contri
butors":null,"is_quote_status":false,"retweet_count":0,"favorite_count":0,"favorited":false,"r
etweeted":false,"lang":"en"}
#+END_EXAMPLE
** Twitcurl API
*structure*:
- [[file:code/twitter/twitcurl/twitterClient/include/oauthlib.h][/include/oauthlib.h]]: contains the authentication mechanisms for Twitter
  - it also contains an interesting way of including constants in a src file
- [[file:code/twitter/twitcurl/twitterClient/include/twitcurl.h][/include/twitcurl.h]]: prototypes to handle twitter platform
- [[file:code/twitter/twitcurl/twitterClient/include/curl][/include/curl]]: curl library
*** Account Management
1) set username and password:
   #+BEGIN_SRC c++
   // twitcurl.h
   void setTwitterUsername( const std::string& userName /* in */ );
   void setTwitterPassword( const std::string& passWord /* in */ );
   //twitterClient.cpp
   twitCurl twitterObj;
   twitterObj.setTwitterUsername( userName );
   twitterObj.setTwitterPassword( passWord );
   #+END_SRC
2) OAuth flow:
   1) Set consumer key and secret
      #+BEGIN_SRC c++
      void setConsumerKey( const std::string& consumerKey /* in */ );
      void setConsumerSecret( const std::string& consumerSecret /* in */ );
      
    /* Step 0: Set OAuth related params. These are got by registering your app at twitter.com */
      twitterObj.getOAuth().setConsumerKey( std::string( API_KEY ) );
      twitterObj.getOAuth().setConsumerSecret( std::string( API_KEY_SECRET ) );
      #+END_SRC
   2) Check if we already have OAuth tokens
      #+BEGIN_SRC c++
    /* Step 1: Check if we alredy have OAuth access token from a previous run */
    std::string myOAuthAccessTokenKey("");
    std::string myOAuthAccessTokenSecret("");
    std::ifstream oAuthTokenKeyIn;
    std::ifstream oAuthTokenSecretIn;

    oAuthTokenKeyIn.open( "twitterClient_token_key.txt" );
    oAuthTokenSecretIn.open( "twitterClient_token_secret.txt" );

    memset( tmpBuf, 0, 1024 );
    oAuthTokenKeyIn >> tmpBuf;
    myOAuthAccessTokenKey = tmpBuf;

    memset( tmpBuf, 0, 1024 );
    oAuthTokenSecretIn >> tmpBuf;
    myOAuthAccessTokenSecret = tmpBuf;

    oAuthTokenKeyIn.close();
    oAuthTokenSecretIn.close();

    if( myOAuthAccessTokenKey.size() && myOAuthAccessTokenSecret.size() )
    {
        /* If we already have these keys, then no need to go through auth again */
        printf( "\nUsing:\nKey: %s\nSecret: %s\n\n", myOAuthAccessTokenKey.c_str(), myOAuthAccessTokenSecret.c_str() );

        twitterObj.getOAuth().setOAuthTokenKey( myOAuthAccessTokenKey );
        twitterObj.getOAuth().setOAuthTokenSecret( myOAuthAccessTokenSecret );
    }
      #+END_SRC
3) Account credentials verification
   #+BEGIN_SRC c++
    /* Account credentials verification */
    if( twitterObj.accountVerifyCredGet() )
    {
        twitterObj.getLastWebResponse( replyMsg );
        printf( "\ntwitterClient:: twitCurl::accountVerifyCredGet web response:\n%s\n", replyMsg.c_str() );
    }
    else
    {
        twitterObj.getLastCurlError( replyMsg );
        printf( "\ntwitterClient:: twitCurl::accountVerifyCredGet error:\n%s\n", replyMsg.c_str() );
    }
   #+END_SRC
*** Handle responses and errors
#+BEGIN_SRC c++
// twitcurl.h
    /* cURL APIs */
    bool isCurlInit();
    void getLastWebResponse( std::string& outWebResp /* out */ );
    void getLastCurlError( std::string& outErrResp /* out */);

    /* Internal cURL related methods */
    int saveLastWebResponse( char*& data, size_t size );

// TwitterClient.cpp
    if( twitterObj.accountVerifyCredGet() )
    {
        twitterObj.getLastWebResponse( replyMsg );
        printf( "\ntwitterClient:: twitCurl::accountVerifyCredGet web response:\n%s\n", replyMsg.c_str() );
    }
    else
    {
        twitterObj.getLastCurlError( replyMsg );
        printf( "\ntwitterClient:: twitCurl::accountVerifyCredGet error:\n%s\n", replyMsg.c_str() );
    }
#+END_SRC
*** Post a new status message
    #+BEGIN_SRC c++
    /* Post a new status message */
    memset( tmpBuf, 0, 1024 );
    printf( "\nEnter a new status message: " );
    fgets( tmpBuf, sizeof( tmpBuf ), stdin );
    tmpStr = tmpBuf;
    replyMsg = "";
    if( twitterObj.statusUpdate( tmpStr ) )
    {
        twitterObj.getLastWebResponse( replyMsg );
        printf( "\ntwitterClient:: twitCurl::statusUpdate web response:\n%s\n", replyMsg.c_str() );
    }
    else
    {
        twitterObj.getLastCurlError( replyMsg );
        printf( "\ntwitterClient:: twitCurl::statusUpdate error:\n%s\n", replyMsg.c_str() );
    }
    #+END_SRC
*** Extensions
**** Add media to a new status message
 Twitcurl allows to post a new status message, but it doesn't enable adding
 media to it.
***** Upload media
****** Overview
A media object represents a single photo, video or animated GIF. Media objects
are used by many endpoints within the Twitter API, and may be included in
Tweets, Direct Messages, user profiles, advertising creatives and
elsewhere. Each media object may have multiple display or playback variants,
with different resolutions or formats.

******* Media types & size restrictions
Size restrictions for uploading via API 
- Image 5 MB
- GIF 15 MB
- Video 512 MB (when using media_category=amplify)

******* Creation
Objects such as Tweets, Direct Messages, user profile pictures, hosted Ads
cards, etc. can contain one or more media objects. 
+ These top-level objects are collectively known as entities.
+ The relevant entity creation API (e.g. POST statuses/update) can be passed one
  or more media objects using a =unique media_id=.

An entity which contains media object(s) can be created by following these
steps:
1) *Upload the media file(s)* using either:
   1) the recommended chunked upload (images/GIF/video),
   2) or the older simple upload (images only).
2) *Receive a media_id* from step 1. This step may be repeated multiple times
   with different media if the entity allows multiple media_id parameters to be
   passed in.
3) *Create the entity by calling the appropriate endpoint*, including the
   media_id and other required parameters.
   + For example, attach a =media_id= to a Tweet using the POST statuses/update
     endpoint.

****** Best Practices
 [[https://developer.twitter.com/en/docs/twitter-api/v1/media/upload-media/uploading-media/media-best-practices][src]]
******* Keep in mind                                              :Important:
 *Because the method uses multipart POST, OAuth is handled differently*. POST or
 query string parameters are not used when calculating an OAuth signature
 basestring or signature. *Only the =oauth_*= parameters are used*.
 - You may attach up to 4 photos, 1 animated GIF or 1 video in a Tweet.
 - The image passed should be the raw binary of the image or binary base64
   encoded, no need to otherwise encode or escape the contents as long as the
   Content-Type is set appropriately (when in doubt: application/octet-stream).
 - When posting base64 encoded images, be sure to set the
   “Content-Transfer-Encoding: base64” on the image part of the message.
 - Multi-part message boundaries must be on their own line and terminated by a
   CRLF.
 - For working examples of how to POST using this endpoint, we recommend testing
   with *twurl*. Also, take a look at the Twitter Libraries available, including
   the large-video-upload-python code sample.
 - Use the media_id_string provided in the API response for Javascript and any
   other languages that cannot accurately represent a long integer.

******* Media Categories
 The Media Category parameter defines the use case of the media file to be
 uploaded, and can affect file size limits or other constraints enforced for
 media uploads. It’s important to use the correct media category when uploading
 media to avoid problems when trying to use the media. It is an optional value
 passed in the INIT request as part of the upload flow. 
 - *If media category is not specified, the uploaded media is assumed to be a
   Tweet media (image, video, or GIF), depending on the content type*.


 The most common media categories are as follows:
 - TweetImage
 - TweetVideo
 - TweetGif
 - DmImage
 - DmVideo
 - DmGif
 - Subtitles

  If you are an Ads API partner please refer to these docs for more information on recommended media category for promoted video.

******* Image specifications and recommendations                  :Important:
 Image files must meet all of the following criteria:
 - Supported image media types: JPG, PNG, GIF, WEBP
 - Size:
   - Image size <= 5 MB
   - animated GIF size <= 15 MB

 The file size limit above is enforced by the media upload endpoint. In addition, there is a separate product entity specific file size limit which is applied when calling the Tweet creation (or similar) endpoints with media_id. The file size limit and other constraints may vary depending on the media_category parameter.

******* Animated GIF recommendations                              :Important:
  A GIF may fail during Tweet creation even if it is within the file size limit. Adhere to the following constraints to improve success rates.
 - Resolution should be <= 1280x1080 (width x height)
 - Number of frames <= 350
 - Number of pixels (width * height * num_frames) <= 300 million
 - File size <= 15Mb

  In order to process larger GIFs, use the chunked upload endpoint with the media_category parameter. This allows the server to process the GIF file asynchronously, which is a requirement for processing larger files. Pass media_category=tweet_gif to enable async upload behavior for Tweets with an animated GIF.
 
******* Video specifications and recommendations

  Please use the Async Path for media uploads.

  Recommended:

      Recommended Video Codec: H264 High Profile
      Recommended Frame Rates: 30 FPS, 60 FPS
      Recommended Video Resolution: 1280x720 (landscape), 720x1280 (portrait), 720x720 (square)
      Recommended Minimum Video Bitrate: 5,000 kbps
      Recommended Minimum Audio Bitrate: 128 kbps
      Recommended Audio Codec: AAC LC
      Recommended Aspect Ratio: 16:9 (landscape or portrait), 1:1 (square)

  Advanced:

      Frame rate must be 60 FPS or less
      Dimensions must be between 32x32 and 1280x1024
      File size must not exceed 512 mb
      Duration must be between 0.5 seconds and 140 seconds
      Aspect ratio must be between 1:3 and 3:1
      Must have 1:1 pixel aspect ratio
      Only YUV 4:2:0 pixel format is supported
      Audio must be AAC with Low Complexity profile. High-Efficiency AAC is not supported
      Audio must be mono or stereo, not 5.1 or greater
      Must not have open GOP
      Must use progressive scan

****** API reference
******* =POST media/upload (INIT)=
[[https://developer.twitter.com/en/docs/twitter-api/v1/media/upload-media/api-reference/post-media-upload-init][src]]
******** Overview
 The INIT command request is used to initiate a file upload session. It returns a media_id which should be used to execute all subsequent requests. The next step after a successful return from INIT command is the APPEND command.

 See the Uploading media guide for constraints and requirements on media files.

******** Request
Requests should be either multipart/form-data or application/x-www-form-urlencoded POST formats.

 Note: The domain for this endpoint is upload.twitter.com

******** Response
The response provides a media identifier in the media_id (64-bit integer) and media_id_string (string) fields. Use the media_id_string provided in the API response from JavaScript and other languages that cannot accurately represent a long integer.

The entire file must be uploaded before expires_after_secs seconds.

The additional_owners field enables media to be uploaded media as user A and then used to create Tweets as user B.

******** Parameters
| Name        | Req. | Description                               | Default Val | Example   |
| command     | yes  | must be set to INIT (case sensitive)      |             |           |
| total_bytes | yes  | size of media being uploaded in bytes     |             |           |
| media_type  | yes  | the MIME type of the media being uploaded |             | video/mp4 |
|             |      |                                           |             |           |
- command 	required 	Must be set to INIT (case sensitive). 		
- total_bytes 	required 	The size of the media being uploaded in bytes. 		
- media_type 	required 	The MIME type of the media being uploaded.
  video/mp4

******** Example request
#+BEGIN_SRC bash
POST https://upload.twitter.com/1.1/media/upload.json?command=INIT&total_bytes=10240&media_type=image/jpeg

https://upload.twitter.com/1.1/media/upload.json?command=INIT&total_bytes=36287&media_type=image%2Fjpeg                                                                             

Enter a new status message: Hello test friday
postUrl: https://api.twitter.com/1.1/statuses/update.json
dataStr: status=Hello%20test%20friday%0A
AuthHeader: Authorization: OAuth oauth_consumer_key="rVgHlnUTgSQlaIOq5O1Es63QG",oauth_nonce="1
642768834262",oauth_signature="XIGrtr1gX7ZBzFGvcu8tM1kJVTs%3D",oauth_signature_method="HMAC-SH
A1",oauth_timestamp="1642768834",oauth_token="1483403576558895106-YOcwI1mbBMBqVZoiGMtB6taancaA
KO",oauth_version="1.0"


postUrl: https://upload.twitter.com/1.1/media/upload.json
dataStr: ?command=INIT&total_bytes=36287&media_type=image%2Fjpeg
AuthHeader: Authorization: OAuth oauth_consumer_key="rVgHlnUTgSQlaIOq5O1Es63QG",oauth_nonce="1
6427688357b",oauth_signature="Bdm%2FbA%2ByEbao7BBVZA2it25H2ZE%3D",oauth_signature_method="HMAC
-SHA1",oauth_timestamp="1642768835",oauth_token="1483403576558895106-YOcwI1mbBMBqVZoiGMtB6taan
caAKO",oauth_version="1.0"                                                                   
UPLOAD INIT response: 
{"errors":[{"code":38,"message":"media parameter is missing."}]}
UPLOAD INIT error: 
#+END_SRC

******** Example Response
#+BEGIN_SRC js
{
  "media_id": 710511363345354753,
  "media_id_string": "710511363345354753",
  "size": 11065,
  "expires_after_secs": 86400,
  "image": {
    "image_type": "image/jpeg",
    "w": 800,
    "h": 320
  }
}
#+END_SRC

******* =POST media/upload (APPEND)=
[[https://developer.twitter.com/en/docs/twitter-api/v1/media/upload-media/api-reference/post-media-upload-append][src]]

******** Overview
The APPEND command is used to upload a chunk (consecutive byte range) of the
media file. 
For example, a 3 MB file could be split into 3 chunks of size 1 MB, and uploaded
using 3 APPEND command requests. 
After the entire file is uploaded, the next step is to call the FINALIZE command.

There are a number of advantages of uploading a media file in small chunks:
- Improved reliability and success rates under low bandwidth network conditions
- Uploads can be paused and resumed
- File chunks can be retried individually
- Ability to tune chunk sizes to match changing network conditions e.g on
  cellular clients

******** Request
Requests should be multipart/form-data POST format.

Note: The domain for this endpoint is upload.twitter.com

******** Response
A successful response returns HTTP 2xx.

******** Resource URL
https://upload.twitter.com/1.1/media/upload.json

******** Resource Information
- Response formats  JSON  
- Requires authentication?  Yes (user context only)  
- Rate limited?  Yes  

*Parameters*
 Name  Required  Description 
 command  required  Must be set to APPEND (case sensitive).      
 media_id  required  The media_id returned from the INIT      
     command.      
 media  required  The raw binary file content being uploaded.      
     It must be <= 5 MB, and cannot be used with      
     media_data.      
 media_data  required  The base64-encoded chunk of media file. It      
     must be <= 5 MB and cannot be used with      
     media. Use raw binary (media parameter)      
     when possible.      
 segment_index  required  An ordered index of file chunk. It must be      
     between 0-999 inclusive. The first segment      
     has index 0, second segment has index 1,      
     and so on.      

******** Example Request
POST
https://upload.twitter.com/1.1/media/upload.json?command=APPEND&media_id=123&segment_index=2&media_data=123

******** Example Result
// Successful response returns HTTP 2XX code without any content body.

******* =POST media/upload (FINALIZE)=

******** Overview
The FINALIZE command should be called after the entire media file is uploaded using APPEND commands.
- If and (only if) the response of the FINALIZE command contains a
  processing_info field, it may also be necessary to use a STATUS command and
  wait for it to return success before proceeding to Tweet creation.

******** Request
Requests should be either multipart/form-data or application/x-www-form-urlencoded POST formats.

 Note: The domain for this endpoint is upload.twitter.com

******** Response
The response provides a media identifier in the media_id (64-bit integer) and
media_id_string (string) fields. 
- Use the media_id_string provided in the API response from JavaScript and other
  languages that cannot accurately represent a long integer.

The returned *mediaId* is only valid for expires_after_secs seconds. 
- Any attempt to use mediaId after this time period in other API calls will
  result in a Bad Request (HTTP 4xx) response.

If the response contains a processing_info field, then use the STATUS command to
poll for the status of the FINALIZE operation. Otherwise, the media_id is ready
for use in other API endpoints.

The async finalize approach is used for cases where media processing requires
more time. 
- In future, all video and animated GIF processing will only be supported using
  async finalize.
- This behavior is enabled if an upload session was initialized with a
  media_category parameter, and when then media type is either video or animated
  GIF.
******** Resource Information
- Response formats  JSON  
- Requires authentication?  Yes (user context only)  
- Rate limited?  Yes

******** Parameters
  Name  Required  Description 
  command  required  Must be set to FINALIZE (case sensitive).      
  media_id  required  The media_id returned from the INIT      
      command.      

******** Example request
 POST
 https://upload.twitter.com/1.1/media/upload.json?command=FINALIZE&media_id=710511363345354753

******** Example Result
#+BEGIN_SRC js
 // Example of sync FINALIZE response
 {
   "media_id": 710511363345354753,
   "media_id_string": "710511363345354753",
   "size": 11065,
   "expires_after_secs": 86400,
   "video": {
     "video_type": "video/mp4"
   }
 }

 // Example of async FINALIZE response which requires further STATUS command call(s)
 {
   "media_id": 710511363345354753,
   "media_id_string": "710511363345354753",
   "expires_after_secs": 86400,
   "size": 10240,
   "processing_info": {
     "state": "pending",
     "check_after_secs": 5 // check after 5 seconds for update using STATUS command
   }
 }
#+END_SRC


******* All together                                              :Important:
- =POST media/upload (INIT)=:
    #+BEGIN_SRC bash
    POST https://upload.twitter.com/1.1/media/upload.json?command=INIT&total_bytes=10240&media_type=image/jpeg
    #+END_SRC

    #+BEGIN_SRC js
    {
    "media_id": 710511363345354753,
    "media_id_string": "710511363345354753",
    "size": 11065,
    "expires_after_secs": 86400,
    "image": {
	"image_type": "image/jpeg",
	"w": 800,
	"h": 320
	}
    }
    #+END_SRC
- =POST media/upload (APPEND)=:
  #+BEGIN_SRC bash
  POST
https://upload.twitter.com/1.1/media/upload.json?command=APPEND&media_id=123&segment_index=2&media_data=123
  #+END_SRC
  #+BEGIN_SRC js
  // Successful response returns HTTP 2XX code without any content body.
  #+END_SRC
- =POST media/upload (FINALIZE)=:
  #+BEGIN_SRC bash
  POST
 https://upload.twitter.com/1.1/media/upload.json?command=FINALIZE&media_id=710511363345354753
  #+END_SRC

  #+BEGIN_SRC js
  // Example of sync FINALIZE response
  {
  "media_id": 710511363345354753,
  "media_id_string": "710511363345354753",
  "size": 11065,
  "expires_after_secs": 86400,
  "video": {
  "video_type": "video/mp4"
  }
  }

  // Example of async FINALIZE response which requires further STATUS command call(s)
  {
  "media_id": 710511363345354753,
  "media_id_string": "710511363345354753",
  "expires_after_secs": 86400,
  "size": 10240,
  "processing_info": {
"state": "pending",
"check_after_secs": 5 // check after 5 seconds for update using STATUS command
  }
  }
  #+END_SRC
* Mine                                                             :noexport:
** Implementation
*** Project: LS App
- [[file:code/LS/LSApp/readme.org][readme]]
*** Notes
- Qt app is build with =qmake= for now
  - Maybe it will be better to use cmake for integration with opencv and other
    external libraries
  - It works with cmake and qmake, so one can choose it after
*** Local System [0/2]
 1) [ ] ~HW~ [0/3]
    1) [ ] Tests [0/6]:
       1) [ ] Power Supply
       2) [ ] LCD Display
       3) [ ] Speakers
       4) [ ] Fragrance diffuser
       5) [ ] Camera
       6) [ ] Ultrasonic sensor
    2) [ ] Mechanical design
    3) [ ] PCB manufacturing
 2) [ ] SW: implementation [0/2]
    1) [ ] Linux & Raspbian tests [0/8]
       1) [ ] *UI*
	  1) [ ] Design the UI, populating with items, by phases:
	     1) Initial view: Login + Register
	     2) Main view: only if Login was successful
       2) [ ] *Sockets & Client/Server Arch* [0/3]
	  1) [ ] Connect
	  2) [ ] Send
	  3) [ ] Recv
       3) [ ] *Computer Vision* [0/4]
	  1) [ ] *Frame grabbing*
	  2) [ ] *Face detection*
	  3) [ ] *Gesture recognition*
	     1) [ ] Determine set of gestures required
	     2) [ ] Create database of gestures
	  4) [ ] /Image filter overlay/
       4) [ ] /Normal mode/ [0/3]
	  1) [ ] *Video reproduction*
	  2) [ ] *Audio reproduction*
	  3) [ ] /Fragrance diffusion/ [0/2]
	     1) [ ] Device driver
	     2) [ ] Daemon
       5) [ ] /User detection/
	  1) [ ] /Device driver/
	  2) [ ] /Daemon/
       6) [ ] /Twitter sharing/
	  1) [ ] Create post
	  2) [ ] Share
	  3) [ ] Handle response
       7) [ ] ~GIF generator~
       8) [ ] ~File transfer~
    2) [ ] Raspberry Pi deployment [0/3]
       1) [ ] Buildroot customization
       2) [ ] Startup/Shutdown sequence
       3) [ ] System verification

 _Legend_:
 - *Top priority*
 - /Medium priority/
 - ~Low priority~
*** Qt [1/5]
    :PROPERTIES:
    :ID:       7e7c3a09-af4f-4395-a7f8-681dab1b95c1
    :END:
1) [X] Start a [[file:code/LS/LSApp][QWidget application]] on Qt
2) [-] Use the build system to externally to obtain a makefile with LSP
   integration [1/2]
   1) [ ] =qmake=
       #+BEGIN_SRC bash
       cd /path/to/project
       qmake -project # if the project is not yet created, otherwise
       qmake -makefile
       make -j$(nproc) #
       #+END_SRC
      1) LSP integration: using =bear=: For a make-based build, you can 
      2) =make clean=;
      3) =bear -- make=: to generate the file (and run a clean build!).
	 1) if it is an older copy of bear, [[https://github.com/rizsotto/Bear/issues/304#issuecomment-703194531][use]]: =bear make=
   2) [X] =cmake=: see [[id:8387549a-aa60-47d8-ab41-245bd8be131e][here]]:
      1) compile_commands.json will be written to your build directory.
      2) You should symlink it (or simply copy it) to the root of your
	source tree, if they are different.
3) [-] Design the UI
   1) [X] Draw the several UI views following [[id:9fbc087a-a9db-49d1-83ff-f16804ee72dc][method 2]]
      1) [X] Welcome view: =MainWindow=
	 - displayed when no Ad is supposed to be running
	 - contains a description about the project, how to rent ads, etc.
      2) [X] Normal mode view: =NormalWindow=
	 - Reproduces video in fullscreen (with audio)
      3) [X] Interaction mode view: =InterWindow=
	 1) includes:
	    1) =Main menu=: page0
	    2) =Picture mode=: page1
	    3) =GIF mode=: page2
      4) [X] Image filtering view: =ImgFiltWindow=
	 1) includes:
	    1) =Main menu=
      5) [X] Sharing mode: =SharWindow=
	 1) includes:
	    1) =Main menu=: selection (page 0)
	    2) =Editing=: presents a virtual keyboard (page 1)
	    3) =Status=: presents a status message box to the user (page 2)
   2) [X] Use dummy pushbuttons to navigate through the several UI views
   3) [ ] =GestureRecognitionEngine= belongs to =MainWindow=.
      1) It must detect a predefined user gesture
      2) When matched, it emits a signal =gestureDetected()= for the
         =MainWindow= to handle through a =on_gestureDetected()= slot
      3) The =on_gestureDetected()= slot should identify the type of gesture and
         the screen coordinates where the gesture was mapped too, i.e:
	 #+BEGIN_SRC cpp
	    void MainWindow::on_gestureDetected(){
		enum gestType gestT = ui->GDE->gesture();
		struct coords2D coords = ui->GDE->coords();
		enum UIView curUI = ui->stackedWidget->currentIndex();
		switch (curUI){
		case UIView::INTER :
		    emit gesture_detected_Inter(gestT, coords);
		    break;
		case UIView::IMGFILT :
		    emit gesture_detected_ImgFilt(gestT, coords);
		    break;
		case UIView::SHARING :
		    emit gesture_detected_Shar(gestT, coords);
		    break;
    // ignore these events
		case UIView::WELCOME :
		case UIView::NORMAL :
		default:
		    break;
		}
	    }
	 #+END_SRC
      4) Then each =Window= class, should have a slot to handle this specific
         signal. E.g., for class =InterWindow=:
	 #+BEGIN_SRC cpp
	   InterWindow::InterWindow()
	   {
	       connect( UI::MainWindow *, gesture_detected_Inter(enum gestureType, struct coords2d), this, SLOT( on_gesture_detected_Inter(enum gestureType, struct coords2d ) ) )
	   }

	   InterWindow::on_gesture_detected_Inter(enum gestureType gestT, struct coords2d coords)
	   {
	       enum UIView curUI = ui->stackedWidget->currentIndex();
	       switch (curUI){
	       case UIPages::PAGE0 :
		   if(coords.x > 240 && coords.x < 340 && coords.y > 100 && coords.y < 200)
		       emit pushbutton_clicked();
		   else
		       if(coords.x > 20 && coords.x < 40 && coords.y > 10 && coords.y < 20)
			   emit pushbutton_2_clicked();
		   break;
		   // TODO: add more buttons to handle
	       case UIPages::PAGE1 :
		   // TODO: add more pages 
	       default:
		   break;
	       }
	   }
	 #+END_SRC
4) [ ] Classes
   1) [ ] =Camera=: manages the camera
5) [ ] Threads [0/1]
   1) [ ] Frame grabber implemented
      1) Need to emit the correct signals to trigger it
      2) Need to check resources; it is lagging, complaining about not being
         able to start timers from anoter thread
	 #+BEGIN_SRC bash
	 QObject::startTimer: Timers cannot be started from another thread 
	 #+END_SRC
*** ☛ TODO PRIORITIES [2/6]
    :LOGBOOK:
    - State "☛ TODO"     from              [2022-01-06 qui 01:35]
    :END:
1) [X] Implement face detection algorithm (see [[file:code/computer-vision/face-detection/c++/main.cpp][here]])
   1) [X] Data models are in /models
   2) [X] Add it to mainWindow
2) [X] Implement image filter overlay
   1) Image overlay algorithm ([[http://opencv-tutorials-hub.blogspot.com/2016/04/how-to-overlay-smaller-image-over-bigger-image-in-opencv.html][src]])
      1) Read the bigger image (frame)
      2) Read the smaller image (filter)
      3) Create a region of interest (ROI) over the bigger image
	 #+BEGIN_SRC c++
	 cv::Mat small_img;
	 cv::Mat big_img;
	 // somehow fill small_img and big_img with your data
	 small_image.copyTo( big_image ( 
	 cv::Rect( x, y, small_image.cols, small_image.rows))
	 );
	 #+END_SRC
      4) Full code
	 #+BEGIN_SRC c++
	   //Copying One Image/Overlaying one image over the other
	   #include <opencv2/core/core.hpp>
	   #include <opencv2/highgui/highgui.hpp>
	   #include <iostream>

	   using namespace cv;
	   using namespace std;

	   int main()
	   {
		Mat src1, src2;
		src1 = imread("C:\\Users\\arjun\\Desktop\\image1.jpg",1);
		src2 = imread("C:\\Users\\arjun\\Desktop\\opencv-logo.png",1);
		if(!src1.data){
		    cout<<"Error loading src1"<<endl;
		    return -1;
		}
		if(!src2.data){
		    cout<<"Error loading src2"<<endl;
		    return -1;
		}
		if(src1.size <= src2.size){
		    cout<<"Error First Image should be larger than Second Image"<<endl;
		}
		src2.copyTo( src1( cv::Rect(10,10,src2.cols, src2.rows) ) );

		namedWindow("Image Window src1", CV_WINDOW_AUTOSIZE);
		imshow("Image Window src1", src1);

		waitKey(0);
		return(0);
	}


	 #+END_SRC
   2) Image overlay with blending
      #+BEGIN_SRC python
   import numpy as np
   import cv2

   face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

   specs_ori = cv2.imread('glass.png', -1)
   cigar_ori = cv2.imread('cigar.png', -1)
   mus_ori = cv2.imread('mustache.png', -1)

   # Camera Init
   cap = cv2.VideoCapture(0) 
   cap.set(cv2.CAP_PROP_FPS, 30)


   def transparentOverlay(src, overlay, pos=(0, 0), scale=1):
       overlay = cv2.resize(overlay, (0, 0), fx=scale, fy=scale)
       h, w, _ = overlay.shape  # Size of foreground
       rows, cols, _ = src.shape  # Size of background Image
       y, x = pos[0], pos[1]  # Position of foreground/overlay image

       for i in range(h):
           for j in range(w):
               if x + i >= rows or y + j >= cols:
                   continue
               alpha = float(overlay[i][j][3] / 255.0)  # read the alpha channel
               src[x + i][y + j] = alpha * overlay[i][j][:3] + (1 - alpha) * src[x + i][y + j]
       return src

   while 1:
       ret, img = cap.read()
       gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
       faces = face_cascade.detectMultiScale(img, 1.2, 5, 0, (120, 120), (350, 350))
    
	   for (x, y, w, h) in faces:
           if h > 0 and w > 0:
               glass_symin = int(y + 1.5 * h / 5)
               glass_symax = int(y + 2.5 * h / 5)
               sh_glass = glass_symax - glass_symin
               face_glass_roi_color = img[glass_symin:glass_symax, x:x + w]
               specs = cv2.resize(specs_ori, (w, sh_glass), interpolation=cv2.INTER_CUBIC)

               cigar_symin = int(y + 4 * h / 6)
               cigar_symax = int(y + 5.5 * h / 6)
               sh_cigar = cigar_symax - cigar_symin
               face_cigar_roi_color = img[cigar_symin:cigar_symax, x:x + w]
               cigar = cv2.resize(cigar_ori, (w, sh_cigar), interpolation=cv2.INTER_CUBIC)

               mus_symin = int(y + 3.5 * h / 6)
               mus_symax = int(y + 5 * h / 6)
               sh_mus = mus_symax - mus_symin
               face_mus_roi_color = img[mus_symin:mus_symax, x:x + w]
               mustache = cv2.resize(mus_ori, (w, sh_mus), interpolation=cv2.INTER_CUBIC)

               transparentOverlay(face_glass_roi_color, specs)
               transparentOverlay(face_cigar_roi_color, cigar, (int(w / 2), int(sh_cigar / 2)))
               #transparentOverlay(face_mus_roi_color, mustache)

       cv2.imshow('Thug Life', img)
       key = cv2.waitKey(1) & 0xFF
       if key == ord("q"):
           break

       k = cv2.waitKey(30) & 0xff
       if k == 27:
           cv2.imwrite('img.jpg', img)
           break

   cap.release()
   cv2.destroyAllWindows()
      #+END_SRC
   3) Image overlay linear blending method ([[https://www.codetd.com/en/article/12880310][src]])
      1) Use the ROI of the region of interest to achieve image overlay
	 1) Create ROI area
	    #+BEGIN_SRC c++
	    Mat img;
	    img = imread("file.png", -1);
	    Mat imgROI;
	    imgROI = img(cv::Rect(800, 350, logo.cols, logo.rows));
	    #+END_SRC
	    1) This is equivalent to using pointer knowledge to point the roi
               area to the corresponding area of ​​the image Rect(x,y,w,h)
	    2) x,y are the coordinates of the *upper left corner* of the ROI
               area of ​​the image; the coordinates of the upper left corner of
               the image (0, 0)
	    3) =x+logo.cols= cannot be greater than the width of the image
               object image
	    4) =y+logo.rows= cannot be greater than the height of the image
               object image
	 2) Constructing ROI area mask (basic of image overlay)
	    #+BEGIN_SRC c++
	    Mat ROI = image(Rect(200, 250, logo.cols, logo.rows));
	    Mat mask = imread("logo.jpg", 0);
	    logo.copyTo(ROI, mask);
	    #+END_SRC
	 3) Code implementation and effect: The following shows how to use ROI
            to add a picture to the specified position of another picture
	    #+BEGIN_SRC c++
	      #include <opencv2/core/core.hpp>
	      #include <opencv2/highgui/highgui.hpp>
	      using namespace cv;

	      int main()
	      {
		      Mat srcImage1 = imread("49.jpg");
		      Mat logoImage = imread("61.jpg");
       
		      namedWindow("[1]");
		      imshow("[1] src", srcImage1);
		      namedWindow("[2]");
		      imshow("[2] logo", logoImage);

		      Mat imageROI = srcImage1(Rect(320, 120,
						    logoImage.cols, logoImage.rows));

		      Mat mask = imread("61.jpg", 0);
		      logoImage.copyTo(imageROI, mask);

		      namedWindow("[3]");
		      imshow("[3] ROI", srcImage1);

		      waitKey();
		      return 0;
	      }
	    
	    #+END_SRC
	    1) This function first loads two jpg images into srcImage1 and
               logoImage, then defines a Mat type imageROI, and uses cv::Rect to
               set its region of interest to a region in srcImage1, and
               associate imageROI with srcImage1.
	    2) Then define a Mat type mask and read in dota_logo.jpg, use Mat::
               copyTo to copy the contents of the mask to the imageROI, and then
               get the final rendering, namedWindow and imshow are used together
               to show the final the result of.
      2) The =addWeighted= function realizes linear blending of images
	 #+BEGIN_QUOTE
Linear mixing means that two pictures or two videos are superimposed and presented in a certain functional relationship.

For the two input images A and B, take the pixel values ​​at the same position and linearly add them, and then assign the result to the pixel at the same position in the target image. The parameter α controls the weight of the two pictures in the target image.

                              g(x)=αA(x)+(1−α)B(x)

What is the effect of linear blending of images? In slideshow page turning or film production, it is often necessary to produce the effect of screen superimposition. In the above formula, as long as α is gradually reduced from 1 to 0, the superimposition effect when transitioning from image I0 to image I1 can be produced.

OpenCV provides an API for linear blending of two images. The calculation formula on which the API is based is as follows:

                     dst = src1[i] * α + src2[i] * β + γ;

*addWeighted original function*: 
=void addWeighted(InputArray src1, double alpha, InputArray src2, double beta, double gamma, OutputArray dst, int dtype = -1);=
- Parameters:
  1) the first image to be superimposed Mat
  2) Identifies the weight of the superposition of the first parameter
  3) represents the second superimposed image, it needs to have the same size
     and number of channels as the first array
  4) represents the weight of the second superimposed image
  5) the output parameter, which needs to have the same number of channels and
     size as the first two images
  6) a scalar value added to the total weight (fill in 0 just fine)
  7) the depth of the output array has a default value of -1, when the two
     superimposed images have the same depth, the parameter is -1

     =addWeighted(imageROI, 0.5, logo, 0.5, 0, imageROI);=

*Notes*:
1) It is worth noting that the size and type of the two images must be the
   same. If the two images are inconsistent, first use the ROI to extract the
   smaller image and then superimpose it.
2) Although it is not mandatory that the two weights must add up to 1, but it is
   usually that dry.
	 #+END_QUOTE
	 1) Code
	    #+BEGIN_SRC c++
	      #include <opencv2/core/core.hpp>
	      #include <opencv2/highgui/highgui.hpp>
	      using namespace cv;

	      int main()
	      {
		  double α = 0.4;
		  double β;
		  Mat srcImage2, srcImage3, dstImage;

		  srcImage2 = imread("67.jpg");
		  srcImage3 = imread("68.jpg");

		  β = (1.0 - α);
		  addWeighted(srcImage2, α, srcImage3, β, 0., dstImage);

		  namedWindow("<1>");
		  imshow("<1>", srcImage2);
		  namedWindow("<2>");
		  imshow("<2>", srcImage3);

		  namedWindow("<3>");
		  imshow("<3>", dstImage);

		  waitKey();
		      return 0;
	      }
	    #+END_SRC
	    1) Picture 1 and Picture 2 Original picture ( two pictures must be
               the same size)
	       1) here is a screenshot picture, the size may be different, it
                  will not run
	       2) If you download less than two identical pictures, you can
                  refer to this link to change any size
                  https://blog.csdn.net/m0_51233386/article/details/112393204.
      3) ROI and =addWeighted= combined with image blending operation: sometimes
         we want to process certain regions in the image,
         that is, only interested in certain regions, then the ROI and
         addWeighted function are used in combination. Specify the area to
         perform image blending operations.
	 #+BEGIN_SRC c++
	   #include <opencv2/core/core.hpp>
	   #include <opencv2/highgui/highgui.hpp>
	   using namespace cv;

	   int main()
	   {
		   Mat image = imread("49.jpg");
		   Mat logo = imread("61.jpg");

		   Mat imageROI;
		   imageROI = image(Rect(300, 80, logo.cols, logo.rows));

		   addWeighted(imageROI, 1, logo, 0.5, 0, imageROI);

		   namedWindow("[3] 49");
		   imshow("[3] 49+61", image);

		   imwrite("blend.jpg", image);
		   waitKey();
		   return 0;
	   }
	 #+END_SRC
      4) Direct addition (non-linear)
	 #+BEGIN_SRC c++
	   #include <opencv2/core/core.hpp>
	   #include <opencv2/highgui/highgui.hpp>
	   using namespace cv;

	   int main()
	   { 
	       Mat src1, src2, dst_add;
	       src1 = imread("67.jpg");
	       src2 = imread("68.jpg");

	       add(src1, src2, dst_add);

	       imshow("src1", src1);
	       imshow("src2", src2);
	       imshow("dst_add", dst_add);
	       waitKey();

	       return 0;
	   } 
	 #+END_SRC
      5) Only works if the images have the same nr of channels
	 1) Overlaying a png (4-channels BGRA) over a frame (3-channels BGR),
            even after conversion, does not use the alpha channel, so it's not
            useful
	    #+BEGIN_SRC c++
	    /**< Extract ROI from frame */
            cv::cvtColor(frame, frame, cv::COLOR_BGR2BGRA);
            roi = frame(Rect(x, y + 0.6 * h, overlay.cols, overlay.rows));
            addWeighted(roi, alpha, overlay, beta, 0, roi);
	    #+END_SRC
   4) *Image overlay with alpha blending*: steps
      1) Preparing background
	 1) Capture frame
	 2) Detect faces (ROIs)
      2) Preparing overlay
	 1) Instantiate a filter struct
	 2) Open the png file: =cv::Mat filter = imread(filt.name, -1) //
            4-channel BGRA=
      3) For every face (ROI)
	 1) Get its coordinates and dimensions
	 2) Resize filter to match ROI
	 3) Extract overlap ROI to perform blending
	 4) Apply overlay over overlapping ROI and then copy it to original
            frame
3) [ ] Implement gesture recognition algorithm
   1) To recognize a gesture, several approaches may be used, namely: image
      segmentation with background subtraction and thresholding; deep neural
      networks; convex hull and defects; machine learning with cascade
      classifiers (haar classifiers).
   2) The most reliable strategy is to use a machine learning or deep learning
      technique. The latter, however, is more computationally expensive.
   3) Thus, machine learning with haar classifiers was used. For this technique
      to work it is required to ([[https://www.youtube.com/watch?v=XrCAvs9AePM][src]]):
      1) *Train a machine learning model* to obtain a suitable cluster of haar
         classifiers. The training of the model requires:
	 1) Image samples:
	    1) Positive: images that contain the pattern to be recognized
	    2) Negative (aka background images): images that don't contain
               the pattern to be recognized
	 2) A method for image gathering: web scraping, live image capture,
            =opencv_createsamples=, etc.
	 3) Labeling:
	    1) Both negative and positive images require annotation: the former
               just to identify what images are used; the latter containing the
               filenames, position, size, and number of occurrences within a
               file.
	    2) The labeling can be performed with =opencv_annotation= tool: only
               present in OpenCV 3.4.
	 4) Training: one can use =opencv_traincascade= (only available in
            OpenCV 3.4)., providing:
	    1) The path where to store the output model
	    2) The positive and negative samples path
	    3) Minimum size of the samples to look for (width and height)
	    4) Number of positive and negative samples (the ratio is important)
	    5) The number of stages of the machine learning model (something
               between 9-12 stages is recommended).
	 5) The training will produce an output model containing the cascade of
            haar classifiers to use, e.g., =palm.xml=.
      2) *Use the machine learning model*: For that OpenCV provides the
         following functions:
	 1) =CascadeClassifier::load(std::string pathToModel)=: loads the
            cascade classifiers model.
	 2) =CascadeClassifier::detectMultiScale()=: Detects objects of
            different sizes in the input image. The detected objects are
            returned as a list of rectangles.
   4) The machine learning model training must be performed for each gesture to
      be recognized.
      1) This is time expensive: a model can take many hours or several days to
         train depending on the complexity and data samples size.
      2) Requires tweaking: for accurate results of prediction, the machine
         learning model training requires the programmer to tweak the training
         parameters.
      3) May not produce the intended results: the accuracy may not be the
         expected, due to data samples selection or the parameters used.
   5) First, one tried to train a machine learning model for a pointing finger.
      1) However, the results were not as expected.
   6) As such, one changed the approach:
      1) Identify a reliable machine learning model for a hand gesture
      2) Adapt the UI to be triggered by that particular gesture instead of
         multiple hand gestures.

4) [ ] Wrap it in the gesture recognition engine
5) [ ] Implement Twitter sharing
6) [ ] Implement client-server architecture

